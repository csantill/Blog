{
  "pages": [
    {
      "title": "Sublime Text Setup",
      "text": "I have been using Sublime text as my primary editor for some time now. Here I wanted to share my current setup for the editor including all settings, packages, shortcut keys and themes.\n\n\nTable of Contents\n\nPackages\nShortcut Keys\nTheme and Color Scheme\nUser Settings / Preferences\n\n\n\n\n\n\nPackages\nFirst thing you will need to install is the Package Control. This can be easily done by following the\ndirections at their installation instructions.\nOnce you have installed the package manager and restarted sublime text, now you\ncan install all other packages using the powerful command pallet. Hit ctrl +\nshift + P and type \"Install\", choose \"Package Control : Install Package\".\nNow you can search for any package that you wish to install, and then press\nEnter to install it.\nHere is a list of packages that I currently use:\n\nAlignment\nBracket Highlighter\nC++11\nColumn Select\nDocBlockr_Python\nGitGutter\nMagicPython\nrsub\nSearch In Project\nSublimeLinter\nSublimeLinter-flake8\n\nAlignment provides a simple key-binding for aligning multi-line and multiple\nselections. Bracket Highlighter, as the name suggests, matches a variety of\nbrackets such as: [], (), {}, \"\", '', #!xml <tag></tag>, and even custom\nbrackets. C++11 provides better coloring scheme for c++11 syntax.\nColumn Select plug-in provides an alternate behavior for Sublime keyboard\ncolumn selection. The differences are:\n\nAllows reversing direction (go down too far, just go back up).\nAdded PageUp/PageDown, Home/End, and mouse selection.\nSkip rows that are too short.\nIf you start at the end of a line, then it will stay at the end of each line.\n\nDocBlockr_Python makes writing documentation a breeze for python code.\nGitGutter is a handy plug-in to show information about files in a git\nrepository. Main Features are:\n\nGutter Icons indicating inserted, modified or deleted lines\nDiff Popup with details about modified lines\nStatus Bar Text with information about file and repository\nJumping Between Changes to easily navigate between modified lines\n\nMagicPython is a package with preferences and syntax highlighter for cutting\nedge Python 3. It is meant to be a drop-in replacement for the default Python\npackage. MagicPython correctly highlights all Python 3.5 and 3.6 syntax\nfeatures, including type annotations, f-strings and regular expressions. It is\nbuilt from scratch for robustness with an extensive test suite.\nrsub is an implementation of TextMate 2's 'rmate' feature for Sublime Text,\nallowing files to be edited on a remote server using SSH port forwarding /\ntunneling. Please make sure you have installed a version of\nrmate and are using correct\nport forwarding.\nSearch in Project lets you use your favorite search tool (grep, ack, ag,\npt, rg, git grep, or findstr) to find strings across your entire current Sublime\nText project. I personally use\nthe silver_seracher (ag) for this purpose.\nSublimeLinter and SublimeLinter-flake8 is plug-in that provides an\ninterface to flake8.\nIt will be used with files that have the \u201cPython\u201d syntax.\n\n\nShortcut Keys\nHere is a summary of my key map:\n[\n    { \"keys\": [\"shift+alt+a\"], \"command\": \"find_all_under\" },\n    { \"keys\": [\"control+v\"], \"command\": \"paste_and_indent\" },\n    { \"keys\": [\"control+shift+v\"], \"command\": \"paste\" },\n    { \"keys\": [\"ctrl+alt+;\"], \"command\": \"alignment\" },\n    { \"keys\": [\"ctrl+alt+up\"], \"command\": \"column_select\", \"args\": {\"by\": \"lines\", \"forward\": false}},\n    { \"keys\": [\"ctrl+alt+down\"], \"command\": \"column_select\", \"args\": {\"by\": \"lines\", \"forward\": true}},\n    { \"keys\": [\"ctrl+alt+pageup\"], \"command\": \"column_select\", \"args\": {\"by\": \"pages\", \"forward\": false}},\n    { \"keys\": [\"ctrl+alt+pagedown\"], \"command\": \"column_select\", \"args\": {\"by\": \"pages\", \"forward\": true}},\n    { \"keys\": [\"ctrl+alt+home\"], \"command\": \"column_select\", \"args\": {\"by\": \"all\", \"forward\": false}},\n    { \"keys\": [\"ctrl+alt+end\"], \"command\": \"column_select\", \"args\": {\"by\": \"all\", \"forward\": true}}\n]\n\n\nTheme and Color Scheme\nI like using the material theme. In particular, I use the \"Materialize\" theme.\nYou can use this by installing the following packages:\n\nMaterialize\nMaterialize-Appbar\nMaterialize-White-Panels\n\nWith these installation, you will also get a lot of color schemes.\nI prefer to use the \"Material Oceanic Next\" color scheme.\nAll other settings for this theme can be seen in my settings below.\n\n\nUser Settings / Preferences\nHere is my complete set of settings for Sublime Text. Please feel free to\nleave comments below for any questions or suggestions.\n{\n    \"always_show_minimap_viewport\": true,\n    \"auto_complete\": true,\n    \"bold_folder_labels\": true,\n    \"caret_extra_width\": 1.5,\n    \"color_scheme\": \"Material Oceanic Next (SL).tmTheme\",\n    \"default_line_ending\": \"unix\",\n    \"drag_text\": false,\n    \"draw_white_space\": \"all\",\n    \"enable_tab_scrolling\": false,\n    \"font_face\": \"Hack\",\n    \"font_options\":\n    [\n        \"directwrite\",\n        \"gray_antialias\",\n        \"subpixel_antialias\"\n    ],\n    \"font_size\": 13,\n    \"ignored_packages\":\n    [\n        \"C++\",\n        \"Python\",\n        \"Vintage\"\n    ],\n    \"indent_guide_options\":\n    [\n        \"draw_normal\",\n        \"draw_active\"\n    ],\n    \"line_padding_bottom\": 1,\n    \"line_padding_top\": 1,\n    \"material_theme_bold_tab\": true,\n    \"material_theme_compact_panel\": true,\n    \"material_theme_compact_sidebar\": false,\n    \"material_theme_contrast_mode\": true,\n    \"material_theme_disable_fileicons\": false,\n    \"material_theme_disable_folder_animation\": true,\n    \"material_theme_disable_tree_indicator\": true,\n    \"material_theme_panel_separator\": true,\n    \"material_theme_small_statusbar\": true,\n    \"material_theme_small_tab\": true,\n    \"material_theme_tabs_autowidth\": true,\n    \"material_theme_tabs_separator\": true,\n    \"material_theme_tree_headings\": true,\n    \"overlay_scroll_bars\": \"enabled\",\n    \"rulers\":\n    [\n        80\n    ],\n    \"scroll_past_end\": true,\n    \"soda_classic_tabs\": true,\n    \"soda_folder_icons\": true,\n    \"tab_completion\": false,\n    \"tab_size\": 4,\n    \"theme\": \"Material Oceanic Next.sublime-theme\",\n    \"translate_tabs_to_spaces\": true,\n    \"trim_trailing_white_space_on_save\": true,\n    \"word_wrap\": true,\n    \"hot_exit\": false,\n    \"remember_open_files\": false\n}",
      "tags": "Editor",
      "url": "https://sadanand-singh.github.io/posts/sublimetext/"
    },
    {
      "title": "Support Vector Machines",
      "text": "In this post we will explore a class of machine learning methods called\nSupport Vector Machines also known commonly\nas SVM.\n\n\nTable of Contents\n\nIntroduction\nSupport Vector Classifier\nThe Kernel Trick\nCurse of Dimensionality.... huh!!!\n\n\n\nIntroduction\nSVM is a supervised machine learning algorithm which can be\nused for both classification and regression.\n\n\n\nIn the simplest classification problem, given some data points each belonging\nto one of the two classes, the goal is to decide which class a new data point\nwill be in. A simple linear solution to this problem can be viewed in a\nframework where a data point is viewed as a p-dimensional vector, and we want\nto know whether we can separate such points with a (p-1)-dimensional\nhyperplane.\nThere are many hyperplanes that might classify the data. One reasonable choice\nas the best hyperplane is the one that represents the largest separation, or\nmargin, between the two classes. So we choose the hyperplane so that the\ndistance from it to the nearest data point on each side is maximized. If such a\nhyperplane exists, it is known as the maximum-margin hyperplane and the linear\nclassifier it defines is known as a maximum margin classifier; or\nequivalently, the perceptron of optimal stability.\nThe figure on the right is a binary classification problem (points labeled\n\\(y_i = \\pm 1\\)) that is linearly separable in space defined by the vector\nx. Green and purple line separate two classes with a small margin, whereas\nyellow line separates them with the maximum margin.\n\n\n\nMathematically, for the linearly separable case, any point x lying on the\nseparating hyperplane satisfies: \\(\\mathbf{x}T\\mathbf{w} = 0\\), where\nw is the vector normal to the hyperplane, and b is a constant that\ndescribes how much plane is shifted relative to the origin. The distance of the\nhyperplane from the origin is \\(\\frac{b}{\\lVert \\mathbf{w} \\rVert}\\).\nNow draw parallel planes on either side of the decision boundary, so we have\nwhat looks like a channel, with the decision boundary as the central line, and\nthe additional planes as gutters. The margin, i.e. the width of the channel, is\n\\((d_+ + d_-)\\) and is restricted by the data points closest to the\nboundary, which lie on the gutters. The two bounding hyperplanes of the channel\ncan be represented by a constant shift in the decision boundary. In other\nwords, these planes ensure that all the points are at least a signed distance\nd away from the decision boundary. The channel region can be also\nrepresented by the following equations:\n\n\\begin{equation*}\n\\begin{aligned}\n& \\mathbf{x}_iT\\mathbf{w} + b \\ge +a, \\text{for  } y_i = +1 \\\\\n& \\mathbf{x}_iT\\mathbf{w} + b \\le -a, \\text{for  } y_i = -1\n\\end{aligned}\n\\end{equation*}\n\nThese conditions can be put more succinctly as:\n\n\\begin{equation*}\ny_i (\\mathbf{x}_iT\\mathbf{w} + b) \\ge a, \\forall i\n\\end{equation*}\n\nUsing the formulation of distance from origin of three hyper planes, we can\nshow that, the margin, M is equivalent to\n\\(d_+ + d_- = 2a / \\lVert \\mathbf{w} \\rVert\\).\nWithout any loss of generality, we can set \\(a = 1\\), since it only\nsets  the scale (units) of \\(b\\) and \\(\\mathbf{w}\\).\nSo to maximize the margin, we have to maximize\n\\(1 / \\lVert \\mathbf{w} \\rVert\\). Such a non-convex\nobjective function can be avoided if we choose in stead to minimize\n\\({\\lVert \\mathbf{w} \\rVert}2\\).\nIn summary, for a problem with m numbers of training data points, we need to\nsolve the following quadratic programming problem:\n\n\\begin{equation*}\n\\begin{aligned}\n& {\\text{maximize  }}\nM \\\\\n& \\text{subject to  }\ny_i (\\mathbf{x}_iT\\mathbf{w} + b) \\ge M, \\forall \\text{ } i = 1 \\ldots m\n\\end{aligned}\n\\end{equation*}\n\nThis can be more conveniently put as:\n\n\\begin{equation*}\n\\begin{aligned}\n& {\\text{minimize  }}\nf(w)  \\equiv \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}2 \\\\\n& \\text{subject to  }\ng(\\mathbf{w}, b) \\equiv -y_i (\\mathbf{x}_iT\\mathbf{w} + b) + 1 \\le 0, i = 1 \\ldots m\n\\end{aligned}\n\\end{equation*}\n\nThe maximal margin classifier is a very natural way to perform classification,\nif a separating hyperplane exists. However, in most real-life cases no\nseparating hyperplane exists, and so there is no maximal margin classifier.\n\n\nSupport Vector Classifier\n\n\n\nWe can extend the concept of a separating hyperplane in order to develop a\nhyperplane that almost separates the classes, using a so-called \"soft\nmargin\". The generalization of the maximal margin classifier to the non-\nseparable case is known as the \"support vector\" classifier.\nAssuming the classes overlap in the given feature space. One way to deal with\nthe overlap is to still maximize M, but allow for some points to be on the\nwrong side of the margin. In order to allow these, we can define the slack variables as,\n\\(\\xi = ( \\xi_1, \\xi_2 \\ldots \\xi_m)\\).\nNow, keeping the above optimization problem as a convex problem,\nwe can modify the constraints as:\n\n\\begin{equation*}\n\\begin{aligned}\n& y_i (\\mathbf{x}_iT\\mathbf{w} + b) \\ge M(1-\\xi_i), \\forall \\text{  } i = 1 \\ldots m, \\\\\n& \\xi_i \\ge 0 \\text{   and   } \\sum_{i=1}{m}\\xi_i \\le C \\text{  }\\forall \\text{   } i = 1 \\ldots m,\n\\end{aligned}\n\\end{equation*}\n\nWe can think of this formulation in the following context. The value\n\\(\\xi_i\\) in the constraint\n\\(y_i (\\mathbf{x}_iT\\mathbf{w} + b) \\ge M(1-\\xi_i)\\)\nis the proportional amount by which the prediction\n\\(f(x_i)=x_iT\\mathbf{w} + b\\) is on the wrong side of its margin. Hence by\nbounding the sum \\(\\sum \\xi_i\\), we can bound the total proportional amount by\nwhich predictions fall on the wrong side of their margin. Mis-classifications\noccur when \\(\\xi_i > 1\\), so bounding \\(\\sum \\xi_i\\) at a value K\nsay, bounds the total number of training mis-classifications at K.\nSimilar to the case of maximum margin classifier, we can rewrite the\noptimization problem more conveniently as,\n\n\\begin{equation*}\n\\begin{aligned}\n& {\\text{minimize  }}\n\\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}2 + C \\sum_{i}{m} \\xi_i\\\\\n& \\text{subject to  }\ny_i (\\mathbf{x}_iT\\mathbf{w} + b) \\ge 1 - \\xi_i, \\text{  }\n\\text{   and   } \\xi_i \\ge 0, \\text{   } i = 1 \\ldots m\n\\end{aligned}\n\\end{equation*}\n\nNow, the question before us is to find a way to solve this optimization\nproblem efficiently.\nThe problem above is quadratic with linear constraints, hence is a convex\noptimization problem. We can describe a quadratic programming solution using\nLagrange multipliers and then solving using the Wolfe dual problem.\nThe Lagrange (primal) function for this problem is:\n\n\\begin{equation*}\nL_P = \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}2 + C \\sum_{i}{m} \\xi_i - \\sum_{i=1}{m} \\alpha_i[y_i (\\mathbf{x}_iT\\mathbf{w} + b) - (1 - \\xi_i)] - \\sum_{i=1}{m} \\mu_i \\xi_i,\n\\end{equation*}\n\nwhich we can minimize w.r.t. \\(\\mathbf{w}\\), b, and \\(\\xi_i\\).\nSetting the respective derivatives to zero, we get,\n\n\\begin{equation*}\n\\begin{aligned}\n& \\mathbf{w} = \\sum_{i=1}{m} \\alpha_i y_i \\mathbf{x_i} \\\\\n& 0 = \\sum_{i=1}{m} \\alpha_i y_i \\\\\n& \\alpha_i = C - \\mu_i, \\forall i,\n\\end{aligned}\n\\end{equation*}\n\nas well as the positivity constraints, \\(\\alpha_i\\), \\(\\mu_i\\),\n\\(\\xi_i \\ge 0, \\text{  } \\forall i\\). By substituting these conditions back\ninto the Lagrange primal function, we get the Wolfe dual of the problem as,\n\n\\begin{equation*}\nL_D = \\sum_{i=1}{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}{m} \\sum_{j=1}{m} \\alpha_i \\alpha_j y_i y_j x_iT x_j\n\\end{equation*}\n\nwhich gives a lower bound on the original objective function of the quadratic\nprogramming problem for any feasible point. We maximize \\(L_D\\) subject to\n\\(0 \\le \\alpha_i \\le C\\) and \\(\\sum_{i=1}{m} \\alpha_i y_i = 0\\). In\naddition to above constraints, the Karush-Kuhn-Tucker (KKT) conditions include\nthe following constraints,\n\n\\begin{equation*}\n\\begin{aligned}\n& \\alpha_i[y_i (\\mathbf{x}_iT\\mathbf{w} + b) - (1 - \\xi_i)] = 0, \\\\\n& \\mu_i \\xi_i = 0, \\\\\n& y_i (\\mathbf{x}_iT\\mathbf{w} + b) - (1 - \\xi_i) \\ge 0,\n\\end{aligned}\n\\end{equation*}\n\nfor \\(i = 1 \\ldots m\\). Together these equations uniquely characterize the\nsolution to the primal and the dual problem.\nLet us look at some special properties of the solution. We can see that the\nsolution for \\(\\mathbf{w}\\) has the for\n\n\\begin{equation*}\n\\mathbf{\\hat{w}} = \\sum_{i=1}{m} \\hat{\\alpha_i} y_i \\mathbf{x_i}\n\\end{equation*}\n\nwith nonzero coefficients \\(\\hat{\\alpha}_i\\) only for those i for which\n\\(y_i (\\mathbf{x}_iT\\mathbf{w} + b) - (1 - \\xi_i) = 0\\). These i\nobservations are called \"support vectors\"  since \\(\\mathbf{w}\\) is\nrepresented in terms of them alone. Among these support points, some will lie\non the edge of the margin \\((\\hat{\\xi}_i = 0)\\), and hence characterized by\n\\(0 < \\hat{\\alpha}_i < C\\); the remainder \\((\\hat{\\xi}_i > 0)\\) have\n\\(\\hat{\\alpha}_i = C\\). Any of these margin points can be used to solve for\nb. Typically, once can use an average value from all of the solutions from\nthe support points.\nIn this formulation, C is model hyper parameter and can be used as a\nregularizer to control the capacity and generalization error of the model.\n\n\nThe Kernel Trick\nThe support vector classifier described so far finds linear boundaries in the\ninput feature space. As with other linear methods, we can make the procedure\nmore flexible by enlarging the feature space using basis expansions such as\npolynomials or splines. Generally linear boundaries in the enlarged space\nachieve better training-class separation, and translate to nonlinear boundaries\nin the original space. Once the basis functions \\(h_i(x), i=1 \\ldots m\\)\nare selected, the procedure remains same as before.\nNow recall that in calculating the actual classifier, we needed only support\nvector points, i.e. we need smaller amount of computation if data has better\ntraining-class separation. Furthermore, if one looks closely, we can find an\nadditional trick. The separating plane can be given by the function:\n\n\\begin{equation*}\n\\begin{aligned}\nf(x) & = \\mathbf{x}T \\mathbf{w} + b \\\\\n     & = \\mathbf{x}T \\sum_{i=1}{m} \\hat{\\alpha_i} y_i \\mathbf{x_i} + b\\\\\n     & = \\sum_{i=1}{m} \\hat{\\alpha_i} y_i \\mathbf{x}T \\mathbf{x}_i + b\\\\\n     & = \\sum_{i=1}{m} \\hat{\\alpha_i} y_i \\langle\\mathbf{x} \\mathbf{x}_i\\rangle + b\n\\end{aligned}\n\\end{equation*}\n\nwhere, \\(\\langle \\mathbf{x} \\mathbf{y} \\rangle\\) denotes inner product of\nvectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\). This shows us that we can\nrewrite training phase operations completely in terms of inner products!\nIf we were to replace linear terms with a predefined non-linear operation\n\\(h(x)\\), the above formulation of the separating plane will simply modify\ninto:\n\n\\begin{equation*}\n\\begin{aligned}\nf(x) & = h(\\mathbf{x})T \\mathbf{w} + b \\\\\n     & = h(\\mathbf{x})T \\sum_{i=1}{m} \\hat{\\alpha_i} y_i h(\\mathbf{x}_i) + b\\\\\n     & = \\sum_{i=1}{m} \\hat{\\alpha_i} y_i h(\\mathbf{x})T h(\\mathbf{x}_i) + b\\\\\n     & = \\sum_{i=1}{m} \\hat{\\alpha_i} y_i \\langle h(\\mathbf{x}) h(\\mathbf{x}_i) \\rangle + b\n\\end{aligned}\n\\end{equation*}\n\nAs before, given \\(\\hat{\\alpha_i}\\), b can be determined by solving\n\\(y_i f(\\mathbf{x}_i) = 1\\) for any (or all) \\(x_i\\) for which\n\\(0 < \\hat{\\alpha}_i < C\\).\nMore importantly, this tells us that we do not need to\nspecify the exact nonlinear transformation \\(h(x)\\) at all, rather only the\nknowledge of the Kernel function \\(K(x, x') = \\langle h(x)h(x') \\rangle\\)\nthat computes inner products in the transformed space is enough.\nNote that for the dual problem to be convex quadratic programming problem,\n`K` would need to be symmetric positive semi-definite.\nSome common choices of kernels are:\n\\(d{th}\\) degree polynomial:\n\\(K(x, x') = (1+\\langle x x' \\rangle )d\\)\nRadial basis:\n\\(K(x, x') = \\exp (-\\gamma  \\lVert \\mathbf{x - x'} \\rVert2 )\\)\nNeural network:\n\\(K(x, x') = \\tanh (\\kappa_1 \\langle x x' \\rangle + \\kappa_2)\\)\nThe role of the hyper-parameter C is clearer in an enlarged feature\nspace, since perfect separation is often achievable there. A large value of C\nwill discourage any positive \\(\\xi_i\\), and lead to an over-fit wiggly\nboundary in the original feature space; a small value of C will encourage a\nsmall value of \\(\\lVert w \\rVert\\), which in turn causes \\(f(x)\\) and\nhence the boundary to be smoother, potentially at the cost of more points as\nsupport vectors.\n\n\nCurse of Dimensionality.... huh!!!\nWith m training examples, p predictors and M support vectors, the SVM\nrequires \\(M3 + Mm + mpM\\) operations. This suggests the choice of the\nkernel and hence number of support vectors M will play a big role in\nfeasibility of this method. For a really good choice of kernel that leads to\nvery high training-class separation, i.e. \\(M <<< m\\), the method can be\nviewed as linear in m. However, for a bad choice case, \\(M \\approx m\\)\nwe will be looking at an \\(O (m3)\\) algorithm.\nThe modern incarnation of deep learning was designed to overcome these\nlimitations (large order of computations and clever problem-specific choice of\nkernels) of kernel machines. We will look at the details of a generic deep\nlearning algorithm in a future post.",
      "tags": "Algorithms,Machine Learning,mathjax",
      "url": "https://sadanand-singh.github.io/posts/svmModels/"
    },
    {
      "title": "Python Tutorial - Week 2",
      "text": "In the Week 1 we got started with Python. Now that we can interact with python, lets dig deeper into it.\n\nThis week we will go over some additional fundamental things common in any program - interactive input from users, adding comments to your code, use of conditional logic i.e. if - else conditions, loops, formatted output with strings and print() statements.\nPython Week 2\u00b6User Inputs\u00b6There are hardly any programs without any input. Input can come in various ways, for example from a database, another computer, mouse clicks and movements or from the internet. Yet, in most cases the input stems from the keyboard. For this purpose, Python provides the function input(). input() has an optional parameter, which is the prompt string, i.e. the text that will be shown when asking for input.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nname = input(\"What's your name? \")\nprint(\"Nice to meet you \" + name + \"!\")\nage = input(\"Your age? \")\nprint(\"So, you are already \" + age + \" years old, \" + name + \"!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat's your name? Sadanand\nNice to meet you Sadanand!\nYour age? 30\nSo, you are already 30 years old, Sadanand!\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if you try to do some mathematical operation on the age? You will get a TypeError as follows:\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nage = 12 + age\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-2-3d9ce720d6f3> in <module>()\n----> 1 age = 12 + age\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\n\n\n\n\n\n\n\n\n\n\nThis says that by default all data is read as raw input i.e. strings. If we want numbers we need to convert them ourselves. For example:\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n cities_canada = input(\"Largest cities in Canada: \")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLargest cities in Canada: [\"Montreal\", \"Ottawa\", \"Calgary\", \"Toronto\"]\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \n print(cities_canada, type(cities_canada))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[\"Montreal\", \"Ottawa\", \"Calgary\", \"Toronto\"] <class 'str'>\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ncities_canada = eval(input(\"Largest cities in Canada: \"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLargest cities in Canada: [\"Montreal\", \"Ottawa\", \"Calgary\", \"Toronto\"]\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n print(cities_canada, type(cities_canada))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n['Montreal', 'Ottawa', 'Calgary', 'Toronto'] <class 'list'>\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \npopulation = input(\"Population of Portland? \")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation of Portland? 604596\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n print(population, type(population))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n604596 <class 'str'>\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \npopulation = int(input(\"Population of Portland? \"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPopulation of Portland? 604596\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \n print(population, type(population))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n604596 <class 'int'>\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \npi = input(\"Value of PI is?\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue of PI is?3.14\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \n print(pi, type(pi))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.14 <class 'str'>\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \npi = float(input(\"Value of PI is?\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue of PI is?3.14\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \n print(pi, type(pi))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.14 <class 'float'>\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice the use of various methods like eval(), int() and float() to get user input in correct formats. In summary, eval() is used to get data into various native python formats, e.g. lists, dictionaries etc. We will look at these in more detail in next few tutorials. int() is used to convert input to integer numbers (numbers without decimals), while float() is used to get floating point numbers.\nAlso, of interest above is the type() method used in print statements. You can get the type of any variable in python using this method. In the output of this we see something like: < class 'float'> - if variable is of float type. For the time being we will ignore the \"class\" in this.\nIndentation Blocks\u00b6Python programs get structured through indentation, i.e. code blocks are defined by their indentation (The amount of blank space before any line). This principle makes it easier to read and understand other people's Python code.\nAll statements with the same distance to the right belong to the same block of code, i.e. the statements within a block line up vertically. The block ends at a line less indented or the end of the file. If a block has to be more deeply nested, it is simply indented further to the right.\nIn the following sections below we will see extensive use of such indentation blocks. Consider the following example to calculate Pythagorean triples. You do not need to understand the full code right here. We will revisit this code at the end of this tutorial.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nfrom math import sqrt\nn = input(\"Maximum Number? \")\nn = int(n)+1\nfor a in range(1,n):\n    for b in range(a,n):\n        c_square = a**2 + b**2\n        c = int(sqrt(c_square))\n        if ((c_square - c**2) == 0):\n            print(a, b, c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Number? 10\n3 4 5\n6 8 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above code, we see three indentation blocks, first and second \"for\" loops and the third \"if\" condition. There is another aspect of structuring in Python, which we haven't mentioned so far, which you can see in the example. Loops and Conditional statements end with a colon \":\" - the same is true for functions and other structures introducing blocks. So, we should have said Python structures by colons and indentation.\nComments in Python\u00b6Python has two ways to annotate/comment Python code. One is by using comments to indicate what some part of the code does. Single-line comments begin with the hash character (\"#\") and are terminated by the end of line. Here is an example:\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \n# This is a comment in Python before print statement\nprint(\"Hello World\") #This is also a comment in Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHello World\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditionals\u00b6Conditionals, - mostly in the form of if statements - are one of the essential features of a programming language. A decision has to be taken when the script or program comes to a point where it has a choice of actions, i.e. different computations, to choose from.\nThe decision depends in most cases on the value of variables or arithmetic expressions. These expressions are evaluated to the Boolean values True or False. The statements for the decision taking are called conditional statements. Alternatively they are also known as conditional expressions or conditional constructs.\nConditional statements in Python use indentation blocks to conditionally execute certain code. The general form of the if statement in Python looks like this:\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nif condition_1:\n    statement_block_1\nelif condition_2:\n    statement_block_2\n\n...\n\nelif another_condition:    \n    another_statement_block\nelse:\n    else_block\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf the condition \"condition_1\" is True, the statements of the block statement_block_1 will be executed. If not, condition_2 will be evaluated. If condition_2 evaluates to True, statement_block_2 will be executed, if condition_2 is False, the other conditions of the following elif conditions will be checked, and finally if none of them has been evaluated to True, the indented block below the else keyword will be executed.\nTypical examples of \"condition\" statements follow some of following operations:\nmathematical comparisons like, \"<\", \">\", \"<=\", \">=\", \"==\"\nobject comparisons like \"is\" i.e. this is exactly something or not.\nboolean logic operators like \"not\", \"or\", \"and\", \"xor\" etc.\nThe following objects are evaluated by Python as False:\n\nnumerical zero values (0, 0L, 0.0, 0.0+0.0j),\nthe Boolean value False,\nempty strings,\nempty lists and empty tuples,\nempty dictionaries.\nthe special value None.\n\nAll other values are considered to be True.\nLet us try to solve this simple DNA sequence problem:\nGiven the an input DNA sequence, print the sequence if its length is less than equal to 20. Print \"Error\" if the sequence is empty or its length is larger than 25. If length is between 21 and 25, print the last 5 bases only.\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \ndna = \"ATGCCGATTTATCGGGAACCNNNAATTCCGG\"\n\nif len(dna) <= 20:\n    if len(dna) > 0:\n        print(dna)\n    else:\n        print(\"ERROR!\")\nelif len(dna) <= 25:\n        print(dna[-5:])\nelse:\n    print(\"ERROR!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERROR!\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ndna = \"ATGCAATGCN\"\n\nif len(dna) <= 20:\n    if len(dna) > 0:\n        print(dna)\n    else:\n        print(\"ERROR!\")\nelif len(dna) <= 25:\n        print(dna[-5:])\nelse:\n    print(\"ERROR!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nATGCAATGCN\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ndna = \"\"\n\nif len(dna) <= 20:\n    if len(dna) > 0:\n        print(dna)\n    else:\n        print(\"ERROR!\")\nelif len(dna) <= 25:\n        print(dna[-5:])\nelse:\n    print(\"ERROR!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nERROR!\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \ndna = \"ATGCCGATTTATCGGGAACCNNN\"\n\nif len(dna) <= 20:\n    if len(dna) > 0:\n        print(dna)\n    else:\n        print(\"ERROR!\")\nelif len(dna) <= 25:\n        print(dna[-5:])\nelse:\n    print(\"ERROR!\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCCNNN\n\n\n\n\n\n\n\n\n\n\n\n\n\nif else conditions can also be combined in a regular assignment expression to assign values. For example,\nIn the DNA case, we want to store length of DNA. However, we want length to number only if length of sequence is between 1 and 25. In all other cases, we want to store the length of sequence as -1. A typical way to do this would be:\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \ndna = \"ATGCCGATTTATCGGGAACCNNN\"\nlength = -1\nif 0 < len(dna) <= 20:\n    length = len(dna)\n    \nprint(length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-1\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \ndna = \"CCGGGAACCTCACG\"\nlength = -1\nif 0 < len(dna) <= 20:\n    length = len(dna)\n    \nprint(length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis example can be written in a much shorter fashion as well. Such conditions are commonly called as ternary if statements.\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \ndna = \"ATGCCGATTTATCGGGAACCNNN\"\nlength = len(dna) if 0 < len(dna) <= 20 else -1\nprint(length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-1\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \ndna = \"CCGGGAACCTCACG\"\nlength = len(dna) if 0 < len(dna) <= 20 else -1\nprint(length)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoops\u00b6Many algorithms make it necessary for a programming language to have a construct which makes it possible to carry out a sequence of statements repeatedly. The code within the loop, i.e. the code carried out repeatedly, is called the body of the loop.\nThere are two types of loops in Python -\n\nwhile Loop\nfor Loop\n\nThe while Loop\nThese are a type of loop called \"Condition-controlled loop\". As suggested by the name, the loop will be repeated until a given condition changes, i.e. changes from True to False or from False to True, depending on the kind of loop.\nLet us consider the following example of DNA sequence:\nWe want to print every base of a given sequence, until we have found 2 As.\n\n\n\n\n\n\nIn\u00a0[28]:\n\n    \ndna = \"ATGCCGATTTATCGGGAACCNNN\"\ncountA = 0\nindex = 0\nwhile countA < 2:\n    print(dna[index])\n    if dna[index] == 'A':\n        countA = countA + 1\n    index = index + 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nT\nG\nC\nC\nG\nA\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above example, the loop (code under the while block) was executed until countA < 2 statement remained true.\nThe loops can be made to exit before its actual completion using the break statements.\nConsider the following example of DNA sequence.\nWe want to print every base of a given sequence, until we have found 2 As. However, we want to stop printing as soon as we have found an 'N' base.\n\n\n\n\n\n\nIn\u00a0[29]:\n\n    \ndna = \"ATGCNCGATTTATCGGGAACCNNN\"\ncountA = 0\nindex = 0\nwhile countA < 2:\n    if dna[index] == 'N':\n        break\n    if dna[index] == 'A':\n        countA = countA + 1\n    print(dna[index])\n    index = index + 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nT\nG\nC\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let us consider another case while looping over something. We want to skip over a part of code at certain condition. In such cases, continue statement comes handy.\nConsider the following example wrt to DNA sequencing.\nGiven a sequence of dna, we do NOT want to print the base name if it is 'N'\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \ndna = \"ATGCNCN\"\nindex = 0\nwhile index < len(dna):\n    index = index + 1\n    if dna[index-1] == 'N':\n        continue\n    print(dna[index-1])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nT\nG\nC\nC\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe for Loop\nA for loop is similar to while loop, except it is used to loop over certain elements, unlike while loop that continues until certain condition is satisfied. In the case DNA sequences, say, one case of for loop would be to loop over all bases in a sequence.\nConsider the following example:\nGiven a DNA sequence, we want to count the number of all 'A', and 'T bases.\n\n\n\n\n\n\nIn\u00a0[31]:\n\n    \ndna = \"ATGCNCGATTTATCGGGAACCNNN\"\ncount = 0\nfor base in dna:\n    if base == 'A' or base == 'T':\n        count += 1\n\nprint(\"Number of A, T bases is:\", count)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of A, T bases is: 10\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to while loops, we can use break and continue statements with for loops as well.\nLet us look at somewhat complicated use of for loop:\nGiven a DNA sequence, we want to count the number of doublets of bases, i.e. no. of times certain bases come twice exactly. If some base occur more than twice, we do not want to count that.\n\n\n\n\n\n\nIn\u00a0[32]:\n\n    \ndna = \"ATGGCNCGAATTTAAATCGGGAACCNNN\"\ncountPairs = 0\npairFound = 0\nprevBase = ''\nfor base in dna:\n    if (base == prevBase):\n        pairFound += 1\n    else:\n        if pairFound == 1:\n            countPairs += 1\n        pairFound = 0\n    prevBase = base\n\nprint(\"Number of paired bases is:\", countPairs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber of paired bases is: 4\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormatting of Output\u00b6Final topic for this week is the formatting of text in the print statements. Consider the following case:\nWe have following variables:\nname = \"Sadanand\", age = 30, and gender = \"male\".\nWe would like to print a quite cumbersome statement like as follows. This can be quite easily done using the format method.\n\n\n\n\n\n\nIn\u00a0[33]:\n\n    \nname = \"Sadanand\"\nage = 30\ngender = \"male\"\nmsg = \"Hi {0}, You are a {1}, and you have seen {2} winters as you are {2} years old! Thanks {0}!\"\nprint(msg.format(name, gender, age))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHi Sadanand, You are a male, and you have seen 30 winters as you are 30 years old! Thanks Sadanand!\n\n\n\n\n\n\n\n\n\n\n\n\n\nThus format method provides us with easy way to mix different types of variables in the strings.\nThats it for this week. Next we will look at strings and lists in Python in more detail.\nExercise\nGiven the following sequence of dna - \"ATGGCNCGAATTTAAATCGGGAACCNNN\",\n\nWrite a program to count number of all triplets in it.\n\nWrite a program that prints all non 'T' bases that come after 'T', but stops when two or more continuous 'T' has been found.\n\nWrite a program to generate new sequence with every 3rd base from the above sequence.\n\nWrite a program to calculate sum of all numbers from 1 to 10. HINT: Please take a look at the range method.",
      "tags": "Programming,Python",
      "url": "https://sadanand-singh.github.io/posts/pythontutorialweek2/"
    },
    {
      "title": "Projects",
      "text": "Predict Amyloid Protein Structures\n\nA deep learning model in Tensorflow to\nanalyze important features and predict secondary structures of\namyloid proteins.\nPlease look at some of my past\npublications for\nadditional details of this work.\n\n\nFlac To mp3 Converter\nA simple python script to maintain flac and mp3 files in sync, including\ntags, and album art. This script can be used as a cron job to keep files in sync.\nThe code repository is\navailable on Github.\n\n\nKaggle Competitions\nPython code and notebooks for different Kaggle competitions that I have\nparticipated. The repositories (names starting with kaggle) can be found on\nmy Github account page.\n\n\nTicTacToe in Qt5\n\nAn unbeatable game of TicTacToe using minimax algorithm and PyQt5.\nThe code repository\nis available on Github. You need to have PyQt5 installed to run this.\n\n\nRNA-binding Residues in Proteins\n\nProtein-RNA interface residue predictors trained using various machine learning\nmodels built with scikit-learn ,\nkeras  and tensorflow .\n\n\nLudo in Qt5\n\nA working game of Ludo using Qt5.\nThe source for this project is\navailable on github . You need to have Qt5 installed to run this.",
      "tags": "mathjax",
      "url": "https://sadanand-singh.github.io/projects/"
    },
    {
      "title": "Curious and Always Looking for Questions",
      "text": "My name is  Sadanand Singh. I am sadanand-singh at Github and @sadanandsingh at twitter.\nI am a chemical engineer, a physicist, a scientist, a programmer, and a human being; with interests in technology, traveling, politics, economics, and society.\nThis space is for my personal views on different subjects. If you have interests in any of these topics, you are welcome to have a peek into my world. Please jot down your views through comments or any of the social media links.\n\n  We should take care not to make the intellect our God; it has, of course, powerful muscles, but no personality.\n  Albert Einstein\n\n\nRecent Posts\n\n\n\n    \n            \n            2017-04-23 16:57\n            \u00a0\n            Sublime Text Setup\n            \n            \n            2016-11-11 10:30\n            \u00a0\n            Support Vector Machines\n            \n            \n            2016-10-03 19:15\n            \u00a0\n            Python Tutorial - Week 2\n            \n            \n            2016-05-20 21:20\n            \u00a0\n            EDA of Lending Club Data - II\n            \n            \n            2016-03-29 18:00\n            \u00a0\n            EDA of Lending Club Data\n            \n            \n            2015-12-18 20:30\n            \u00a0\n            Exploring Multiple Variables\n            \n            \n            2015-12-13 12:05\n            \u00a0\n            Pseudo Facebook Data - Exploring Two Variables\n            \n            \n            2015-12-09 18:05\n            \u00a0\n            Pseudo Facebook Data - Plots in Python\n            \n            \n            2015-12-08 20:00\n            \u00a0\n            Reddit Survey: Introduction to Pandas\n            \n            \n            2015-08-30 13:04\n            \u00a0\n            Python Tutorial - Week 1\n            \n    \n\n\n\n\n\nMachine Learning Posts\n\n\n\n    \n            \n            2016-11-11 10:30\n            \u00a0\n            Support Vector Machines\n            \n            \n            2016-05-20 21:20\n            \u00a0\n            EDA of Lending Club Data - II\n            \n            \n            2016-03-29 18:00\n            \u00a0\n            EDA of Lending Club Data\n            \n            \n            2015-12-18 20:30\n            \u00a0\n            Exploring Multiple Variables\n            \n            \n            2015-12-13 12:05\n            \u00a0\n            Pseudo Facebook Data - Exploring Two Variables\n            \n            \n            2015-12-09 18:05\n            \u00a0\n            Pseudo Facebook Data - Plots in Python\n            \n            \n            2015-12-08 20:00\n            \u00a0\n            Reddit Survey: Introduction to Pandas",
      "tags": "mathjax",
      "url": "https://sadanand-singh.github.io/"
    },
    {
      "title": "EDA of Lending Club Data - II",
      "text": "In the last post we looked at some initial cleanup of the data. We will start from there by loading the pickled dataframe.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\ndf = pd.read_pickle(\"/home/ssingh/LendingClubData/Part1.pickle\")\n\n\n\n\n\n\n\n\n\n\n\n\nLets first check what all columns are remaining in our dataframe. As there are still more than 100 variables left, we will initially focus on the first 25 ones only.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nprint(df.columns)\nprint(df.columns.shape)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex(['loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate',\n       'installment', 'grade', 'sub_grade', 'emp_length', 'home_ownership',\n       ...\n       'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'pct_tl_nvr_dlq',\n       'percent_bc_gt_75', 'pub_rec_bankruptcies', 'tax_liens',\n       'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit',\n       'total_il_high_credit_limit'],\n      dtype='object', length=111)\n(111,)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the data dictionary, we can see that funded_amnt is total amount committed till now, and funded_amnt_inv is the amount funded by investors. It is difficult to think of a direct correlation between the charged interest rate and the actual funded amount. However, this amount can give us a range of risk that one will be taking when investing. Given the two committed amounts are very similar, we will drop the the \"funded_amnt\" column. The installment column gives us feel of how much burden the loan will be on the borrower. However, this will be direct function of term and rate of the loan and hence should be dropped from any further analysis. the \"grade\" and \"sub_grade\" are LC assigned grades to the loan. We can keep these as secondary variables to check the liability of models used by LC.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ndf.drop(['funded_amnt', 'installment', \"pymnt_plan\"],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ndf.ix[:4,11:19]\n\n\n\n\n\n\n\n\n\n\n\nOut[4]:\n\n\n\n\n\n  \n    \n      \n      url\n      desc\n      purpose\n      title\n      zip_code\n      addr_state\n      dti\n      delinq_2yrs\n    \n  \n  \n    \n      0\n      https://www.lendingclub.com/browse/loanDetail....\n      NaN\n      debt_consolidation\n      Debt consolidation\n      235xx\n      VA\n      12.03\n      0\n    \n    \n      1\n      https://www.lendingclub.com/browse/loanDetail....\n      NaN\n      credit_card\n      Credit card refinancing\n      937xx\n      CA\n      14.92\n      0\n    \n    \n      2\n      https://www.lendingclub.com/browse/loanDetail....\n      NaN\n      debt_consolidation\n      Debt consolidation\n      850xx\n      AZ\n      34.81\n      0\n    \n    \n      3\n      https://www.lendingclub.com/browse/loanDetail....\n      NaN\n      car\n      Car financing\n      953xx\n      CA\n      8.31\n      1\n    \n    \n      4\n      https://www.lendingclub.com/browse/loanDetail....\n      NaN\n      debt_consolidation\n      Debt consolidation\n      077xx\n      NJ\n      25.81\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor our purpose, we will not be going into any kind of natural language processing, hence, the description and the url variables are of no use to us.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndf.drop(['url', 'desc'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nLet us check what are typical \"purpose\" used for requesting loans. We can view this as a histogram plot.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \nsns.set()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\ntotal = float(len(df.index)) \nax = sns.countplot(x=\"purpose\", data=df, palette=\"Set2\");\nax.set(yscale = \"log\")\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also look for any kind of correlation between the purpose and the interest rate of loan using a box plot. We can clearly see this could be useful for building our model!\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \nsns.boxplot(x=\"purpose\", y=\"int_rate\", data=df)\nplt.xticks(rotation=90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet also look for any kind of correlations between \"employment length\", \"rate\" and \"status\" of loans. Status here, if you remember from the previous post refers to the risk factor involved with the loan.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \nsns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(df, vars=[\"int_rate\", \"emp_length\"], hue=\"loan_status\", diag_kind=\"kde\")\n\n\n\n\n\n\n\n\n\n\n\nOut[8]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e52116278>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs expected, we find good loans to have larger employment length. Interestingly, interest rate tends to be all over the place for high risk loans. But, if you think about it, that is what we are trying to fix here!\nAnalyzing tile of loans could be tricky. Again, due to lack of any kind of natural language processing, let us drop this as well.\nThe location address of borrowers can say interesting pattern about the interest rates. First three letters of zip code can give much more information than states. However, if the zip info is missing, state can provide a reasonable approx. of the data. Lets check if we have any data where zip data is missing. If none, we can simply drop the state information.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \ndf['zip_code'] = df['zip_code'].str.replace('xx','')\n\n\n\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \ndf.drop(['title'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ndf.zip_code.isnull().sum()\n\n\n\n\n\n\n\n\n\n\n\nOut[11]:\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \ndf.drop(['addr_state'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nThe \"dti\" column in the data dictionary has been described as - \"A ratio calculated using the borrower's total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower's self-reported monthly income\". Based on this information, Debt_to_Income ratio is a direct measure of the loan risk.\nLets check effects of delinquency over last 2 years on interest rate using a box plot:\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nsns.boxplot(x=\"delinq_2yrs\", y=\"int_rate\", data=df)\n\n\n\n\n\n\n\n\n\n\n\nOut[13]:\n\n\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2e502c3a58>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see visualize effects of delinquency over last 2 years. Let us bin this data into three bins - Low, Medium and High. We will now move on to the next set of columns.\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \ndf[\"delinq_2yrs\"] = pd.cut(df.delinq_2yrs, bins=3, labels=[\"Low\", \"Medium\", \"High\"], include_lowest = True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \ndf.ix[:4,15:23]\n\n\n\n\n\n\n\n\n\n\n\nOut[15]:\n\n\n\n\n\n  \n    \n      \n      earliest_cr_line\n      fico_range_low\n      fico_range_high\n      inq_last_6mths\n      mths_since_last_delinq\n      mths_since_last_record\n      open_acc\n      pub_rec\n    \n  \n  \n    \n      0\n      Aug-1994\n      750\n      754\n      0\n      NaN\n      NaN\n      6\n      0\n    \n    \n      1\n      Sep-1989\n      710\n      714\n      2\n      42.0\n      NaN\n      17\n      0\n    \n    \n      2\n      Aug-2002\n      685\n      689\n      1\n      NaN\n      NaN\n      11\n      0\n    \n    \n      3\n      Oct-2000\n      665\n      669\n      0\n      17.0\n      NaN\n      8\n      0\n    \n    \n      4\n      Nov-1992\n      680\n      684\n      0\n      NaN\n      NaN\n      12\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarliest credit line should play an important role in determining the rate. We will replace this column by something more quantitative - credit_age.\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nnow = pd.Timestamp('20160501')\ndf[\"credit_age\"] = pd.to_datetime(df.earliest_cr_line, format=\"%b-%Y\")\ndf['credit_age'] = (now - df['credit_age']).dt.days.divide(30).astype(\"int64\")\ndf.drop(['earliest_cr_line'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nLet us try to find a trend between interest rate, fico ranges and loan status.\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nsns.pairplot(df, vars=[\"int_rate\", \"fico_range_low\", \"fico_range_high\"], hue=\"loan_status\", diag_kind=\"kde\")\n\n\n\n\n\n\n\n\n\n\n\nOut[17]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e5239cef0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe find 2 FICO scores to be highly collinear. Further, high risk loans have much larger lower values of fico scores. We can safely replace these with the mean values of fico scores.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \ndf['fico'] = 0.5*(df['fico_range_high'] + df['fico_range_low'])\ndf.drop(['fico_range_high'],1, inplace=True)\ndf.drop(['fico_range_low'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to the 2 year delinquency, let us also look at the  6 month inquiry data. Other data like mths_since_last_delinq and mths_since_last_record can be safely removed, as they will be correlated to 2 year delinquency data.\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nsns.boxplot(x=\"inq_last_6mths\", y=\"int_rate\", data=df)\n\n\n\n\n\n\n\n\n\n\n\nOut[19]:\n\n\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2e50ff59b0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us find correlations between many of these similar variables.\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nsns.pairplot(df, vars=[\"int_rate\", \"pub_rec\", \"open_acc\", \"inq_last_6mths\"], hue=\"delinq_2yrs\", diag_kind=\"kde\")\n\n\n\n\n\n\n\n\n\n\n\nOut[20]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e50dea1d0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth open_acc and inq_last_6_mnths have a strong correlation with delinq_2year, and hence can be safely dropped. pub_rec too has a distinct shape for each levels of delinq_2yrs showing interdependence and hence we can drop this as well.\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ndf.drop(['pub_rec'],1, inplace=True)\ndf.drop(['open_acc'],1, inplace=True)\ndf.drop(['inq_last_6mths'],1, inplace=True)\ndf.drop(['mths_since_last_delinq'],1, inplace=True)\ndf.drop(['mths_since_last_record'],1, inplace=True) \n\n\n\n\n\n\n\n\n\n\n\n\nWe will now move on to the next set of columns.\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ndf.ix[:4,15:25]\n\n\n\n\n\n\n\n\n\n\n\nOut[22]:\n\n\n\n\n\n  \n    \n      \n      revol_bal\n      revol_util\n      total_acc\n      initial_list_status\n      out_prncp\n      out_prncp_inv\n      total_pymnt\n      total_pymnt_inv\n      total_rec_prncp\n      total_rec_int\n    \n  \n  \n    \n      0\n      138008\n      29%\n      17\n      w\n      12484.99\n      12484.99\n      4364.64\n      4364.64\n      2515.01\n      1849.63\n    \n    \n      1\n      6133\n      31.6%\n      36\n      w\n      6892.58\n      6892.58\n      4163.94\n      4163.94\n      3507.42\n      656.52\n    \n    \n      2\n      16822\n      91.9%\n      20\n      f\n      0.00\n      0.00\n      2281.98\n      2281.98\n      704.38\n      339.61\n    \n    \n      3\n      5753\n      100.9%\n      13\n      w\n      10868.67\n      10868.67\n      4117.57\n      4117.57\n      1931.33\n      2186.24\n    \n    \n      4\n      16388\n      59.4%\n      44\n      f\n      0.00\n      0.00\n      9973.43\n      9973.43\n      9600.00\n      373.43\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRevolving balance and revolving utilization, is a measure of \"how leveraged your credit cards are\". revol_util should provide a relative measure of leverage, whereas revol_bal should provide an absolute measurement. Before we proceed, we need to convert '%' data to fraction.\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \ndf.revol_util = pd.Series(df.revol_util).str.replace('%', '').astype(float)\ndf.revol_util = df.revol_util * 0.01\n\n\n\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \ng = sns.pairplot(df, vars=[\"revol_bal\", \"revol_util\", \"total_acc\"], hue=\"loan_status\", diag_kind=\"kde\")\nfor ax in g.axes.flat:  \n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNone of these variables seem to make any direct correlation with the risk levels of the loan. Given their direct use in the FICO score calculation, we will keep these in our analysis.\n\n\n\n\n\n\n\n\n\nLet us take a look at the initial listing status of the loan. Then, we can find a correlation between these and the risk level.\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \nsns.countplot(x=\"initial_list_status\", hue=\"loan_status\", data=df)\n\n\n\n\n\n\n\n\n\n\n\nOut[25]:\n\n\n\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2e4fe23898>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor high risk loans as well low risk ones, there does not seem to be any significant difference among two types of initial listing of the loan and hence we can drop it.\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \ndf.drop(['initial_list_status'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nFollowing variables remaining in the list refer to the current state of the loan and hence will not be playing any effect on the general state or risk level of the loan, therefore should be dropped from our analysis. We will also not consider any joint data for this analysis.\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \ndf.drop(['out_prncp'],1, inplace=True)\ndf.drop(['out_prncp_inv'],1, inplace=True)\ndf.drop(['total_pymnt'],1, inplace=True)\ndf.drop(['total_pymnt_inv'],1, inplace=True)\ndf.drop(['total_rec_prncp'],1, inplace=True)\ndf.drop(['total_rec_int'],1, inplace=True)\ndf.drop(['total_rec_late_fee'],1, inplace=True)\ndf.drop(['recoveries'],1, inplace=True)\ndf.drop(['collection_recovery_fee'],1, inplace=True)\ndf.drop(['last_pymnt_d'],1, inplace=True) \ndf.drop(['last_pymnt_amnt'],1, inplace=True)\ndf.drop(['next_pymnt_d'],1, inplace=True)\ndf.drop(['policy_code'],1, inplace=True)\ndf.drop(['application_type'],1, inplace=True)\ndf.drop(['annual_inc_joint'],1, inplace=True) \ndf.drop(['dti_joint'],1, inplace=True)\ndf.drop(['verification_status_joint'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[28]:\n\n    \ndf.ix[:4,18:24]\n\n\n\n\n\n\n\n\n\n\n\nOut[28]:\n\n\n\n\n\n  \n    \n      \n      last_credit_pull_d\n      last_fico_range_high\n      last_fico_range_low\n      collections_12_mths_ex_med\n      mths_since_last_major_derog\n      acc_now_delinq\n    \n  \n  \n    \n      0\n      Feb-2016\n      684\n      680\n      0\n      NaN\n      0\n    \n    \n      1\n      Feb-2016\n      679\n      675\n      0\n      59.0\n      0\n    \n    \n      2\n      Dec-2015\n      539\n      535\n      0\n      NaN\n      0\n    \n    \n      3\n      Feb-2016\n      704\n      700\n      0\n      36.0\n      0\n    \n    \n      4\n      Feb-2016\n      684\n      680\n      0\n      NaN\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst we need to convert, last credit pull day to a numeric value as days since lst credit pull. Let us find if there are any NA values.\n\n\n\n\n\n\nIn\u00a0[29]:\n\n    \nprint(\"No. of Data with NA values = {}\".format(len(df.last_credit_pull_d) - df.last_credit_pull_d.count()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo. of Data with NA values = 27\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will replace these NA values with, Day corresponding with the oldest date of their account, i.e. now - credit history date.\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \ndf.last_credit_pull_d.fillna(\"Jan-1980\", inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[31]:\n\n    \ndf[\"last_credit_pull_d\"] = pd.to_datetime(df.last_credit_pull_d, format=\"%b-%Y\")\ndf['last_credit_pull_d'] = (now - df['last_credit_pull_d']).dt.days.divide(30).astype(\"int64\")\ndf[df['last_credit_pull_d'] >= 7000].last_credit_pull_d = df[df['last_credit_pull_d'] >= 7000].credit_age\n\n\n\n\n\n\n\n\n\n\n\n\nLet us compare last fico score to the overall fico score.\n\n\n\n\n\n\nIn\u00a0[32]:\n\n    \nsns.pairplot(df, vars=[\"last_fico_range_high\", \"last_fico_range_low\", \"fico\"], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[32]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e5004d828>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs before, last fico high and low scores are correlated, and also with overall fico score, and hence we can get rid of these.\n\n\n\n\n\n\nIn\u00a0[33]:\n\n    \ndf.drop(['last_fico_range_high'],1, inplace=True)\ndf.drop(['last_fico_range_low'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also get of \"collections_12_mths_ex_med\" column as this corresponds only to the current state of loan. Other two variables, \"mths_since_last_major_derog\" and \"acc_now_delinq\" should have no additional impact than ones already considered.\n\n\n\n\n\n\nIn\u00a0[34]:\n\n    \ndf.drop(['collections_12_mths_ex_med'],1, inplace=True)\ndf.drop(['mths_since_last_major_derog'],1, inplace=True)\ndf.drop(['acc_now_delinq'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[35]:\n\n    \ndf.ix[:4,19:29]\n\n\n\n\n\n\n\n\n\n\n\nOut[35]:\n\n\n\n\n\n  \n    \n      \n      tot_coll_amt\n      tot_cur_bal\n      open_acc_6m\n      open_il_6m\n      open_il_12m\n      open_il_24m\n      mths_since_rcnt_il\n      total_bal_il\n      il_util\n      open_rv_12m\n    \n  \n  \n    \n      0\n      0\n      149140\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      0\n      162110\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      0\n      64426\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      0\n      261815\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      0\n      38566\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain, we can go on and delete all the columns that are related to only the current states of loans, including the ones with large amount of missing data.\n\n\n\n\n\n\nIn\u00a0[36]:\n\n    \ndf.drop(['tot_coll_amt'],1, inplace=True)\ndf.drop(['open_acc_6m'],1, inplace=True)\ndf.drop(['tot_cur_bal'],1, inplace=True)\ndf.drop(['open_il_6m'],1, inplace=True)\ndf.drop(['open_il_12m'],1, inplace=True)\ndf.drop(['open_il_24m'],1, inplace=True)\ndf.drop(['mths_since_rcnt_il'],1, inplace=True)\ndf.drop(['total_bal_il'],1, inplace=True)\ndf.drop(['il_util'],1, inplace=True)\ndf.drop(['open_rv_12m'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[37]:\n\n    \ndf.ix[:4,19:29]\n\n\n\n\n\n\n\n\n\n\n\nOut[37]:\n\n\n\n\n\n  \n    \n      \n      open_rv_24m\n      max_bal_bc\n      all_util\n      total_rev_hi_lim\n      inq_fi\n      total_cu_tl\n      inq_last_12m\n      acc_open_past_24mths\n      avg_cur_bal\n      bc_open_to_buy\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      184500\n      NaN\n      NaN\n      NaN\n      5\n      29828.0\n      9525.0\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      19400\n      NaN\n      NaN\n      NaN\n      7\n      9536.0\n      7599.0\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      18300\n      NaN\n      NaN\n      NaN\n      6\n      5857.0\n      332.0\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      5700\n      NaN\n      NaN\n      NaN\n      2\n      32727.0\n      0.0\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      27600\n      NaN\n      NaN\n      NaN\n      8\n      3214.0\n      6494.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of these variables, only \"avg_cur_bal\" is viable additional feature for our model. We will also look at the distribution of average current balance. However, in order to use it correctly, we need to if there are any NA values in to and replace them correctly.\n\n\n\n\n\n\nIn\u00a0[38]:\n\n    \ndf.drop(['open_rv_24m'],1, inplace=True)\ndf.drop(['max_bal_bc'],1, inplace=True)\ndf.drop(['all_util'],1, inplace=True)\ndf.drop(['inq_fi'],1, inplace=True)\ndf.drop(['total_cu_tl'],1, inplace=True)\ndf.drop(['inq_last_12m'],1, inplace=True)\ndf.drop(['acc_open_past_24mths'],1, inplace=True)\ndf.drop(['bc_open_to_buy'],1, inplace=True)\ndf.drop(['total_rev_hi_lim'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[39]:\n\n    \nprint(\"No. of Data with NA values = {}\".format(len(df.avg_cur_bal) - df.avg_cur_bal.count()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo. of Data with NA values = 6\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[40]:\n\n    \ndf.avg_cur_bal.fillna(df.avg_cur_bal.min(), inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[41]:\n\n    \ng = sns.pairplot(df, vars=[\"avg_cur_bal\", \"int_rate\"], hue=\"loan_status\")\nfor ax in g.axes.flat:  \n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[42]:\n\n    \ndf.ix[:4,20:28]\n\n\n\n\n\n\n\n\n\n\n\nOut[42]:\n\n\n\n\n\n  \n    \n      \n      bc_util\n      chargeoff_within_12_mths\n      delinq_amnt\n      mo_sin_old_il_acct\n      mo_sin_old_rev_tl_op\n      mo_sin_rcnt_rev_tl_op\n      mo_sin_rcnt_tl\n      mort_acc\n    \n  \n  \n    \n      0\n      4.7\n      0\n      0\n      103.0\n      244\n      1\n      1\n      0\n    \n    \n      1\n      41.5\n      0\n      0\n      76.0\n      290\n      1\n      1\n      1\n    \n    \n      2\n      93.2\n      0\n      0\n      137.0\n      148\n      8\n      8\n      0\n    \n    \n      3\n      103.2\n      0\n      0\n      16.0\n      170\n      21\n      16\n      5\n    \n    \n      4\n      69.2\n      0\n      0\n      183.0\n      265\n      23\n      3\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilar to before, we can again get rid of variables that will not make significant impact on our model. Then look at the pair-wise effect of rest of them. We will also replace NAs with the mean values.\n\n\n\n\n\n\nIn\u00a0[43]:\n\n    \ndf.drop(['chargeoff_within_12_mths'],1, inplace=True)\ndf.drop(['delinq_amnt'],1, inplace=True)\ndf.drop(['mo_sin_old_il_acct'],1, inplace=True)\ndf.drop(['mo_sin_old_rev_tl_op'],1, inplace=True)\ndf.drop(['mo_sin_rcnt_rev_tl_op'],1, inplace=True)\ndf.drop(['mo_sin_rcnt_tl'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[44]:\n\n    \ndf.bc_util.fillna(df.bc_util.min(), inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[45]:\n\n    \nsns.pairplot(df, vars=[\"bc_util\", \"int_rate\"], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[45]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4e1ab550>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[46]:\n\n    \ndf.ix[:4,21:27]\n\n\n\n\n\n\n\n\n\n\n\nOut[46]:\n\n\n\n\n\n  \n    \n      \n      mort_acc\n      mths_since_recent_bc\n      mths_since_recent_bc_dlq\n      mths_since_recent_inq\n      mths_since_recent_revol_delinq\n      num_accts_ever_120_pd\n    \n  \n  \n    \n      0\n      0\n      47.0\n      NaN\n      NaN\n      NaN\n      0\n    \n    \n      1\n      1\n      5.0\n      42.0\n      1.0\n      42.0\n      4\n    \n    \n      2\n      0\n      17.0\n      NaN\n      3.0\n      NaN\n      0\n    \n    \n      3\n      5\n      21.0\n      17.0\n      1.0\n      17.0\n      1\n    \n    \n      4\n      0\n      24.0\n      NaN\n      17.0\n      NaN\n      0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this list only variable of our interest is number of mortgage accounts.\n\n\n\n\n\n\nIn\u00a0[47]:\n\n    \nsns.pairplot(df, vars=[\"mort_acc\", \"int_rate\"], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[47]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4dd0ed30>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[48]:\n\n    \ndf.drop(['mths_since_recent_bc'],1, inplace=True)\ndf.drop(['mths_since_recent_bc_dlq'],1, inplace=True)\ndf.drop(['mths_since_recent_inq'],1, inplace=True)\ndf.drop(['mths_since_recent_revol_delinq'],1, inplace=True)\ndf.drop(['num_accts_ever_120_pd'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[49]:\n\n    \ndf.ix[:4,22:31]\n\n\n\n\n\n\n\n\n\n\n\nOut[49]:\n\n\n\n\n\n  \n    \n      \n      num_actv_bc_tl\n      num_actv_rev_tl\n      num_bc_sats\n      num_bc_tl\n      num_il_tl\n      num_op_rev_tl\n      num_rev_accts\n      num_rev_tl_bal_gt_0\n      num_sats\n    \n  \n  \n    \n      0\n      1\n      4\n      1\n      2\n      8\n      5\n      9\n      4\n      6\n    \n    \n      1\n      6\n      9\n      7\n      18\n      2\n      14\n      32\n      9\n      17\n    \n    \n      2\n      1\n      4\n      1\n      4\n      12\n      4\n      8\n      4\n      11\n    \n    \n      3\n      3\n      5\n      3\n      5\n      1\n      5\n      7\n      5\n      8\n    \n    \n      4\n      4\n      7\n      5\n      16\n      17\n      8\n      26\n      7\n      12\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll of these variables are related to some kind of number of accounts. Lets take a look at their inter-dependence.\n\n\n\n\n\n\nIn\u00a0[50]:\n\n    \nsns.pairplot(df, vars=[\"num_actv_bc_tl\", \"num_actv_rev_tl\", \"num_bc_sats\", \"num_bc_tl\",\n                       \"num_op_rev_tl\"], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[50]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4de95208>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[51]:\n\n    \nsns.pairplot(df, vars=[\"num_op_rev_tl\", \"num_il_tl\", \"num_rev_accts\", \"num_rev_tl_bal_gt_0\",\n                       \"num_sats\"], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[51]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4caaedd8>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets also look at the remaining \"num\" of account variables.\n\n\n\n\n\n\nIn\u00a0[52]:\n\n    \nsns.pairplot(df, vars=[\"num_sats\", 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m'], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[52]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4bf1b2b0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see that first 9 sets of variables are all quite strongly correlated. The best set to consider for our model could be a sum of a subset of these. Lets consider a new variable, i.e. sum of all types of opened loan accounts, consisting of num_actv_bc_tl, num_actv_rev_tl, num_rev_tl_bal_gt_0, and num_tl_90g_dpd_24m. Additionally, we should also keep no. of accounts open in last year as a variable, i.e. num_tl_op_past_12m.\n\n\n\n\n\n\nIn\u00a0[53]:\n\n    \ndf['num_accounts'] = df['num_actv_bc_tl'] + df['num_actv_rev_tl'] + df['num_rev_tl_bal_gt_0'] + df['num_tl_90g_dpd_24m']\n\n\n\n\n\n\n\n\n\nIn\u00a0[54]:\n\n    \ndropped_vars=[\"num_actv_bc_tl\", \"num_actv_rev_tl\", \"num_bc_sats\", \"num_bc_tl\", \"num_op_rev_tl\", \"num_il_tl\",\n      \"num_rev_accts\", \"num_rev_tl_bal_gt_0\", \"num_sats\", 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'num_tl_120dpd_2m']\ndf.drop(dropped_vars,1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[55]:\n\n    \ndf.ix[:4,24:32]\n\n\n\n\n\n\n\n\n\n\n\nOut[55]:\n\n\n\n\n\n  \n    \n      \n      percent_bc_gt_75\n      pub_rec_bankruptcies\n      tax_liens\n      tot_hi_cred_lim\n      total_bal_ex_mort\n      total_bc_limit\n      total_il_high_credit_limit\n      credit_age\n    \n  \n  \n    \n      0\n      0.0\n      0\n      0\n      196500\n      149140\n      10000\n      12000\n      264\n    \n    \n      1\n      14.3\n      0\n      0\n      179407\n      15030\n      13000\n      11325\n      324\n    \n    \n      2\n      100.0\n      0\n      0\n      82331\n      64426\n      4900\n      64031\n      167\n    \n    \n      3\n      100.0\n      0\n      0\n      368700\n      18007\n      4400\n      18000\n      189\n    \n    \n      4\n      60.0\n      0\n      0\n      52490\n      38566\n      21100\n      24890\n      286\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOut of these 8 remaining features, lets first focus on first two percentages. We will first examining their role of loan status. percent_bc_gt_75 has very high amount of missing data and hence we will remove it from consideration.\n\n\n\n\n\n\nIn\u00a0[56]:\n\n    \nsns.pairplot(df, vars=[\"pct_tl_nvr_dlq\", 'int_rate'], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[56]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4b63e860>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe find even pct_tl_nvr_dlq to be not of much help!\n\n\n\n\n\n\nIn\u00a0[57]:\n\n    \ndf.drop(['pct_tl_nvr_dlq', 'percent_bc_gt_75'],1, inplace=True)\n\n\n\n\n\n\n\n\n\nIn\u00a0[58]:\n\n    \nsns.pairplot(df, vars=[\"pub_rec_bankruptcies\", 'tax_liens'], hue=\"loan_status\")\n\n\n\n\n\n\n\n\n\n\n\nOut[58]:\n\n\n\n\n<seaborn.axisgrid.PairGrid at 0x7f2e4b357e10>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis clearly shows a strong relationship between loan status and these features and hence need to be an integral part of the model.\nFinally let us take a look at the remaining total credit/balance related features.\n\n\n\n\n\n\nIn\u00a0[59]:\n\n    \ng = sns.pairplot(df, vars=[\"tot_hi_cred_lim\", 'total_bal_ex_mort', 'total_bc_limit',\n                       'total_il_high_credit_limit'], hue=\"loan_status\")\nfor ax in g.axes.flat:  \n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see a clear effect of outliers on low status of loan on all these variables. However, we can also see a strong correlation between total_il_high_credit_limit, tot_hi_cred_lim and total_bal_ex_mort. Let us choose only tot_hi_cred_lim to be in out feature set.\n\n\n\n\n\n\nIn\u00a0[60]:\n\n    \ndf.drop(['total_il_high_credit_limit', 'total_bal_ex_mort'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nLet us take a final look at all the remaining variables in our data.\n\n\n\n\n\n\nIn\u00a0[61]:\n\n    \nprint(df.columns)\nprint(df.columns.shape)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex(['loan_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'grade',\n       'sub_grade', 'emp_length', 'home_ownership', 'annual_inc',\n       'verification_status', 'loan_status', 'purpose', 'zip_code', 'dti',\n       'delinq_2yrs', 'revol_bal', 'revol_util', 'total_acc',\n       'last_credit_pull_d', 'avg_cur_bal', 'bc_util', 'mort_acc',\n       'num_tl_op_past_12m', 'pub_rec_bankruptcies', 'tax_liens',\n       'tot_hi_cred_lim', 'total_bc_limit', 'credit_age', 'fico',\n       'num_accounts'],\n      dtype='object')\n(30,)\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us stop here for this post. We will continue our model creation. We will save our pandas object as pickle and then catch up from there.\n\n\n\n\n\n\nIn\u00a0[62]:\n\n    \ndf.to_pickle(\"/home/ssingh/LendingClubData/Part2.pickle\")",
      "tags": "Data Science,EDA,Machine Learning,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/lcEDA2/"
    },
    {
      "title": "EDA of Lending Club Data",
      "text": "We will first look at various aspects of the LendingClub data using techniques of Exploratory Data Analysis (EDA).\nPlease look at my past post for finding further details on EDA techniques.\n\nDifferent data files for this analysis have already been downloaded in the current folder.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nimport warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n\n\n\n\n\n\n\n\n\n\nLet's also take a quick look at the data via shell scripts (file size, head, line count, column count).\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \n!du -h /home/ssingh/LendingClubData/LoanStats3c_securev1.csv\n#!tail -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv\n#!head -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n181M\t/home/ssingh/LendingClubData/LoanStats3c_securev1.csv\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamining the data we see that most of feature names are intuitive. We can get the specifics from the provided data dictionary.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \n!wc -l < /home/ssingh/LendingClubData/LoanStats3c_securev1.csv\n!head -2 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv | sed 's/[,]//g' | wc -c\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n235633\n116\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the above analysis, we find that we have a total of 235633-2-2 = 235659 rows and 116 - 1 = 115 columns of data! Let us first look at the detailed description of columns from the dictionary of the data.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndf = pd.read_csv(\"/home/ssingh/LendingClubData/LoanStatsDict.csv\", sep=\",\", engine='c', encoding = \"ISO-8859-1\", na_filter=False)\ndf = df.ix[1:,0:2]\nfrom IPython.display import HTML\nHTML(df.to_html())\n\n\n\n\n\n\n\n\n\n\n\nOut[5]:\n\n\n\n\n  \n    \n      \n      LoanStatNew\n      Description\n    \n  \n  \n    \n      1\n      acc_open_past_24mths\n      Number of trades opened in past 24 months.\n    \n    \n      2\n      addr_state\n      The state provided by the borrower in the loan...\n    \n    \n      3\n      all_util\n      Balance to credit limit on all trades\n    \n    \n      4\n      annual_inc\n      The self-reported annual income provided by th...\n    \n    \n      5\n      annual_inc_joint\n      The combined self-reported annual income provi...\n    \n    \n      6\n      application_type\n      Indicates whether the loan is an individual ap...\n    \n    \n      7\n      avg_cur_bal\n      Average current balance of all accounts\n    \n    \n      8\n      bc_open_to_buy\n      Total open to buy on revolving bankcards.\n    \n    \n      9\n      bc_util\n      Ratio of total current balance to high credit/...\n    \n    \n      10\n      chargeoff_within_12_mths\n      Number of charge-offs within 12 months\n    \n    \n      11\n      collection_recovery_fee\n      post charge off collection fee\n    \n    \n      12\n      collections_12_mths_ex_med\n      Number of collections in 12 months excluding m...\n    \n    \n      13\n      delinq_2yrs\n      The number of 30+ days past-due incidences of ...\n    \n    \n      14\n      delinq_amnt\n      The past-due amount owed for the accounts on w...\n    \n    \n      15\n      desc\n      Loan description provided by the borrower\n    \n    \n      16\n      dti\n      A ratio calculated using the borrower\u00d5s total ...\n    \n    \n      17\n      dti_joint\n      A ratio calculated using the co-borrowers' tot...\n    \n    \n      18\n      earliest_cr_line\n      The month the borrower's earliest reported cre...\n    \n    \n      19\n      emp_length\n      Employment length in years. Possible values ar...\n    \n    \n      20\n      emp_title\n      The job title supplied by the Borrower when ap...\n    \n    \n      21\n      fico_range_high\n      The upper boundary range the borrower\u00d5s FICO a...\n    \n    \n      22\n      fico_range_low\n      The lower boundary range the borrower\u00d5s FICO a...\n    \n    \n      23\n      funded_amnt\n      The total amount committed to that loan at tha...\n    \n    \n      24\n      funded_amnt_inv\n      The total amount committed by investors for th...\n    \n    \n      25\n      grade\n      LC assigned loan grade\n    \n    \n      26\n      home_ownership\n      The home ownership status provided by the borr...\n    \n    \n      27\n      id\n      A unique LC assigned ID for the loan listing.\n    \n    \n      28\n      il_util\n      Ratio of total current balance to high credit/...\n    \n    \n      29\n      initial_list_status\n      The initial listing status of the loan. Possib...\n    \n    \n      30\n      inq_fi\n      Number of personal finance inquiries\n    \n    \n      31\n      inq_last_12m\n      Number of credit inquiries in past 12 months\n    \n    \n      32\n      inq_last_6mths\n      The number of inquiries in past 6 months (excl...\n    \n    \n      33\n      installment\n      The monthly payment owed by the borrower if th...\n    \n    \n      34\n      int_rate\n      Interest Rate on the loan\n    \n    \n      35\n      issue_d\n      The month which the loan was funded\n    \n    \n      36\n      last_credit_pull_d\n      The most recent month LC pulled credit for thi...\n    \n    \n      37\n      last_fico_range_high\n      The upper boundary range the borrower\u00d5s last F...\n    \n    \n      38\n      last_fico_range_low\n      The lower boundary range the borrower\u00d5s last F...\n    \n    \n      39\n      last_pymnt_amnt\n      Last total payment amount received\n    \n    \n      40\n      last_pymnt_d\n      Last month payment was received\n    \n    \n      41\n      loan_amnt\n      The listed amount of the loan applied for by t...\n    \n    \n      42\n      loan_status\n      Current status of the loan\n    \n    \n      43\n      max_bal_bc\n      Maximum current balance owed on all revolving ...\n    \n    \n      44\n      member_id\n      A unique LC assigned Id for the borrower member.\n    \n    \n      45\n      mo_sin_old_il_acct\n      Months since oldest bank installment account o...\n    \n    \n      46\n      mo_sin_old_rev_tl_op\n      Months since oldest revolving account opened\n    \n    \n      47\n      mo_sin_rcnt_rev_tl_op\n      Months since most recent revolving account opened\n    \n    \n      48\n      mo_sin_rcnt_tl\n      Months since most recent account opened\n    \n    \n      49\n      mort_acc\n      Number of mortgage accounts.\n    \n    \n      50\n      mths_since_last_delinq\n      The number of months since the borrower's last...\n    \n    \n      51\n      mths_since_last_major_derog\n      Months since most recent 90-day or worse rating\n    \n    \n      52\n      mths_since_last_record\n      The number of months since the last public rec...\n    \n    \n      53\n      mths_since_rcnt_il\n      Months since most recent installment accounts ...\n    \n    \n      54\n      mths_since_recent_bc\n      Months since most recent bankcard account opened.\n    \n    \n      55\n      mths_since_recent_bc_dlq\n      Months since most recent bankcard delinquency\n    \n    \n      56\n      mths_since_recent_inq\n      Months since most recent inquiry.\n    \n    \n      57\n      mths_since_recent_revol_delinq\n      Months since most recent revolving delinquency.\n    \n    \n      58\n      next_pymnt_d\n      Next scheduled payment date\n    \n    \n      59\n      num_accts_ever_120_pd\n      Number of accounts ever 120 or more days past due\n    \n    \n      60\n      num_actv_bc_tl\n      Number of currently active bankcard accounts\n    \n    \n      61\n      num_actv_rev_tl\n      Number of currently active revolving trades\n    \n    \n      62\n      num_bc_sats\n      Number of satisfactory bankcard accounts\n    \n    \n      63\n      num_bc_tl\n      Number of bankcard accounts\n    \n    \n      64\n      num_il_tl\n      Number of installment accounts\n    \n    \n      65\n      num_op_rev_tl\n      Number of open revolving accounts\n    \n    \n      66\n      num_rev_accts\n      Number of revolving accounts\n    \n    \n      67\n      num_rev_tl_bal_gt_0\n      Number of revolving trades with balance >0\n    \n    \n      68\n      num_sats\n      Number of satisfactory accounts\n    \n    \n      69\n      num_tl_120dpd_2m\n      Number of accounts currently 120 days past due...\n    \n    \n      70\n      num_tl_30dpd\n      Number of accounts currently 30 days past due ...\n    \n    \n      71\n      num_tl_90g_dpd_24m\n      Number of accounts 90 or more days past due in...\n    \n    \n      72\n      num_tl_op_past_12m\n      Number of accounts opened in past 12 months\n    \n    \n      73\n      open_acc\n      The number of open credit lines in the borrowe...\n    \n    \n      74\n      open_acc_6m\n      Number of open trades in last 6 months\n    \n    \n      75\n      open_il_12m\n      Number of installment accounts opened in past ...\n    \n    \n      76\n      open_il_24m\n      Number of installment accounts opened in past ...\n    \n    \n      77\n      open_il_6m\n      Number of currently active installment trades\n    \n    \n      78\n      open_rv_12m\n      Number of revolving trades opened in past 12 m...\n    \n    \n      79\n      open_rv_24m\n      Number of revolving trades opened in past 24 m...\n    \n    \n      80\n      out_prncp\n      Remaining outstanding principal for total amou...\n    \n    \n      81\n      out_prncp_inv\n      Remaining outstanding principal for portion of...\n    \n    \n      82\n      pct_tl_nvr_dlq\n      Percent of trades never delinquent\n    \n    \n      83\n      percent_bc_gt_75\n      Percentage of all bankcard accounts > 75% of l...\n    \n    \n      84\n      policy_code\n      publicly available policy_code=1 new products ...\n    \n    \n      85\n      pub_rec\n      Number of derogatory public records\n    \n    \n      86\n      pub_rec_bankruptcies\n      Number of public record bankruptcies\n    \n    \n      87\n      purpose\n      A category provided by the borrower for the lo...\n    \n    \n      88\n      pymnt_plan\n      Indicates if a payment plan has been put in pl...\n    \n    \n      89\n      recoveries\n      post charge off gross recovery\n    \n    \n      90\n      revol_bal\n      Total credit revolving balance\n    \n    \n      91\n      revol_util\n      Revolving line utilization rate, or the amount...\n    \n    \n      92\n      sub_grade\n      LC assigned loan subgrade\n    \n    \n      93\n      tax_liens\n      Number of tax liens\n    \n    \n      94\n      term\n      The number of payments on the loan. Values are...\n    \n    \n      95\n      title\n      The loan title provided by the borrower\n    \n    \n      96\n      tot_coll_amt\n      Total collection amounts ever owed\n    \n    \n      97\n      tot_cur_bal\n      Total current balance of all accounts\n    \n    \n      98\n      tot_hi_cred_lim\n      Total high credit/credit limit\n    \n    \n      99\n      total_acc\n      The total number of credit lines currently in ...\n    \n    \n      100\n      total_bal_ex_mort\n      Total credit balance excluding mortgage\n    \n    \n      101\n      total_bal_il\n      Total current balance of all installment accounts\n    \n    \n      102\n      total_bc_limit\n      Total bankcard high credit/credit limit\n    \n    \n      103\n      total_cu_tl\n      Number of finance trades\n    \n    \n      104\n      total_il_high_credit_limit\n      Total installment high credit/credit limit\n    \n    \n      105\n      total_pymnt\n      Payments received to date for total amount funded\n    \n    \n      106\n      total_pymnt_inv\n      Payments received to date for portion of total...\n    \n    \n      107\n      total_rec_int\n      Interest received to date\n    \n    \n      108\n      total_rec_late_fee\n      Late fees received to date\n    \n    \n      109\n      total_rec_prncp\n      Principal received to date\n    \n    \n      110\n      total_rev_hi_lim \u00ca\n      Total revolving high credit/credit limit\n    \n    \n      111\n      url\n      URL for the LC page with listing data.\n    \n    \n      112\n      verification_status\n      Indicates if income was verified by LC, not ve...\n    \n    \n      113\n      verified_status_joint\n      Indicates if the co-borrowers' joint income wa...\n    \n    \n      114\n      zip_code\n      The first 3 numbers of the zip code provided b...\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets choose some of the most important variables from these.\nThe Response Variable:\n\nInterest Rate (int_rate)\n\nAnd some of possible important factors are:\n\nAnnual Income (annual_inc)\nState (addr_state)\nPurpose (purpose)\nDescription for Loan (desc)\nAmount Requested (loan_amount)\nAmount Funded (funded_amnt)\nLoan Length (term)\nDebt Income Ratio (dti)\nHome Ownership status (home_ownership)\nFICO high (fico_range_high)\nFICO low (fico_range_low)\nLast FICO low (last_fico_range_low)\nLast FICO high (last_fico_range_high)\nAverage current balance (avg_cur_bal)\nCharge Offs in last Year (chargeoff_within_12_mths)\nNumber of 30+ days past-due incidences (delinq_2yrs)\nEmployment Length (emp_length)\nNo. of Credit Inquiries (inq_last_6mths)\nMaximum current balance owed on all revolving (max_bal_bc)\nTotal credit revolving balance (revol_bal)\nLC Verification status (verification_status)\nRevolving line utilization rate (revol_util)\nPercentage of account never delinquent (pct_tl_nvr_dlq)\nMonths since most recent 90-day or worse rating (mths_since_last_major_derog)\nTotal Credit Balance (total_bal_ex_mort)\n\nWe will first look at effects of some of these variables using EDA.\nLater, if we find any need to use some additional variables, we will revisit this list.\nFirst, lets load our data as a Pandas data frame:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \ndf = pd.read_csv(\"/home/ssingh/LendingClubData/LoanStats3c_securev1.csv\", skiprows=1, skipfooter=2)\ndf.info(verbose = False)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 235629 entries, 0 to 235628\nColumns: 115 entries, id to total_il_high_credit_limit\ndtypes: float64(44), int64(47), object(24)\nmemory usage: 206.7+ MB\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \ndf.head(3)\n\n\n\n\n\n\n\n\n\n\n\nOut[6]:\n\n\n\n\n\n  \n    \n      \n      id\n      member_id\n      loan_amnt\n      funded_amnt\n      funded_amnt_inv\n      term\n      int_rate\n      installment\n      grade\n      sub_grade\n      ...\n      num_tl_90g_dpd_24m\n      num_tl_op_past_12m\n      pct_tl_nvr_dlq\n      percent_bc_gt_75\n      pub_rec_bankruptcies\n      tax_liens\n      tot_hi_cred_lim\n      total_bal_ex_mort\n      total_bc_limit\n      total_il_high_credit_limit\n    \n  \n  \n    \n      0\n      38098114\n      40860827\n      15000\n      15000\n      15000\n      60 months\n      12.39%\n      336.64\n      C\n      C1\n      ...\n      0\n      4\n      100.0\n      0.0\n      0\n      0\n      196500\n      149140\n      10000\n      12000\n    \n    \n      1\n      36805548\n      39558264\n      10400\n      10400\n      10400\n      36 months\n      6.99%\n      321.08\n      A\n      A3\n      ...\n      0\n      4\n      83.3\n      14.3\n      0\n      0\n      179407\n      15030\n      13000\n      11325\n    \n    \n      2\n      37662224\n      40425321\n      7650\n      7650\n      7650\n      36 months\n      13.66%\n      260.20\n      C\n      C3\n      ...\n      0\n      2\n      100.0\n      100.0\n      0\n      0\n      82331\n      64426\n      4900\n      64031\n    \n  \n\n3 rows \u00d7 115 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLets us remove columns/data that is redundant for prediction of interest rates. For example, id, member_id for sure are of no importance to us. Let's work through the columns in batches to keep the cognitive burden low:\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n# .ix[row slice, column slice] \ndf.ix[:4,:7]\n\n\n\n\n\n\n\n\n\n\n\nOut[7]:\n\n\n\n\n\n  \n    \n      \n      id\n      member_id\n      loan_amnt\n      funded_amnt\n      funded_amnt_inv\n      term\n      int_rate\n    \n  \n  \n    \n      0\n      38098114\n      40860827\n      15000\n      15000\n      15000\n      60 months\n      12.39%\n    \n    \n      1\n      36805548\n      39558264\n      10400\n      10400\n      10400\n      36 months\n      6.99%\n    \n    \n      2\n      37662224\n      40425321\n      7650\n      7650\n      7650\n      36 months\n      13.66%\n    \n    \n      3\n      37612354\n      40375473\n      12800\n      12800\n      12800\n      60 months\n      17.14%\n    \n    \n      4\n      37822187\n      40585251\n      9600\n      9600\n      9600\n      36 months\n      13.66%\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe won't need id or member_id as it has no real predictive power so we can drop them from this table\nint_rate was loaded as an object data type instead of float due to the '%' character. Let's strip that out and convert the column type. And Also do similar transformation for term variable to get rid of months.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \ndf.drop(['id','member_id'], 1, inplace=True)\ndf.int_rate = pd.Series(df.int_rate).str.replace('%', '').astype(float)\ndf['term'].replace(to_replace='[0-9]+', value='', inplace=True, regex=True)\ndf['term'] = df['term'].convert_objects(convert_numeric=True)\n\n\n\n\n\n\n\n\n\n\n\n\nMoving on to next columns:\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \ndf.ix[:4,5:15]\n\n\n\n\n\n\n\n\n\n\n\nOut[9]:\n\n\n\n\n\n  \n    \n      \n      installment\n      grade\n      sub_grade\n      emp_title\n      emp_length\n      home_ownership\n      annual_inc\n      verification_status\n      issue_d\n      loan_status\n    \n  \n  \n    \n      0\n      336.64\n      C\n      C1\n      MANAGEMENT\n      10+ years\n      RENT\n      78000.0\n      Source Verified\n      Dec-2014\n      Current\n    \n    \n      1\n      321.08\n      A\n      A3\n      Truck Driver Delivery Personel\n      8 years\n      MORTGAGE\n      58000.0\n      Not Verified\n      Dec-2014\n      Current\n    \n    \n      2\n      260.20\n      C\n      C3\n      Technical Specialist\n      < 1 year\n      RENT\n      50000.0\n      Source Verified\n      Dec-2014\n      Charged Off\n    \n    \n      3\n      319.08\n      D\n      D4\n      Senior Sales Professional\n      10+ years\n      MORTGAGE\n      125000.0\n      Verified\n      Dec-2014\n      Current\n    \n    \n      4\n      326.53\n      C\n      C3\n      Admin Specialist\n      10+ years\n      RENT\n      69000.0\n      Source Verified\n      Dec-2014\n      Fully Paid\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt first, it seems we employment title should be important. Let us first have a look at how many unique values we have for these. We would like to convert emp_length into an integer variable.\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \nprint(df.emp_title.value_counts().head())\nprint(df.emp_title.value_counts().tail())\ndf.emp_title.unique().shape\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTeacher             4569\nManager             3772\nRegistered Nurse    1960\nRN                  1816\nSupervisor          1663\nName: emp_title, dtype: int64\nCare Aid                         1\nDeputy Probation                 1\nFront office staff               1\nfactor                           1\nIndependent Contractor/Driver    1\nName: emp_title, dtype: int64\n\n\n\n\n\nOut[10]:\n\n\n\n\n(75353,)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is just too many. Unless, we do some semantics based grouping of these titles, we would not be able to get any meaningful data out of this. If you think harder, this should be highly correlated with income. Just for the purpose of loan, it is highly unlikely (not impossible though!) that a high paying job title would be different than another!\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \ndf.drop(['emp_title'], 1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nLet first look at unique value of emp_legth variable.\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \ndf.emp_length.value_counts()\n\n\n\n\n\n\n\n\n\n\n\nOut[12]:\n\n\n\n\n10+ years    79505\n2 years      20487\n3 years      18267\n< 1 year     17982\n1 year       14593\n4 years      13528\n7 years      13099\n5 years      13051\nn/a          12019\n8 years      11853\n6 years      11821\n9 years       9424\nName: emp_length, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us make this variable simple integers. We will replace na entries with 0, and all non numeric entries.\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \ndf.replace('n/a', np.nan, inplace=True)\ndf.emp_length.fillna(value=0,inplace=True)\ndf['emp_length'].replace(to_replace='<', value=0.0, inplace=True, regex=True)\ndf['emp_length'].replace(to_replace='[0-9]+', value='', inplace=True, regex=True)\ndf['emp_length'] = df['emp_length'].astype(int)\n\n\n\n\n\n\n\n\n\n\n\n\nLets us look at unique emp_length entries again:\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \ndf.emp_length.value_counts()\n\n\n\n\n\n\n\n\n\n\n\nOut[14]:\n\n\n\n\n10    79505\n0     30001\n2     20487\n3     18267\n1     14593\n4     13528\n7     13099\n5     13051\n8     11853\n6     11821\n9      9424\nName: emp_length, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe should convert verification status into three ordinal values. Lets see what are different possible values of the verification status. We will convert these to ordinals accordingly.\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \ndf.verification_status.value_counts()\n\n\n\n\n\n\n\n\n\n\n\nOut[15]:\n\n\n\n\nSource Verified    97741\nNot Verified       70659\nVerified           67229\nName: verification_status, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will assign the lowest rating to Not Verified and the highest rating to Verified.\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \ndf[\"verification_status\"] = df[\"verification_status\"].astype('category')\ndf[\"verification_status\"] = df[\"verification_status\"].cat.set_categories([\"Not Verified\", \"Source Verified\", \"Verified\"], ordered = True)\n\n\n\n\n\n\n\n\n\n\n\n\nLoan status and issue dates are not of any interest to us, as we only plan to use this data for making models that can predict interest rate. However, let us do some digging into this data to see some statistics of loans on Lending Club! Let us look at a histogram of different states of loans.\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nimport seaborn as sns\nsns.set()\nsns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\ntotal = float(len(df.index)) \nax = sns.countplot(x=\"loan_status\", data=df, palette=\"Set2\");\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x(), height+18, '%2.2f'%(height*100/total)+\"%\")\nplt.xticks(rotation=60)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also look at a histogram of number of loans issued based on the month of the year. Just for fun, we will also look for any correlation between the issue month of loans and their states.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \ndf['issue_d'].replace(to_replace='[A-Z,a-z]+', value='', inplace=True, regex=True)\nax = sns.countplot(x=\"issue_d\", data=df, palette=\"Set2\");\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x(), height+18, '%2.1f'%(height*100/total))\nplt.xticks(rotation=60)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we find that more loans are issued during the holidays seasons: Oct and July! Nothing surprising there.\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \ng = sns.factorplot(\"loan_status\", col=\"issue_d\", col_wrap=4, data=df,kind=\"count\", aspect=1.25)\n(g.set_axis_labels(\"\", \"\")\n .set_titles(\"{col_name}\")\n .set_xticklabels(rotation=60)\n .despine(left=True))\n\n\n\n\n\n\n\n\n\n\n\nOut[19]:\n\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fe6351a72b0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not find any obvious correlation between the issue date and their states at this time. Not interesting. Lets drop both issue date from our further analysis. Moving on to next set of columns.\nWe will also divide loan status into two categories: Low, and High risks.\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \ndefaultList = [\"Default\", \"Charged Off\", \"Late (31-120 days)\", \"Late (16-30 days)\", \"In Grace Period\"]\ndf.loc[df.loan_status.isin(defaultList), \"loan_status\"] = \"High\"\ngoodList = [\"Current\", \"Fully Paid\"]\ndf.loc[df.loan_status.isin(goodList), \"loan_status\"] = \"Low\"\n\n\n\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \ndf.drop(['issue_d'],1, inplace=True)\n\n\n\n\n\n\n\n\n\n\n\n\nLet us stop here for this post. We will continue our EDA analysis in the next. We will save our pandas object as pickle and then catch up from there.\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ndf.to_pickle(\"/home/ssingh/LendingClubData/Part1.pickle\")",
      "tags": "Data Science,EDA,Machine Learning,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/lcEDA1/"
    },
    {
      "title": "Exploring Multiple Variables",
      "text": "In this section, we will continue re-using the data from the previous post based on Pseudo\nFacebook data from udacity.\n\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited tsv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport pandas as pd\nimport numpy as np\n\n#Read csv file\npf = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\", sep = '\\t')\n\ncats = ['userid', 'dob_day', 'dob_year', 'dob_month']\nfor col in pf.columns:\n    if col in cats:\n        pf[col] = pf[col].astype('category')\n\n#summarize data\npf.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n  RuntimeWarning)\n\n\n\n\n\nOut[1]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      userid\n      99003.0\n      99003\n      2.19354e+06\n      1\n      \n      \n      \n      \n      \n    \n    \n      age\n      99003.0\n      \n      \n      \n      37.2802\n      22.5897\n      13\n      28\n      113\n    \n    \n      dob_day\n      99003.0\n      31\n      1\n      7900\n      \n      \n      \n      \n      \n    \n    \n      dob_year\n      99003.0\n      101\n      1995\n      5196\n      \n      \n      \n      \n      \n    \n    \n      dob_month\n      99003.0\n      12\n      1\n      11772\n      \n      \n      \n      \n      \n    \n    \n      gender\n      98828.0\n      2\n      male\n      58574\n      \n      \n      \n      \n      \n    \n    \n      tenure\n      99001.0\n      \n      \n      \n      537.887\n      457.65\n      0\n      \n      3139\n    \n    \n      friend_count\n      99003.0\n      \n      \n      \n      196.351\n      387.304\n      0\n      82\n      4923\n    \n    \n      friendships_initiated\n      99003.0\n      \n      \n      \n      107.452\n      188.787\n      0\n      46\n      4144\n    \n    \n      likes\n      99003.0\n      \n      \n      \n      156.079\n      572.281\n      0\n      11\n      25111\n    \n    \n      likes_received\n      99003.0\n      \n      \n      \n      142.689\n      1387.92\n      0\n      8\n      261197\n    \n    \n      mobile_likes\n      99003.0\n      \n      \n      \n      106.116\n      445.253\n      0\n      4\n      25111\n    \n    \n      mobile_likes_received\n      99003.0\n      \n      \n      \n      84.1205\n      839.889\n      0\n      4\n      138561\n    \n    \n      www_likes\n      99003.0\n      \n      \n      \n      49.9624\n      285.56\n      0\n      0\n      14865\n    \n    \n      www_likes_received\n      99003.0\n      \n      \n      \n      58.5688\n      601.416\n      0\n      2\n      129953\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuing with our analysis from the last post, finding a relationship between age and friends counts, let us add gender to the equation.\nIn order to do this, we will first use groupby() and agg() to get group by data.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ndef groupByStats(df, groupCol, statsCol):\n    ''' return a dataframe with groupByCo\n        groupbyCol: a string or a list of strings for col names in df\n        statsCol: a string for col in df of which we need stats for\n    '''\n    \n    # Define the aggregation calculations\n    aggregations = {\n        statsCol: {\n            (statsCol+'_mean'): 'mean',\n            (statsCol+'_median'): 'median',\n            (statsCol+'_q25'): lambda x: np.percentile(x,25),\n            (statsCol+'_q75'): lambda x: np.percentile(x,75),\n            'n': 'count'\n        }\n    }\n    \n    df_group_by_groupCol = df.groupby(groupCol, as_index=False, group_keys=False).agg(aggregations)\n             \n    df_group_by_groupCol.columns = df_group_by_groupCol.columns.droplevel()\n    if isinstance(groupCol, list):\n        cols = groupCol + list(df_group_by_groupCol.columns)[len(groupCol):]\n    else:\n        cols = [groupCol] + list(df_group_by_groupCol.columns)[1:]\n\n    df_group_by_groupCol.columns = cols\n    return df_group_by_groupCol\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \npf_group_by_age_gender = groupByStats(pf[pf.gender.notnull()], ['gender', 'age'], 'friend_count')\npf_group_by_age_gender.head().T\n\n\n\n\n\n\n\n\n\n\n\nOut[3]:\n\n\n\n\n\n  \n    \n      \n      gender\n      age\n      friend_count_q75\n      n\n      friend_count_mean\n      friend_count_median\n      friend_count_q25\n    \n  \n  \n    \n      0\n      female\n      13\n      316.00\n      193\n      259.160622\n      148.0\n      39.0\n    \n    \n      1\n      female\n      14\n      410.50\n      847\n      362.428571\n      224.0\n      79.0\n    \n    \n      2\n      female\n      15\n      592.50\n      1139\n      538.681299\n      276.0\n      104.0\n    \n    \n      3\n      female\n      16\n      581.25\n      1238\n      519.514540\n      258.5\n      102.0\n    \n    \n      4\n      female\n      17\n      586.75\n      1236\n      538.994337\n      245.5\n      81.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we can use plots to compare friend counts across age and gender.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n%matplotlib inline\n\npf_group_by_age_gender['unit'] = \"U\"\nax = sns.tsplot(time='age', value='friend_count_median', condition='gender', unit='unit', data=pf_group_by_age_gender)\nplt.ylabel(\"Friend Count Median\")\nplt.xlim(10,113)\n\n\n\n\n\n\n\n\n\n\n\nOut[4]:\n\n\n\n\n(10, 113)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt would be helpful to analyze these differences in relative terms. So lets answer a different question - how many times more friends does the average female users have than average male users.\nTo answer the above question we will need to transform our data. Right now, our data is in \u201clong format\u201d - a row of data different values of different variables. We will need to convert this to a \u201cwide format\u201d - where we will create columns called 'male' and 'female', that will have median counts in them.\nThis can be done using the \u201cpivot()\u201d method of pandas.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \npf_group_by_age_gender_wide = pf_group_by_age_gender.pivot(index='age', columns='gender', values='friend_count_median')\npf_group_by_age_gender_wide.columns.name = ''\npf_group_by_age_gender_wide.reset_index(level=0, inplace=True)\npf_group_by_age_gender_wide.head()\n\n\n\n\n\n\n\n\n\n\n\nOut[5]:\n\n\n\n\n\n  \n    \n      \n      age\n      female\n      male\n    \n  \n  \n    \n      0\n      13\n      148.0\n      55.0\n    \n    \n      1\n      14\n      224.0\n      92.5\n    \n    \n      2\n      15\n      276.0\n      106.5\n    \n    \n      3\n      16\n      258.5\n      136.0\n    \n    \n      4\n      17\n      245.5\n      125.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can use the melt() function to convert a wide format data back to a long format data.\nNow lets plot ratio of female to male median friend counts.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \npf_group_by_age_gender_wide['female_male_ratio'] = pf_group_by_age_gender_wide.female/pf_group_by_age_gender_wide.male\npf_group_by_age_gender_wide['unit'] = \"u\"\nax = sns.tsplot(time='age', value='female_male_ratio', unit='unit', data=pf_group_by_age_gender_wide)\nplt.plot([13, 113], [1, 1], linewidth=2, color='r', linestyle='--')\nplt.ylabel(\"Female-Male Ratio\")\n\n\n\n\n\n\n\n\n\n\n\nOut[6]:\n\n\n\n\n<matplotlib.text.Text at 0x7fe86804b4a8>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this particular case of looking at multiple variables, it would make more sense to look at count of friends as function of tenure of Facebook as well. People with a longer tenure of Facebook account are likely to accumulate a larger number of friends.\nIn the following section we will look at friend count of females, males at different ages along with their Facebook tenure.\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \npf['year_joined'] = np.floor(2014 - pf['tenure']/365)\n\n\n\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \npf.year_joined.value_counts(dropna=False)\n\n\n\n\n\n\n\n\n\n\n\nOut[8]:\n\n\n\n\n 2013.0    43588\n 2012.0    33366\n 2011.0     9860\n 2010.0     5448\n 2009.0     4557\n 2008.0     1507\n 2007.0      581\n 2014.0       70\n 2006.0       15\n 2005.0        9\nNaN            2\nName: year_joined, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe tabular view of this shows the distribution. We will next use the cut() method to bin this variable.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \npf['year_joined'] = pd.cut(pf.year_joined, bins=[2004,2009,2011,2012,2014])\npf['year_joined'] = pf['year_joined'].astype('category')\n\n\n\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \npf['year_joined'].value_counts(dropna=False)\n\n\n\n\n\n\n\n\n\n\n\nOut[10]:\n\n\n\n\n(2012, 2014]    43658\n(2011, 2012]    33366\n(2009, 2011]    15308\n(2004, 2009]     6669\nNaN                 2\nName: year_joined, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us now plot all these variables together. In particular, we want to create a plot of friend counts vs age where a different line is shown for each bin of year joined.\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \npf_group_by_age = groupByStats(pf, ['age', 'year_joined'], 'friend_count')\npf_group_by_age.head().T\n\n\n\n\n\n\n\n\n\n\n\nOut[11]:\n\n\n\n\n\n  \n    \n      \n      age\n      year_joined\n      friend_count_q75\n      n\n      friend_count_mean\n      friend_count_median\n      friend_count_q25\n    \n  \n  \n    \n      0\n      13\n      (2009, 2011]\n      691.50\n      11\n      469.818182\n      362.0\n      124.50\n    \n    \n      1\n      13\n      (2011, 2012]\n      406.75\n      48\n      352.333333\n      248.5\n      145.25\n    \n    \n      2\n      13\n      (2012, 2014]\n      167.00\n      425\n      135.668235\n      63.0\n      18.00\n    \n    \n      3\n      14\n      (2009, 2011]\n      1033.75\n      28\n      860.928571\n      517.0\n      279.25\n    \n    \n      4\n      14\n      (2011, 2012]\n      383.00\n      449\n      350.812918\n      224.0\n      109.00\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \npf_group_by_age['unit'] = 'U'\nfig, ax = plt.subplots(figsize=(8,6))\ng = sns.tsplot(time='age', value='friend_count_median', unit='unit',\\\n               condition='year_joined', data=pf_group_by_age, ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOur initial hypothesis seems to be correct here - People with larger Facebook tenure tend to have higher friend counts. Lets us plot the mean of these bins and also the grand means of data.\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nfig, ax = plt.subplots(figsize=(8,6))\ng = sns.tsplot(time='age', value='friend_count_mean', unit='unit',\\\n               condition='year_joined', data=pf_group_by_age, ax=ax)\ng = pf_group_by_age.groupby('age', as_index=False).mean()\ng = g.plot(x='age', y='friend_count_mean', style='--', ax=ax)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSince the trend holds up after conditioning on the each of the buckets of year joined, we can increase our confidence that this observation isn\u2019t just an artifact. Let us look at this from a different angle.\nWe can define a variable called \u201cfriend rate\u201d, i.e. number of friends each person had on a per day basis.\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \ndf = pf.loc[pf['tenure'] >= 1]\n(df['friend_count']/df['tenure']).describe()\n\n\n\n\n\n\n\n\n\n\n\nOut[14]:\n\n\n\n\ncount    98931.000000\nmean         0.609609\nstd          2.557356\nmin          0.000000\n25%          0.077486\n50%          0.220486\n75%          0.565802\nmax        417.000000\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can now look at the effect of this variable in more detail using the following plot:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \ndf.is_copy = False\ns = df['friendships_initiated']/df['tenure']\ndf['friendships_initiated_per_tenure'] = s.astype('int64')\n\ng = sns.lmplot(x=\"tenure\", y=\"friendships_initiated_per_tenure\", hue=\"year_joined\", legend_out=False, \\\n               data=df, size=5, fit_reg=False, scatter_kws={'alpha': 0.5})\nplt.xlim(1,3400)\n\n\n\n\n\n\n\n\n\n\n\nOut[15]:\n\n\n\n\n(1, 3400)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows that people initiate less number of friends as they have longer tenure at Facebook.\nTill now, we have looked at different aspects of the Facebook data set. We will now use a new data set for some more detailed multivariate analysis, and then finally come back to the Facebook data set for comparison.\nWe are going to work with a data set describing household purchases of five flavors of Dannon Yogurt in 8 oz sizes. Their price is recorded with each purchase occasion. This yogurt data set has a quite different structure than our pseudo-Facebook data set. The synthetic Facebook data has one row per individual with that row giving their characteristics and counts of behaviors over a single period of time. On the other hand, the yogurt data has many rows per household, one for each purchase occasion. This kind of micro-data is often useful for answering different types of questions than we\u2019ve looked at so far.\nWe will start by loading the yogurt data set and then looking at its summary.\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nyo = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/yogurt.csv\")\n#summarize data\nyo.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\nOut[16]:\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      obs\n      2380.0\n      1.367797e+03\n      790.076032\n      1.0\n      1369.50\n      2743.00\n    \n    \n      id\n      2380.0\n      2.128592e+06\n      17799.723643\n      2100081.0\n      2126532.00\n      2170639.00\n    \n    \n      time\n      2380.0\n      1.004967e+04\n      227.079811\n      9662.0\n      10045.00\n      10459.00\n    \n    \n      strawberry\n      2380.0\n      6.491597e-01\n      1.058612\n      0.0\n      0.00\n      11.00\n    \n    \n      blueberry\n      2380.0\n      3.571429e-01\n      0.819690\n      0.0\n      0.00\n      12.00\n    \n    \n      pina.colada\n      2380.0\n      3.584034e-01\n      0.803858\n      0.0\n      0.00\n      10.00\n    \n    \n      plain\n      2380.0\n      2.176471e-01\n      0.606556\n      0.0\n      0.00\n      6.00\n    \n    \n      mixed.berry\n      2380.0\n      3.886555e-01\n      0.904311\n      0.0\n      0.00\n      8.00\n    \n    \n      price\n      2380.0\n      5.925089e+01\n      10.913256\n      20.0\n      65.04\n      68.96\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe notice that most of the variables are integers here. We should convert the id variable to factor. This will come handy later since the same household data is available in multiple rows.\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \nyo['id'] = yo['id'].astype('category')\n#summarize data\nyo.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\nOut[17]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      obs\n      2380.0\n      \n      \n      \n      1367.8\n      790.076\n      1\n      1369.5\n      2743\n    \n    \n      id\n      2380.0\n      332\n      2.13229e+06\n      74\n      \n      \n      \n      \n      \n    \n    \n      time\n      2380.0\n      \n      \n      \n      10049.7\n      227.08\n      9662\n      10045\n      10459\n    \n    \n      strawberry\n      2380.0\n      \n      \n      \n      0.64916\n      1.05861\n      0\n      0\n      11\n    \n    \n      blueberry\n      2380.0\n      \n      \n      \n      0.357143\n      0.81969\n      0\n      0\n      12\n    \n    \n      pina.colada\n      2380.0\n      \n      \n      \n      0.358403\n      0.803858\n      0\n      0\n      10\n    \n    \n      plain\n      2380.0\n      \n      \n      \n      0.217647\n      0.606556\n      0\n      0\n      6\n    \n    \n      mixed.berry\n      2380.0\n      \n      \n      \n      0.388655\n      0.904311\n      0\n      0\n      8\n    \n    \n      price\n      2380.0\n      \n      \n      \n      59.2509\n      10.9133\n      20\n      65.04\n      68.96\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us look at the histogram of yogurt prices first.\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \nax = sns.distplot(yo[\"price\"], kde=False, bins=36)\nplt.xlabel('Price', fontsize=12)\nplt.ylabel('Count', fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\nOut[18]:\n\n\n\n\n<matplotlib.text.Text at 0x7fe8665c31d0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have some idea about distribution of prices, let\u2019s figure out on a given purchase occasion how many eight ounce yogurts does a household purchase. To answer this we need to combine counts of the different yogurt flavors into one variable. Then we can look at the histogram.\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nyo['all_purchase'] = yo['strawberry']+yo['blueberry']+yo['plain']+yo['pina.colada']+yo['mixed.berry']\nax = sns.distplot(yo['all_purchase'], kde=False, bins=36)\nplt.xlim(1,22)\nplt.xlabel('All Purchases', fontsize=12)\nplt.ylabel('Count', fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\nOut[19]:\n\n\n\n\n<matplotlib.text.Text at 0x7fe86657a198>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt seems most household purchase one 8 oz yogurt at any given purchase. Now we will look at the scatter plot of prime vs time data.\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nfig, ax = plt.subplots(figsize=(8,6))\nsd = {'alpha': 0.25, 'edgecolors': 'black'}\ng = sns.regplot(x='time', y='price', data=yo, x_jitter=2, y_jitter=0.5, \\\n                fit_reg=False, ax=ax, color='red', scatter_kws=sd)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see the baseline price of yogurt has been increasing over time. We also see some scattered prices around the baseline price, which could be simply due to usage of coupons, sales etc. by customers.\nWhen familiarizing yourself with a new data set that contains multiple observations of the same units, it\u2019s often useful to work with a sample of those units so that it\u2019s easy to display the raw data for that sample. In the case of the yogurt data set, we might want to look at a small sample of households in more detail so that we know what kind of within and between household variation we are working with. This analysis of a sub-sample might come before trying to use within household variation as part of a model. For example, this data set was originally used to model consumer preferences for variety. But, before doing that, we\u2019d want to look at how often we observe households buying yogurt, how often they buy multiple items, and what prices they\u2019re buying yogurt at. One way to do this is to look at some sub-sample in more detail. Let\u2019s pick 16 households at random and take a closer look.\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \nsample_ids = yo.id.unique()\nsample_ids = pd.Series(sample_ids).sample(16, random_state=200)\ndf = yo.ix[yo['id'].isin(list(sample_ids)),['id', 'price', 'time', 'all_purchase']]\ndf = df.reset_index(drop=True)\ndf['id']=pd.Series(list(df.id)).astype('category')\ndf.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\nOut[21]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      id\n      171.0\n      16\n      2.13229e+06\n      74\n      \n      \n      \n      \n      \n    \n    \n      price\n      171.0\n      \n      \n      \n      59.8746\n      9.82084\n      33.04\n      65.04\n      68.96\n    \n    \n      time\n      171.0\n      \n      \n      \n      10002.6\n      233.708\n      9664\n      9981\n      10457\n    \n    \n      all_purchase\n      171.0\n      \n      \n      \n      1.84795\n      1.38489\n      1\n      1\n      10\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \ng = sns.lmplot(x='time',y='price', hue='all_purchase', data=df, scatter_kws={\"s\": 20*df['all_purchase']},\\\n               col='id', col_wrap=4, size=2, aspect=1, fit_reg=False, palette=\"Set1\")\ng.set_xticklabels(rotation=90)\n\n\n\n\n\n\n\n\n\n\n\nOut[22]:\n\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fe866497a90>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom these plots, we can see the variation and how often each household buys yogurt. And it seems that some household purchases more quantities than others with the larger circles. For most of the households, the price of yogurt holds steady, or tends to increase over time.\nNow, there are, of course, some exceptions, like in household 2124545 and in 2118927 household, we might think that the household is using coupons to drive the price down. Now, we don\u2019t have the coupon data to associate with this buying data, but we can see how that information could be paired to this data to better understand the consumer behavior.\nThe general idea is that if we have observations over time, we can facet by the primary unit, case, or individual in the data set. For our yogurt data it was the households we were faceting over.\nThis faceted time series plot is something we can\u2019t generate with our pseudo Facebook data set. Since we don\u2019t have data on our sample of users over time.\nThe Facebook data isn\u2019t great for examining the process of friending over time. The data set is just a cross section, it\u2019s just one snapshot at a fixed point that tells us the characteristics of individuals. Not the individuals over, say, a year.\nBut if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure. This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases.\nMuch of the analysis we\u2019ve done so far focused on some pre-chosen variable, relationship or question of interest. We then used EDA to let those chosen variables speak and surprise us. Most recently, when analyzing the relationship between two variables we look to incorporate more variables in the analysis to improve it. For example, by seeing whether a particular relationship is consistent across values of those other variables. In choosing a third or fourth variable to plot we relied on our domain knowledge. But often, we might want visualizations or summaries to help us identify such auxiliary variables. In some analyses, we may plan to make use of a large number of variables. Perhaps, we are planning on predicting one variable with ten, 20, or hundreds of others. Or maybe we want to summarize a large set of variables into a smaller set of dimensions. Or perhaps, we\u2019re looking for interesting relationships among a large set of variables. In such cases, we can help speed up our exploratory data analysis by producing many plots or comparisons at once. This could be one way to let the data set as a whole speak in part by drawing our attention to variables we didn\u2019t have a preexisting interest in.\nWe should let the data speak to determine variables of interest. There\u2019s a tool that we can use to create a number of scatter plots automatically.\nIt\u2019s called a scatter plot matrix. In a scatter plot matrix. There\u2019s a grid of scatter plots between every pair of variables. As we\u2019ve seen, scatter plots are great, but not necessarily suited for all types of variables. For example, categorical ones. So there are other types of visualizations that can be created instead of scatter plots. Like box plots or histograms when the variables are categorical.\nLet\u2019s produce the scatter plot matrix for our pseudo Facebook data set. We\u2019re going to use the pairplot() method to do so.\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \npf_subset = pf.iloc[:,1:4]\npf_subset['gender'] = pf.gender\ng = sns.pairplot(data=pf_subset.sample(1000), diag_kind='kde', hue='gender')\ng = g.map_diag(sns.distplot)\n\nfor ax in g.axes.flat:\n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \npf_subset = pf.iloc[:,4:8]\npf_subset['gender'] = pf.gender\ng = sns.pairplot(data=pf_subset.sample(1000), diag_kind='kde', hue='gender')\ng = g.map_diag(sns.distplot)\n\nfor ax in g.axes.flat:\n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[25]:\n\n    \npf_subset = pf.iloc[:,8:11]\npf_subset['gender'] = pf.gender\ng = sns.pairplot(data=pf_subset.sample(1000), diag_kind='kde', hue='gender')\ng = g.map_diag(sns.distplot)\n\nfor ax in g.axes.flat:\n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \npf_subset = pf.iloc[:,11:-1]\npf_subset['gender'] = pf.gender\ng = sns.pairplot(data=pf_subset.sample(1000), diag_kind='kde', hue='gender')\ng = g.map_diag(sns.distplot)\n\nfor ax in g.axes.flat:\n    plt.setp(ax.get_xticklabels(), rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n  y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the very least, a scatter plot matrix can be a useful starting point in many analyses.\nA matrix such as this one will be extremely helpful when we have even more variables than those in the pseudo-Facebook data set.\nExamples arise in many areas, but one that has attracted the attention of statisticians is genomic data. In these data sets, they\u2019re often thousands of genetic measurements for each of a small number of samples. In some cases, some of these samples have a disease, and so we\u2019d like to identify genes that are associated with the disease.\nWe will use an example data set of gene expression in tumors. The data contains the expression of 6,830 genes, compared with a larger baseline reference sample.\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \nnci = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/nci.tsv\", sep = '\\s+', header=None)\nnci.columns = list(range(1,65))\n\n\n\n\n\n\n\n\n\nIn\u00a0[56]:\n\n    \nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(data=nci.loc[1:200,:], vmin=0, vmax=1.0, xticklabels=False, yticklabels=False, ax=ax, cmap=\"YlGnBu\")\nplt.xlabel(\"Case\", fontsize=14)\n_ = plt.ylabel(\"Gene\", fontsize=14)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeat maps can also be a good way to look at distributions of large dimensional data.\nIn summary in this post, we started with simple extensions to the scatter plot, and plots of conditional summaries that you worked with in lesson four, such as adding summaries for multiple groups. Then, we tried some techniques for examining a large number of variables at once, such as scatter-plot matrices and heat maps. We also learned how to reshape data, moving from broad data with one row per case, to aggregate data with one row per combination of variables, and we moved back and forth between long and wide formats for our data.\nIn the next and the final post in this series, We will do an in-depth analysis of the US department of Education dataset on college education, highlighting the role of EDA.",
      "tags": "Data Science,EDA,Machine Learning,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/PyPlotsMultiVariables/"
    },
    {
      "title": "Pseudo Facebook Data - Exploring Two Variables",
      "text": "In this section, we will be re-using the data from the previous post based on Pseudo\nFacebook data from udacity.\n\nThe data from the project corresponds to a typical data set at Facebook.\nYou can load the data through the following command. Notice that this is a TAB delimited csv file.\nThis data set consists of 99000 rows of data. We will see the details of different columns using the\ncommand below.\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nimport pandas as pd\nimport numpy as np\n\n#Read csv file\npf = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\", sep = '\\t')\n\ncats = ['userid', 'dob_day', 'dob_year', 'dob_month']\nfor col in pf.columns:\n    if col in cats:\n        pf[col] = pf[col].astype('category')\n\n#summarize data\npf.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile\n  RuntimeWarning)\n\n\n\n\n\nOut[24]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      userid\n      99003.0\n      99003\n      2.19354e+06\n      1\n      \n      \n      \n      \n      \n    \n    \n      age\n      99003.0\n      \n      \n      \n      37.2802\n      22.5897\n      13\n      28\n      113\n    \n    \n      dob_day\n      99003.0\n      31\n      1\n      7900\n      \n      \n      \n      \n      \n    \n    \n      dob_year\n      99003.0\n      101\n      1995\n      5196\n      \n      \n      \n      \n      \n    \n    \n      dob_month\n      99003.0\n      12\n      1\n      11772\n      \n      \n      \n      \n      \n    \n    \n      gender\n      98828.0\n      2\n      male\n      58574\n      \n      \n      \n      \n      \n    \n    \n      tenure\n      99001.0\n      \n      \n      \n      537.887\n      457.65\n      0\n      \n      3139\n    \n    \n      friend_count\n      99003.0\n      \n      \n      \n      196.351\n      387.304\n      0\n      82\n      4923\n    \n    \n      friendships_initiated\n      99003.0\n      \n      \n      \n      107.452\n      188.787\n      0\n      46\n      4144\n    \n    \n      likes\n      99003.0\n      \n      \n      \n      156.079\n      572.281\n      0\n      11\n      25111\n    \n    \n      likes_received\n      99003.0\n      \n      \n      \n      142.689\n      1387.92\n      0\n      8\n      261197\n    \n    \n      mobile_likes\n      99003.0\n      \n      \n      \n      106.116\n      445.253\n      0\n      4\n      25111\n    \n    \n      mobile_likes_received\n      99003.0\n      \n      \n      \n      84.1205\n      839.889\n      0\n      4\n      138561\n    \n    \n      www_likes\n      99003.0\n      \n      \n      \n      49.9624\n      285.56\n      0\n      0\n      14865\n    \n    \n      www_likes_received\n      99003.0\n      \n      \n      \n      58.5688\n      601.416\n      0\n      2\n      129953\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsually, it is best to use a scatter plot to analyze two variables:\n\n\n\n\n\n\nIn\u00a0[47]:\n\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n%matplotlib inline\n\nax = sns.regplot(x='age', y='friend_count', data=pf, fit_reg=False)\nplt.xlim(13, 90)\nplt.ylim(0,5000)\n\n\n\n\n\n\n\n\n\n\n\nOut[47]:\n\n\n\n\n(0, 5000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can notice some really interesting behavior in the ugly scatter plot above:\n\nThe age data is binned, as expected (only integers allowed)\nYoung people around the age of 20 have maximum friend count.\nThere is unusual spike in friends count of people aged more than 100. This could mostly be some flaw in the data, probably based on incorrect entry by the users.\nPeople around the age of 70 too have quite a large amount of friends. This is pretty interesting and could point to the use of the social media site by an unexpected group of people.\n\nWe can use summary command to find the bounds on age and then use that to limit age axis.\n\n\n\n\n\n\nIn\u00a0[26]:\n\n    \npf.age.describe()\n\n\n\n\n\n\n\n\n\n\n\nOut[26]:\n\n\n\n\ncount    99003.000000\nmean        37.280224\nstd         22.589748\nmin         13.000000\n25%         20.000000\n50%         28.000000\n75%         50.000000\nmax        113.000000\nName: age, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nFurthermore, we notice at some areas of the plot being too dense, where as some to be really sparse. The areas where points are too dense is called \u201cover plotting\u201d - It is impossible to extract any meaningful statistics from this region.\nIn order to overcome this, we can set the transparency of the plots using the alpha parameter in the plt.scatter() method. Using a value of 1/20 means, one point that will plotted will be equal to 20 original points.\n\n\n\n\n\n\nIn\u00a0[27]:\n\n    \nax = sns.regplot(x='age', y='friend_count', data=pf, fit_reg=False, color='green', scatter_kws={'alpha': 0.05})\nplt.xlim(13, 90)\nplt.ylim(0,5000)\nplt.plot([13, 90], [600, 600], linewidth=2, color='r')\n\n\n\n\n\n\n\n\n\n\n\nOut[27]:\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f07d45a22e8>]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on these new plots, we can find bulk of higher friend count for younger people is still less than 600. We still find higher count for age group of 70.\nFurthermore, we can do a better representation of data using the coord_trans() method. We will be using a square root function.\nIn order to that, we will first create an \"sqrt\" scale.\n\n\n\n\n\n\nIn\u00a0[28]:\n\n    \nimport numpy as np\nfrom numpy import ma\nfrom matplotlib import scale as mscale\nfrom matplotlib import transforms as mtransforms\nfrom matplotlib.ticker import AutoLocator\n\n\nclass SqrtScale(mscale.ScaleBase):\n    \"\"\"\n    Scales data using np.sqrt method.\n\n    The scale function:\n      np.sqrt(x)\n\n    The inverse scale function:\n      x**2\n    \"\"\"\n\n    # The scale class must have a member ``name`` that defines the\n    # string used to select the scale.  For example,\n    # ``gca().set_yscale(\"mercator\")`` would be used to select this\n    # scale.\n    name = 'sqrt'\n\n    def __init__(self, axis):\n        \"\"\"\n        Any keyword arguments passed to ``set_xscale`` and\n        ``set_yscale`` will be passed along to the scale's\n        constructor.\n        \"\"\"\n        \n        mscale.ScaleBase.__init__(self)\n\n    def get_transform(self):\n        \"\"\"\n        Override this method to return a new instance that does the\n        actual transformation of the data.\n\n        The SqrtTransform class is defined below as a\n        nested class of this one.\n        \"\"\"\n        return self.SqrtTransform()\n\n    def set_default_locators_and_formatters(self, axis):\n        \"\"\"\n        Override to set up the locators and formatters to use with the\n        scale.\n        \"\"\"\n\n        axis.set_major_locator(AutoLocator())\n\n    class SqrtTransform(mtransforms.Transform):\n        # There are two value members that must be defined.\n        # ``input_dims`` and ``output_dims`` specify number of input\n        # dimensions and output dimensions to the transformation.\n        # These are used by the transformation framework to do some\n        # error checking and prevent incompatible transformations from\n        # being connected together.  When defining transforms for a\n        # scale, which are, by definition, separable and have only one\n        # dimension, these members should always be set to 1.\n        input_dims = 1\n        output_dims = 1\n        is_separable = True\n\n        def __init__(self):\n            mtransforms.Transform.__init__(self)\n\n        def transform_non_affine(self, a):\n            \"\"\"\n            This transform takes an Nx1 ``numpy`` array and returns a\n            transformed copy.\n            \"\"\"\n            return np.sqrt(a)\n\n        def inverted(self):\n            \"\"\"\n            Override this method so matplotlib knows how to get the\n            inverse transform for this transform.\n            \"\"\"\n            return SqrtScale.InvertedSqrtTransform()\n\n    class InvertedSqrtTransform(mtransforms.Transform):\n        input_dims = 1\n        output_dims = 1\n        is_separable = True\n\n        def __init__(self):\n            mtransforms.Transform.__init__(self)\n\n        def transform_non_affine(self, a):\n            return a**2\n\n        def inverted(self):\n            return SqrtScale.SqrtTransform()\n\n# Now that the Scale class has been defined, it must be registered so\n# that ``matplotlib`` can find it.\nmscale.register_scale(SqrtScale)\n\n\n\n\n\n\n\n\n\nIn\u00a0[29]:\n\n    \nfig, ax = plt.subplots()\nfig.set_size_inches(8.6, 6.4)\nax = sns.regplot(x='age', y='friend_count', data=pf, fit_reg=False, color='cyan', scatter_kws={'alpha': 0.05}, ax=ax)\nplt.xlim(13, 90)\nplt.ylim(0,4599)\nplt.yscale('sqrt')\nplt.plot([13, 90], [600, 600], linewidth=2, color='r')\n\n\n\n\n\n\n\n\n\n\n\nOut[29]:\n\n\n\n\n[<matplotlib.lines.Line2D at 0x7f07d24f5588>]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOn a similar way, we can look at relationship between friends initiated and age.\n\n\n\n\n\n\nIn\u00a0[30]:\n\n    \nfig, ax = plt.subplots()\nfig.set_size_inches(8.6, 6.4)\nkws = {'alpha': 0.05}\nax = sns.regplot(x='age', y='friendships_initiated', data=pf, fit_reg=False, color='purple', scatter_kws=kws, ax=ax)\nplt.xlim(13, 90)\nplt.ylim(0,4599)\nplt.yscale('sqrt')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, we find this distribution to be very similar to the one for friend count.\nScatter plots try to keep us very close to the data. It represents each and every data point. However, in order to judge the quality of a data, it is important to know its important statistics like mean, median etc. How does average of a variable vary wrt to the some other variable.\nWe want to say, study how does average friend count vary with age. In order to study this we will use the grouping properties of pandas module.\nFirst we want to group our data frame by age. Then, we can create a new data frame that lists friend count mean, median and frequency (n) by using first the groupby() and then using the agg() method. We can look at first few data points of this new data frame using the head() method.\n\n\n\n\n\n\nIn\u00a0[121]:\n\n    \ndef groupByStats(pf, groupCol, statsCol):\n    ''' return a dataframe with groupByCol'''\n    \n    # Define the aggregation calculations\n    aggregations = {\n        statsCol: {\n            (statsCol+'_mean'): 'mean',\n            (statsCol+'_median'): 'median',\n            (statsCol+'_q25'): lambda x: np.percentile(x,25),\n            (statsCol+'_q75'): lambda x: np.percentile(x,75),\n            'n': 'count'\n        }\n    }\n    \n    pf_group_by_age = pf.groupby(groupCol, as_index=False).agg(aggregations).rename(columns = {'':groupCol})\n    pf_group_by_age.columns = pf_group_by_age.columns.droplevel()\n    return pf_group_by_age\n\npf_group_by_age = groupByStats(pf, 'age', 'friend_count')\npf_group_by_age.head(20)\n\n\n\n\n\n\n\n\n\n\n\nOut[121]:\n\n\n\n\n\n  \n    \n      \n      age\n      friend_count_q75\n      n\n      friend_count_mean\n      friend_count_median\n      friend_count_q25\n    \n  \n  \n    \n      0\n      13\n      230.00\n      484\n      164.750000\n      74.0\n      23.75\n    \n    \n      1\n      14\n      293.00\n      1925\n      251.390130\n      132.0\n      44.00\n    \n    \n      2\n      15\n      377.50\n      2618\n      347.692131\n      161.0\n      55.00\n    \n    \n      3\n      16\n      385.75\n      3086\n      351.937135\n      171.5\n      63.00\n    \n    \n      4\n      17\n      360.00\n      3283\n      350.300640\n      156.0\n      56.00\n    \n    \n      5\n      18\n      368.00\n      5196\n      331.166282\n      162.0\n      55.00\n    \n    \n      6\n      19\n      350.00\n      4391\n      333.692097\n      157.0\n      59.00\n    \n    \n      7\n      20\n      304.00\n      3769\n      283.499071\n      135.0\n      49.00\n    \n    \n      8\n      21\n      265.00\n      3671\n      235.941160\n      121.0\n      42.00\n    \n    \n      9\n      22\n      239.00\n      3032\n      211.394789\n      106.0\n      40.00\n    \n    \n      10\n      23\n      216.00\n      4404\n      202.842643\n      93.0\n      32.00\n    \n    \n      11\n      24\n      209.50\n      2827\n      185.712062\n      92.0\n      33.00\n    \n    \n      12\n      25\n      156.00\n      3641\n      131.021148\n      62.0\n      19.00\n    \n    \n      13\n      26\n      169.00\n      2815\n      144.008171\n      75.0\n      28.00\n    \n    \n      14\n      27\n      159.00\n      2240\n      134.147321\n      72.0\n      28.00\n    \n    \n      15\n      28\n      150.00\n      2364\n      125.835448\n      66.0\n      23.00\n    \n    \n      16\n      29\n      142.25\n      1936\n      120.818182\n      66.0\n      25.00\n    \n    \n      17\n      30\n      146.00\n      1716\n      115.208042\n      67.5\n      24.00\n    \n    \n      18\n      31\n      143.00\n      1694\n      118.459858\n      63.0\n      25.00\n    \n    \n      19\n      32\n      140.00\n      1443\n      114.279972\n      63.0\n      21.00\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, let us look at this new data frame visually. We can first look at the relationship between average friend count and age.\n\n\n\n\n\n\nIn\u00a0[97]:\n\n    \npf_group_by_age.plot(x='age', y='friend_count_mean', legend=False)\nplt.ylabel(\"Friend Count Mean\")\n\n\n\n\n\n\n\n\n\n\n\nOut[97]:\n\n\n\n\n<matplotlib.text.Text at 0x7f07d16f9dd8>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use this plot as good summary of the original scatter plot and put the two on top of each other.\n\n\n\n\n\n\nIn\u00a0[109]:\n\n    \nax = sns.regplot(x='age', y='friend_count', data=pf, fit_reg=False, color='cyan', x_jitter=0.5, y_jitter=1.0, scatter_kws={'alpha': 0.05})\npf_group_by_age.plot(x='age', y='friend_count_q25', ax=ax, color='red', style='--')\npf_group_by_age.plot(x='age', y='friend_count_median', ax=ax, color='blue')\npf_group_by_age.plot(x='age', y='friend_count_mean', ax=ax, color='green', style='--')\npf_group_by_age.plot(x='age', y='friend_count_q75', ax=ax, color='red')\nplt.xlim(13, 70)\nplt.ylim(0,1000)\nplt.ylabel(\"Friend Count\")\n\n\n\n\n\n\n\n\n\n\n\nOut[109]:\n\n\n\n\n<matplotlib.text.Text at 0x7f07d091cbe0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the above plot, we can see that between the age group 30-69, 75% of population has less than 200 friends.\nIn stead of using 4 different summary measures to analyze the above data, we can use a single number!\nOften analysts will use correlation coefficients to summarize this. We will use the Pearson product moment correlation (r). You can look at the pandas corr() method for details. This measures a linear correlation between two variables.\n\n\n\n\n\n\nIn\u00a0[112]:\n\n    \ndf = pf[(pf['age'] < 70) & (pf['age'] >= 13)]\ndf['age'].corr(df['friend_count'], method='pearson')\n\n\n\n\n\n\n\n\n\n\n\nOut[112]:\n\n\n\n\n-0.17121437273281295\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also have other measures of relationship. For example, a measure of monotonic relationship would be done using Spearman coefficient. Similarly, a measure of strength of dependence between two variables can be done using the \u201cKendall Rank Coefficient\u201d. A more detailed description about these can be found at http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/.\nWe will now look at variables that are strictly correlated using scatter plots.\nOne such example in our dataset would be a relationship between likes_received (y) vs. www_likes_received (x).\n\n\n\n\n\n\nIn\u00a0[119]:\n\n    \nax = sns.regplot(x='www_likes_received', y='likes_received', data=pf, color='cyan', ci=None, line_kws={'color': 'red'})\nplt.xlim(0, np.percentile(pf.www_likes_received, 95))\nplt.ylim(0, np.percentile(pf.likes_received, 95))\n\n\n\n\n\n\n\n\n\n\n\nOut[119]:\n\n\n\n\n(0, 561.0)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe have used numpy percentile() method to find upper limits of x and y data. Additionally, we added a correlation line using the regplot().\nWe can find the numerical value of the correlation between these two variables.\n\n\n\n\n\n\nIn\u00a0[120]:\n\n    \npf['www_likes_received'].corr(pf['likes_received'], method='pearson')\n\n\n\n\n\n\n\n\n\n\n\nOut[120]:\n\n\n\n\n0.9479901803455516\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation between two variables might not be a good thing always. For example in the above case, it was simply due to the artifact of the two data sets were highly coupled, one was a super set of the other.\nLet us take a look again at the modified data frame created using the groupby methods. In particular, we want to remove any noise in the average values.\n\n\n\n\n\n\nIn\u00a0[150]:\n\n    \npf['age_with_months'] = pf.age + (12-pf.dob_month)/12\npf_group_by_age_with_months = groupByStats(pf, 'age_with_months', 'friend_count')\npf1 = pf_group_by_age[pf_group_by_age['age'] < 71]\npf2 = pf_group_by_age_with_months[pf_group_by_age_with_months['age_with_months'] < 71]\n\n\n\n\n\n\n\n\n\nIn\u00a0[157]:\n\n    \nf, (ax1, ax2, ax3) = plt.subplots(3)\nf.set_size_inches(9, 9)\nsns.regplot(x='age_with_months', y='friend_count_mean', data=pf2, scatter=False, lowess=True, ci=95, line_kws={'color': 'red'}, ax=ax1)\npf2.plot(x='age_with_months', y='friend_count_mean', legend=False, ax=ax1)\nax1.set_xlim([13, 71])\n\nsns.regplot(x='age', y='friend_count_mean', data=pf1, scatter=False, lowess=True, ci=95, line_kws={'color': 'green'}, ax=ax2)\npf1.plot(x='age', y='friend_count_mean', legend=False, ax=ax2)\nax2.set_xlim([13, 71])\n\npf11 = pf1.copy()\npf11['ageRounded'] = np.round(pf1['age']/5.0)*5.0\nsns.regplot(x='ageRounded', y='friend_count_mean', data=pf11, scatter=False, lowess=True, ci=95, line_kws={'color': 'cyan'}, ax=ax3)\npf11.plot(x='ageRounded', y='friend_count_mean', legend=False, ax=ax3)\nax3.set_xlim([13, 71])\n\n\n\n\n\n\n\n\n\n\n\nOut[157]:\n\n\n\n\n(13, 71)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is an example of bias variance trade off, and is similar to the trade off we make when choosing the bin width in histograms. One way, we can do this quite easily in seaborn is using the lowess option, however, in the current implementation it fails to provide any error estimate on the fitted regression!\nLowess option in the seaborn library uses Loess and Lowess method for smoothing. Here, the model is based on the idea that data is continuous and smooth.\nSo, through this post, we have noticed several ways to plot the same data. The obvious that arises is which plot to choose? In EDA, however, answer to this is, simply you should choose. The idea in EDA is that the same data when plotted differently, glean different incites.",
      "tags": "Data Science,EDA,Machine Learning,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/PyPlotsTwoVariables/"
    },
    {
      "title": "Pseudo Facebook Data - Plots in Python",
      "text": "In this post, we will learn about EDA of single variables using simple plots like histograms, frequency plots and box plots.\nData sets used below are part of a project from the UD651 course on udacity by Facebook.\n\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nimport pandas as pd\nimport numpy as np\n\n#Read csv file\npf = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\", sep = '\\t')\n\n#summarize data\npf.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median\n  RuntimeWarning)\n\n\n\n\n\nOut[1]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      userid\n      99003.0\n      \n      \n      \n      1.59705e+06\n      344059\n      1.00001e+06\n      1.59615e+06\n      2.19354e+06\n    \n    \n      age\n      99003.0\n      \n      \n      \n      37.2802\n      22.5897\n      13\n      28\n      113\n    \n    \n      dob_day\n      99003.0\n      \n      \n      \n      14.5304\n      9.01561\n      1\n      14\n      31\n    \n    \n      dob_year\n      99003.0\n      \n      \n      \n      1975.72\n      22.5897\n      1900\n      1985\n      2000\n    \n    \n      dob_month\n      99003.0\n      \n      \n      \n      6.28337\n      3.52967\n      1\n      6\n      12\n    \n    \n      gender\n      98828.0\n      2\n      male\n      58574\n      \n      \n      \n      \n      \n    \n    \n      tenure\n      99001.0\n      \n      \n      \n      537.887\n      457.65\n      0\n      \n      3139\n    \n    \n      friend_count\n      99003.0\n      \n      \n      \n      196.351\n      387.304\n      0\n      82\n      4923\n    \n    \n      friendships_initiated\n      99003.0\n      \n      \n      \n      107.452\n      188.787\n      0\n      46\n      4144\n    \n    \n      likes\n      99003.0\n      \n      \n      \n      156.079\n      572.281\n      0\n      11\n      25111\n    \n    \n      likes_received\n      99003.0\n      \n      \n      \n      142.689\n      1387.92\n      0\n      8\n      261197\n    \n    \n      mobile_likes\n      99003.0\n      \n      \n      \n      106.116\n      445.253\n      0\n      4\n      25111\n    \n    \n      mobile_likes_received\n      99003.0\n      \n      \n      \n      84.1205\n      839.889\n      0\n      4\n      138561\n    \n    \n      www_likes\n      99003.0\n      \n      \n      \n      49.9624\n      285.56\n      0\n      0\n      14865\n    \n    \n      www_likes_received\n      99003.0\n      \n      \n      \n      58.5688\n      601.416\n      0\n      2\n      129953\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe need convert some of the variables from numeric to category.\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \ncats = ['userid', 'dob_day', 'dob_year', 'dob_month']\nfor col in pf.columns:\n    if col in cats:\n        pf[col] = pf[col].astype('category')\n\n#summarize data\npf.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n/p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median\n  RuntimeWarning)\n\n\n\n\n\nOut[2]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n      mean\n      std\n      min\n      50%\n      max\n    \n  \n  \n    \n      userid\n      99003.0\n      99003\n      2.19354e+06\n      1\n      \n      \n      \n      \n      \n    \n    \n      age\n      99003.0\n      \n      \n      \n      37.2802\n      22.5897\n      13\n      28\n      113\n    \n    \n      dob_day\n      99003.0\n      31\n      1\n      7900\n      \n      \n      \n      \n      \n    \n    \n      dob_year\n      99003.0\n      101\n      1995\n      5196\n      \n      \n      \n      \n      \n    \n    \n      dob_month\n      99003.0\n      12\n      1\n      11772\n      \n      \n      \n      \n      \n    \n    \n      gender\n      98828.0\n      2\n      male\n      58574\n      \n      \n      \n      \n      \n    \n    \n      tenure\n      99001.0\n      \n      \n      \n      537.887\n      457.65\n      0\n      \n      3139\n    \n    \n      friend_count\n      99003.0\n      \n      \n      \n      196.351\n      387.304\n      0\n      82\n      4923\n    \n    \n      friendships_initiated\n      99003.0\n      \n      \n      \n      107.452\n      188.787\n      0\n      46\n      4144\n    \n    \n      likes\n      99003.0\n      \n      \n      \n      156.079\n      572.281\n      0\n      11\n      25111\n    \n    \n      likes_received\n      99003.0\n      \n      \n      \n      142.689\n      1387.92\n      0\n      8\n      261197\n    \n    \n      mobile_likes\n      99003.0\n      \n      \n      \n      106.116\n      445.253\n      0\n      4\n      25111\n    \n    \n      mobile_likes_received\n      99003.0\n      \n      \n      \n      84.1205\n      839.889\n      0\n      4\n      138561\n    \n    \n      www_likes\n      99003.0\n      \n      \n      \n      49.9624\n      285.56\n      0\n      0\n      14865\n    \n    \n      www_likes_received\n      99003.0\n      \n      \n      \n      58.5688\n      601.416\n      0\n      2\n      129953\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe goal of this analysis is to understand user behavior and their demographics. We want to understand what they are doing on the Facebook and what they use. Please note this is not a real Facebook dataset.\nOur goal is to do some basic EDA (Exploratory Data Analysis) to understand any underlying patterns in the data.\nWe will first look at a histogram of User's Birthdays.\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n%matplotlib inline\n\nax = sns.countplot(x=\"dob_day\", data=pf)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see some peculiar behavior of the data on the 1st of the month. Let us plot this data in more detail, in per month basis.\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \ng = sns.factorplot(\"dob_day\", col=\"dob_month\", col_wrap=4, data=pf, kind='count', size=2.5, aspect=.8)\ng.set(xticklabels=[])\n\n\n\n\n\n\n\n\n\n\n\nOut[4]:\n\n\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fffcd441208>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis explains the above plot. Because of the default settings, or users privacy concerns, numerous people have 1/1 as their birthdays!\nNow, let us explore the distribution of friend counts in this data.\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nax = sns.distplot(pf[\"friend_count\"], kde=False, bins=100)\nplt.xlim(0,1000)\n\n\n\n\n\n\n\n\n\n\n\nOut[5]:\n\n\n\n\n(0, 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe see the data has some outliers near 5000. This is an example of a long tail data. We want our analysis to be focused on the bunch of Facebook users, so we need to limit the axes of these plots. Additionally, we also want to look at these data as a function of gender. However, We also want to remove any data where gender is NA.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \ndf = pf[pf.gender.notnull()]\ng = sns.FacetGrid(df, col=\"gender\")\ng = g.map(plt.hist, \"friend_count\", bins=100, color=\"b\")\nplt.xlim(0,1000)\n\n\n\n\n\n\n\n\n\n\n\nOut[6]:\n\n\n\n\n(0, 1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we want to know, mean statistics of our data, we can use the 'value_counts' command.\n\n\n\n\n\n\nIn\u00a0[164]:\n\n    \npf.groupby('gender').friend_count.describe()\n\n\n\n\n\n\n\n\n\n\n\nOut[164]:\n\n\n\n\ngender       \nfemale  count    40254.000000\n        mean       241.969941\n        std        476.039706\n        min          0.000000\n        25%         37.000000\n        50%         96.000000\n        75%        244.000000\n        max       4923.000000\nmale    count    58574.000000\n        mean       165.035459\n        std        308.466702\n        min          0.000000\n        25%         27.000000\n        50%         74.000000\n        75%        182.000000\n        max       4917.000000\nName: friend_count, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us know look at the tenure of usage (measured in Years) of Facebook.\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \ndf = pf[pf.tenure.notnull()]\nax = sns.distplot(df[\"tenure\"]/365, kde=False, bins=36)\nplt.xlim(0,7)\nplt.xlabel('Number of years using Facebook', fontsize=12)\nplt.ylabel('Number of users in sample', fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\nOut[8]:\n\n\n\n\n<matplotlib.text.Text at 0x7fffc9590ef0>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will now look at any pattern in the ages of Facebook users in this dataset.\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \nax = sns.distplot(pf[\"age\"], kde=False, bins=100)\nplt.xlim(13,113)\nplt.xlabel('Age of Users in Years', fontsize=12)\nplt.ylabel('Number of users in sample', fontsize=12)\n\n\n\n\n\n\n\n\n\n\n\nOut[9]:\n\n\n\n\n<matplotlib.text.Text at 0x7fffc94f7780>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne general theme of observation here is that most of the data have a long tail. In these circumstances, it is better to look at such data after certain types of transformation. Let us do such an analysis of \u201cfriend_count\u201d.\n\n\n\n\n\n\nIn\u00a0[35]:\n\n    \nax = sns.distplot(pf[\"friend_count\"], kde=False, hist_kws={\"alpha\": 0.9})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[49]:\n\n    \nax = sns.distplot(pf[\"friend_count\"], kde=False, bins=np.logspace(0,4), hist_kws={\"alpha\": 0.9})\nplt.xscale('log')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us try to compare distribution of male vs female friend counts.\n\n\n\n\n\n\nIn\u00a0[175]:\n\n    \ndef plotDensity(x, color=None, label=None, bins=np.linspace(0,1000,200), **kws):\n    \n    w = 100*np.ones_like(x)/x.size\n    plt.hist(x, bins=bins, alpha=0.4, histtype='step', linewidth=2, label=label, color=color, weights=w, **kws)\n    return\n\ng = sns.FacetGrid(df, col=None, hue='gender', size=6.0, xlim=(6,600), ylim=(0,5), legend_out=True)\ng = (g.map(plotDensity, 'friend_count')).add_legend()\ng = g.set_axis_labels('Friend Count', '% of users')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can compare distributions of www likes.\n\n\n\n\n\n\nIn\u00a0[179]:\n\n    \ng = sns.FacetGrid(df, col=None, hue='gender', size=6.0, xlim=(1,15000))\ng = (g.map(plotDensity, 'www_likes', bins=np.logspace(0,5,50))).add_legend()\ng = g.set_axis_labels('www Likes Count', '% of users')\nplt.xscale('log')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe cal also look at the total number of likes numerically per gender, as follows:\n\n\n\n\n\n\nIn\u00a0[173]:\n\n    \npf.groupby('gender').www_likes.sum()\n\n\n\n\n\n\n\n\n\n\n\nOut[173]:\n\n\n\n\ngender\nfemale    3507665\nmale      1430175\nName: www_likes, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also compare two distributions graphically using \u201cbox plots\u201d. We can also look at the actual value using the by command. Here, we are trying to understand which gender initiated more friendships.\n\n\n\n\n\n\nIn\u00a0[185]:\n\n    \nax = sns.boxplot(x='gender', y='friendships_initiated', data=df)\nplt.ylim(0,200)\n\n\n\n\n\n\n\n\n\n\n\nOut[185]:\n\n\n\n\n(0, 200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[186]:\n\n    \npf.groupby('gender').friendships_initiated.describe()\n\n\n\n\n\n\n\n\n\n\n\nOut[186]:\n\n\n\n\ngender       \nfemale  count    40254.000000\n        mean       113.899091\n        std        195.139308\n        min          0.000000\n        25%         19.000000\n        50%         49.000000\n        75%        124.750000\n        max       3654.000000\nmale    count    58574.000000\n        mean       103.066600\n        std        184.292570\n        min          0.000000\n        25%         15.000000\n        50%         44.000000\n        75%        111.000000\n        max       4144.000000\nName: friendships_initiated, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext, we want to understand if users have used certain features of Facebook or not. If we look at the summary of mobile_likes variable, median is close to 0, indicating a lot many users with 0 values for this variable. We can look also look at the logical value if value of this quantity is non-zero. We can additionally create a new variable called mobile_check_in that takes a value 1 if mobile_likes is non-zero.\n\n\n\n\n\n\nIn\u00a0[187]:\n\n    \npf.mobile_likes.describe()\n\n\n\n\n\n\n\n\n\n\n\nOut[187]:\n\n\n\n\ncount    99003.000000\nmean       106.116300\nstd        445.252985\nmin          0.000000\n25%          0.000000\n50%          4.000000\n75%         46.000000\nmax      25111.000000\nName: mobile_likes, dtype: float64\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[201]:\n\n    \n(pf.mobile_likes > 0).value_counts()\n\n\n\n\n\n\n\n\n\n\n\nOut[201]:\n\n\n\n\nTrue     63947\nFalse    35056\nName: mobile_likes, dtype: int64\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[200]:\n\n    \npf['mobile_check_in'] = pd.Series(np.where(pf['mobile_likes'] > 0, 1, 0)).astype('category')\npf.mobile_check_in.value_counts()\n\n\n\n\n\n\n\n\n\n\n\nOut[200]:\n\n\n\n\n1    63947\n0    35056\nName: mobile_check_in, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can find percentage of people who have done mobile check in.\n\n\n\n\n\n\nIn\u00a0[203]:\n\n    \nfrac = (pf.mobile_check_in == 1).sum()/pf.mobile_check_in.size\nprint(\"Fraction of Mobile Check-ins = \", frac)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFraction of Mobile Check-ins =  0.645909719907\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe find that about 65% of people have used mobile devices for check in and hence it would be a good decision to continue development of such products.\nIn summary, here we have learned to make inferences about single variable data using a combination of plots - histograms, box plots and frequency plots; along with various numerical data.",
      "tags": "Data Science,EDA,Machine Learning,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/onevariableeda/"
    },
    {
      "title": "Reddit Survey: Introduction to Pandas",
      "text": "The data set used here is part of a project from UD651 course on udacity by Facebook.\nThe data from the project corresponds to a survey from http://reddit.com. You can load the data through the following command. We will first look at the different attributes of this data using the summary() and describe() pandas methods.\n\n\n\n\n\n\nIn\u00a0[45]:\n\n    \nimport pandas as pd\nimport numpy as np\n\n#Read csv file\nreddit = pd.read_csv(\"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/reddit.csv\").astype(object)\n#summarize data\nreddit.describe(include='all', percentiles=[]).T.replace(np.nan,' ', regex=True)\n\n\n\n\n\n\n\n\n\n\n\nOut[45]:\n\n\n\n\n\n  \n    \n      \n      count\n      unique\n      top\n      freq\n    \n  \n  \n    \n      id\n      32754.0\n      32754.0\n      32756\n      1.0\n    \n    \n      gender\n      32553.0\n      2.0\n      0\n      26418.0\n    \n    \n      age.range\n      32666.0\n      7.0\n      18-24\n      15802.0\n    \n    \n      marital.status\n      32749.0\n      6.0\n      Single\n      10428.0\n    \n    \n      employment.status\n      32603.0\n      6.0\n      Employed full time\n      14814.0\n    \n    \n      military.service\n      32749.0\n      2.0\n      No\n      30526.0\n    \n    \n      children\n      32535.0\n      2.0\n      No\n      27488.0\n    \n    \n      education\n      32610.0\n      7.0\n      Bachelor's degree\n      11046.0\n    \n    \n      country\n      32577.0\n      439.0\n      United States\n      20967.0\n    \n    \n      state\n      20846.0\n      52.0\n      California\n      3401.0\n    \n    \n      income.range\n      31139.0\n      8.0\n      Under $20,000\n      7892.0\n    \n    \n      fav.reddit\n      28393.0\n      1833.0\n      askreddit\n      2123.0\n    \n    \n      dog.cat\n      32749.0\n      3.0\n      I like dogs.\n      17151.0\n    \n    \n      cheese\n      32749.0\n      11.0\n      Other\n      6563.0\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe describe() method helped us get an overview of all the data available to us. We also ensured that all the data read was a categorical data.\nLet us look at the age.range variable in more detail. We can look at the different levels of this variables using the cat.categories property of a Pandas Series.\n\n\n\n\n\n\nIn\u00a0[46]:\n\n    \nreddit[\"age.range\"].astype('category').cat.categories\n\n\n\n\n\n\n\n\n\n\n\nOut[46]:\n\n\n\n\nIndex(['18-24', '25-34', '35-44', '45-54', '55-64', '65 or Above', 'Under 18'], dtype='object')\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis shows there are 7 possible values of this variable and some where no data is available (NA).\nA more pictorial view of this can be seen using a histogram plot of this.\n\n\n\n\n\n\nIn\u00a0[57]:\n\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n%matplotlib inline\n\nnewOrder = [\"Under 18\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65 or Above\"]\nax = sns.countplot(x=\"age.range\", data=reddit, order=newOrder)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimilarly, we can also plot a distribution of income range.\n\n\n\n\n\n\nIn\u00a0[51]:\n\n    \nax = sns.countplot(x=\"income.range\", data=reddit)\nlocs, labels = plt.xticks()\nax = plt.setp(labels, rotation=90)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne problem with the above plots is that the different levels are not ordered. This can be fixed using ordered Factors, instead of regular factor type variables. Additionally, We need to use a more reasonable x-label for plotting income.range.\n\n\n\n\n\n\nIn\u00a0[52]:\n\n    \nnewLevels = [\"100K\", \">150K\", \"20K\",\"30K\", \"40K\", \"50K\", \"70K\", \"<20K\"]\nreddit[\"income.range\"] = reddit[\"income.range\"].astype('category')\nreddit[\"income.range\"] = reddit[\"income.range\"].cat.rename_categories(newLevels)\n\n\n\n\n\n\n\n\n\nIn\u00a0[55]:\n\n    \nnewOrder = [\"<20K\", \"20K\",\"30K\", \"40K\", \"50K\", \"70K\", \"100K\", \">150K\"]\nax = sns.countplot(x=\"income.range\", data=reddit, order=newOrder)",
      "tags": "EDA,mathjax,Python",
      "url": "https://sadanand-singh.github.io/posts/PandasIntroReddit/"
    },
    {
      "title": "Python Tutorial - Week 1",
      "text": "Python is a widely used general-purpose, high-level programming language. Due to its focus on readability, wide-spread popularity and existence of a plethora of libraries (also called modules), it is one of the most preferred programming languages for scientists and engineers.\n\nPython for Non-Programmers\u00b6In this series of python tutorials, I will provide a set of lectures on various basic topics of python. The prime target audience for this series are scientist in non-programming fields like microbiology, genetics, psychology etc. who have some to none programming experience.\nHere is a brief list of topics I will cover per week. I will also post exercises at the end of each session, along with the expected outputs. You should plan to complete these exercises within 5-6 days, before the new tutorial is posted. You will be judging your exercises on your own. The goal should be to match your program's output to the expected output.\n\nWeek 1 : Working with Python on Windows, Concept of Variables & Math Operations, Displaying Outputs \nWeek 2 : User Inputs, Modules, Comments and Basics of Strings\nWeek 3 : More on Strings, Lists and Other Containers\nWeek 4 : Looping/iterating, if/else Conditions\nWeek 5 : Advanced String Operations\nWeek 6 : Regular Expressions and Strings\nWeek 7 : Reading and Writing Files\nWeek 8 : Functions and Writing Scripts\nWeek 9 : Interacting with Operating System\nWeek 10 : Handling and Plotting Data in Python\nWeek 11 : Basic Statistics in Python using Pandas\nWeek 12 : Introduction to BioPython\n\nWeek 1. Introduction to Python\u00b6To start working with any programming language, first thing you need is a working installation of that language. Today, we will go through installation of python on Windows machines.\nTo keep things simple, We will be running our simple programs in google chrome browser, without any need of an installation.\nFor later exercises, from Week 7 onwards, I would highly recommend getting access to a Linux/Mac machine. However, I will also provide doing the same things on Windows machines and Google Chrome as well.\nIn this week's session, I will be assuming you will be using online python in google chrome.\nTo follow these tutorials, please run any code and observe output that you see in code blocks.\nInstalling Python\u00b6Go to the following website to get access to python: http://repl.it/languages/python3\nYou should get a window like this: \nYou can work with this system in two ways:\nA. Write your code interactively (one command at a time) on the dark screen on the left. Pressing ENTER will show you the output of that particular command.\nLets' try our first program:\n\n\n\n\n\n\nIn\u00a0[1]:\n\n    \nprint(\"My Name is so-and-so\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Name is so-and-so\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe above program will simply output whatever was put under \"quotes\". We will learn more about the print() method (what is a method/function in python?) towards the end of this session.\nVariables\u00b6A variable is a symbol or name that stands for a value. For example, in the expression\nx = 23\nx is a name/symbol whose value is numerical and equal to 23. if we write something like this:\ny = x+12-5, then y is a new variable whose value should be equal to 23+12-5.\nLet's try these in a program\n\n\n\n\n\n\nIn\u00a0[2]:\n\n    \nx=23\nprint(\"x = \",x)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx =  23\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[3]:\n\n    \ny=x+12-5\nprint(\"y =\",y)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny = 30\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[4]:\n\n    \nprint(23+12-5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe ran 3 commands. First we created a variable 'x' with a value of 23. We verified it value by using a print() method!\nSecond, we created another variable called 'y' whose values is equal to mathematical operation of 'x+12-5'. If you remember basic algebra, this is exactly that. Finally, we confirmed that value of 'y' is exactly equal to 23+12-5.\nHopefully, you got a feel of variables. Variables are not limited to just numbers though. We can also store text. For example:\n\n\n\n\n\n\nIn\u00a0[5]:\n\n    \nname = \"Sadanand Singh\"\nprint(\"My Name is:\", name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Name is: Sadanand Singh\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere, we saved my name Sadanand Singh in a variable called \"name\".\nConcept of variable is very fundamental to any programming language. You can think of them as tokens that store some value.\nLets consider this example to understand variable in more detail.\nYou are doing your taxes, all of your calculations depend on your net income. If you do all your calculation in terms of actual value of net income, every time you write the number, your chances of making a mistake increases. Furthermore, suppose, you want to update the income due to some mistake in the beginning, now you have update every place where you used your income.\nLets consider a second case where you declare a variable called \"income\", and store income = 30000. Now, any time you do any calculation with income, you will be using the variable \"income\". In this framework, because you have to type your income just once, chances of making mistakes are least and changing it needs just one change!\nMath Operations\u00b6Now that we know how to declare and use variables, we will look at first what all we can do with numbers in python.\nUsing simple math operations are extremely easy in Python. Here are all the basic math operations that one can do in Python.\n\n\nSyntax\nMath\nOperation Name\n\n\n\n\na+b\n$a + b$\nAddition\n\n\na-b\n$a - b$\nSubtraction\n\n\na*b\n$a \\times b$\nMultiplication\n\n\na/b\n$a \\div b$\nDivision\n\n\na**b\n$ab$\nPower/Exponent\n\n\nabs(a)\n$\\lvert a \\rvert$\nAbsolute Value\n\n\n-a\n$-1 \\times a$\nNegation\n\n\na//b\nquotient of $a \\div b$\nQuotient\n\n\na%b\nRemainder of $a \\div b$\nRemainder\n\n\n\nHere are some example operation. Please repeat these and observe the use of parenthesis in using the BODMAS principle.\n\n\n\n\n\n\nIn\u00a0[6]:\n\n    \n3 + 5\n\n\n\n\n\n\n\n\n\n\n\nOut[6]:\n\n\n\n\n8\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[7]:\n\n    \n2 * 3 + 3.34 + 4 - 45.67\n\n\n\n\n\n\n\n\n\n\n\nOut[7]:\n\n\n\n\n-32.33\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[8]:\n\n    \n12.7 - 10 * 23.5 / 0.5\n\n\n\n\n\n\n\n\n\n\n\nOut[8]:\n\n\n\n\n-457.3\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[9]:\n\n    \n12 - (11 + 34) / 2\n\n\n\n\n\n\n\n\n\n\n\nOut[9]:\n\n\n\n\n-10.5\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[10]:\n\n    \na,b=5,6\n\n\n\n\n\n\n\n\n\n\n\n\nAbove is a special case of assigning multiple variables together. In the above example we stored a=5 and b=6. Lets confirm these:\n\n\n\n\n\n\nIn\u00a0[11]:\n\n    \nprint(\"a = \",a)\nprint(\"b = \",b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\na =  5\nb =  6\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[12]:\n\n    \nc = a**b\nprint(\" a raised to the power b is:\",c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n a raised to the power b is: 15625\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[13]:\n\n    \nb = -b\n\n\n\n\n\n\n\n\n\nIn\u00a0[14]:\n\n    \nprint(\"b = \",b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nb =  -6\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan you guess what we did here? First we use negation operation to get \"-b\" i.e. -6. Then, we we redefined b to be equal to -6! Let consider the following example: a = a-b-3. What do you expect the value to be? Now lets check if you are correct:\n\n\n\n\n\n\nIn\u00a0[15]:\n\n    \na = a-b-3\nprint(\"new value of a is: \",a)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew value of a is:  8\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you expect if we do b = -b again?\n\n\n\n\n\n\nIn\u00a0[16]:\n\n    \nb = -b\nprint(\"New Value of b is:\",b)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Value of b is: 6\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[17]:\n\n    \na//b\n\n\n\n\n\n\n\n\n\n\n\nOut[17]:\n\n\n\n\n1\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[18]:\n\n    \na%b\n\n\n\n\n\n\n\n\n\n\n\nOut[18]:\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[19]:\n\n    \nb**-2\n\n\n\n\n\n\n\n\n\n\n\nOut[19]:\n\n\n\n\n0.027777777777777776\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can perform many other advanced math operations using the \"math module\". To use them, first you will need to import the math module in your code like this:\n\n\n\n\n\n\nIn\u00a0[20]:\n\n    \nimport math\n\n\n\n\n\n\n\n\n\n\n\n\nThen, you use operations like the following:\n\n\n\n\n\n\nIn\u00a0[21]:\n\n    \nc = math.sqrt(25)\nprint(c)\nc = math.log(10)\nprint(c)\nc = math.log10(10)\nprint(c)\nc = math.cos(math.pi)\nprint(c)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.0\n2.302585092994046\n1.0\n-1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can all other available mathematical operation in this module on the python website.\n\n\n\n\n\n\n\n\n\nprint() Function in Python\u00b6Primary way to see and print information on your screen in python is using a method/function called \"print()\". We will learn more about functions in python later. For today, you can think of functions as a program that \"does something\". For example, the function print(), does the job of printing things on screen.\nNotice the use of () in functions. () separates a function from a variable. For example, \"foo\" is a variable, whereas \"foo()\" is a function, called sometimes called as method.\nFunctions also have a concept of arguments. Arguments can be thought as inputs to functions. For example, we have function that adds 2 numbers, then this function will need 2 arguments, the two numbers that we want to add. We can denote this function as, addition(a,b).\nSimilarly, functions also have a concept of return values. Return value can be thought as the output of that function. For example, in the above example of addition(a,b) function, sum of two numbers will be the \"return value\" of the function. We can write this as, c = addition(a,b). Here, a and b are arguments to function addition() and c is the return value of this function.\nA function can have any number of arguments, zero to any number; where it can have either zero or 1 return values.\nNow, coming back to the print() method, that we have been using throughout this tutorial.\nprint() method can take any number of arguments separated by commas. All it does is to \"print\" those on your screen. Lets look at some examples:\n\n\n\n\n\n\nIn\u00a0[22]:\n\n    \nprint(3,4)\nprint(\"My Name is Sadanand Singh\")\nprint(\"My Name is \",\"Sadanand Singh\",\"and My age is: \", 29)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 4\nMy Name is Sadanand Singh\nMy Name is  Sadanand Singh and My age is:  29\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, lets try something fun.\n\n\n\n\n\n\nIn\u00a0[23]:\n\n    \nprint(\"My Name is\",\"Sadanand\", sep=\"***\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Name is***Sadanand\n\n\n\n\n\n\n\n\n\n\nIn\u00a0[24]:\n\n    \nprint(\"My Name is\",\"Sadanand\",\"Singh\",\"My Age is\",\"12\",\"\",sep=\"***\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy Name is***Sadanand***Singh***My Age is***12***\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan you explain what is happening here!\n\n\n\n\n\n\n\n\n\nExcercise\u00b6We will all things we have learned today using the exercise below.\nWe will follow the tax preparation example:\n\nCreate a variable to store \"income\"\nCreate another variable called \"taxRate\" which is equal to 1/100000th of \"income\".\nNet federal tax will be equal to 1.5 times income times taxRate\nNet state tax will be equal to square root on federal tax\nNet tax will be federal tax + state tax\nTotal final tax will be Net tax + log to the base of 2 of Net Tax\nPrint following values clearly using print(): income, taxRate, Federal Tax, State Tax, Net Tax and Final Tax.\nFirst run with an income of 60000\nRepeat with an income of 134675\n\nYour output should look like the following in two cases.\nCase 1: income = 60000\n\nTotal Income is: 60000\nTax Rate is: 0.059999999999999998\nTotal Federal Tax is: 1800.0\nTotal State Tax is: 42.426406871192853\nNet Tax is: 1842.4264068711927\nTotal Tax is: 1853.2737981499038\n\nCase 2: income = 134675\n\nTotal Income is: 134675\nTax Rate is: 0.13467499999999999\nTotal Federal Tax is: 9068.6778125000001\nTotal State Tax is: 95.229605756298284\nNet Tax is: 9163.9074182562981\nTotal Tax is: 9177.0691654243201\n\nGreat! Next week we dive into Python further.",
      "tags": "mathjax,Programming,Python",
      "url": "https://sadanand-singh.github.io/posts/PythonTutWeek1/"
    },
    {
      "title": "Carrot Halwa Recipe",
      "text": "Last month or so has been all silent here. No puzzles, no math, no\ncomputers and most important, no Food! And as usual blame is on my work\nschedule.\n\n\nTable of Contents\n\nIngredients\nPreparation\nInteresting Facts\n\n\nBut, its never too late. Especially, when you can start with some exotic\nfood!\nCarrot halwa is a common north Indian dessert, healthy carrots\ncooked in milk and mango puree.\n\n\nFresh Carrots\n\n\nIngredients\n\n8-10 organic carrots (for a more authentic taste).\nCondensed milk (medium size) or 1-2 cup Ricotta cheese\n1-2 cup whole Milk.\nDried nuts (Chopped Almonds, Cashews, Golden raisins and Dates)\nSugar/Fresh Mango puree as per taste.\n3-4 Tablespoon unsalted butter\n3-4 whole cardamom or 1/2 teaspoon cardamom powder\n\n\n\nPreparation\n\nWash and Peel the carrots and grate them.\nHeat a non-stick pan on medium flame and add 3-4 tablespoon of\nbutter.\nWhen the butter melts, add the grated carrots and start frying.\nStir the carrots till all the water from the carrots is lost.\nAdd Ricotta cheese with 1-2 cup whole milk or 1 can of Condensed milk\nand let the carrots cook for 5-7 minutes on low flame. Continuously\nstir the carrots so as not to burn them.\nWhen 3/4th of the milk is dried, add chopped nuts and Sugar.\nTurn off the flame after 3-4 minutes, after all the milk is dried.\nLet the halwa cool down.\nGarnish it with nuts and dried milk.\n\n\n\nMy Creation Before Entering My Tummy\n\n\n\nInteresting Facts\n\nNutritional Value\nof carrots.\nMore about Gajar Halwa",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/CarrotHalwaRecipe/"
    },
    {
      "title": "Moore's Law and Algorithms - Case of Fibonacci Numbers",
      "text": "The world of computers is moving fast. While going through some\nmaterials on algorithms, I have come across an interesting discussion\n-enhancements in hardware (cpu) vis-a-vis algorithms.\n\nOne side of the coin is the hardware - the speed of computers.The famous\nMoore's\nlaw\nstates that:\nThe complexity for minimum component costs has increased at a rate of\nroughly a factor of two per year. Certainly over the short term this\nrate can be expected to continue, if not to increase. Over the longer\nterm, the rate of increase is a bit more uncertain, although there is no\nreason to believe it will not remain nearly constant for at least 10\nyears. -- G. Moore, 1965\nIn simple words, *Moore's\nlaw* is the observation\nthat, over the history of computing hardware, the number of transistors\nin a dense integrated circuit has doubled approximately every two years.\nMore precisely, the number of transistors in a dense integrated circuit\nhas increased by a factor of 1.6 every two years. More recently, keeping\nup with this has been challenging. In the context of this discussion,\nthe inherent assumption is that number of transistors is directly\nproportional to the speed of computers.\nNow, looking at the other side of the coin - speed of algorithms.\nAccording to Excerpt from Report to the President and Congress:\nDesigning a Digital Future, December 2010 (page\n97):\nEveryone knows Moore\u2019s Law \u2013 a prediction made in 1965 by Intel\nco-\u00adfounder Gordon Moore that the density of transistors in integrated\ncircuits would continue to double every 1 to 2 years.... in many areas,\nperformance gains due to improvements in algorithms have vastly exceeded\neven the dramatic performance gains due to increased processor speed.\nThe gain in computing speed due to algorithms have been simply\nphenomenal, unprecedented, to say the least! Being actively involved\nwith realization of Moore's law (guess where I work!), I have been\nnaturally attracted in the study and design of algorithms.\nTo get a more practical perspective on this, lets look at the problem of\nfinding large Fibonacci\nnumbers. These\nnumbers have been used in wide areas ranging from arts to economics to\nbiology to computer science to the game of poker! The simple definition\nof these numbers are:\n\n\\begin{equation*}\nF_{n} = \\begin{cases} F_{n-2} + F_{n-1} & \\text{if } n > 1 \\\\\n1 & \\text{if } n = 1 \\\\\n0 & \\text{if } n = 0\n\\end{cases}\n\\end{equation*}\n\nSo, first few Fibonacci numbers are:\n\\(0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 \\ldots\\) These\nnumbers grow almost as fast as powers of 2: for example,\n\\(F_{30}\\) is over a million, and \\(F_{100}\\) is 21 digits long!\nIn general, \\(F_n \\approx 2{0.694n}\\) Clearly, we need a computing\ndevice to calculate say \\(F_{200}\\).\nHere is a simple plot of first few Fibonacci numbers:\n\n#chart-d1844ffa-80ce-4759-a684-a448557c1bf8{-webkit-user-select:none;-webkit-font-smoothing:antialiased;font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .title{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .legends .legend text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis text.major{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .text-overlay text.value{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .text-overlay text.label{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 text.no_data{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:64px}\n#chart-d1844ffa-80ce-4759-a684-a448557c1bf8{background-color:#f0f0f0}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 path,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 rect,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 circle{-webkit-transition:250ms ease-in;-moz-transition:250ms ease-in;transition:250ms ease-in}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .graph &gt; .background{fill:#f0f0f0}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .plot &gt; .background{fill:#f8f8f8}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .graph{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 text.no_data{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .title{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .legends .legend text{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .legends .legend:hover text{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .line{stroke:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .guide.line{stroke:rgba(0,0,0,0.6)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .major.line{stroke:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis text.major{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y .guides:hover .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .line-graph .axis.x .guides:hover .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .stackedline-graph .axis.x .guides:hover .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .xy-graph .axis.x .guides:hover .guide.line{stroke:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .guides:hover text{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .reactive{fill-opacity:.5;stroke-opacity:.8}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .ci{stroke:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .reactive.active,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .active .reactive{fill-opacity:.9;stroke-opacity:.9;stroke-width:4}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .ci .reactive.active{stroke-width:1.5}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .series text{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip rect{fill:#f8f8f8;stroke:rgba(0,0,0,0.9);-webkit-transition:opacity 250ms ease-in;-moz-transition:opacity 250ms ease-in;transition:opacity 250ms ease-in}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .legend{font-size:.8em;fill:rgba(0,0,0,0.6)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .x_label{font-size:.6em;fill:rgba(0,0,0,0.9)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .xlink{font-size:.5em;text-decoration:underline}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip .value{font-size:1.5em}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .bound{font-size:.5em}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .max-value{font-size:.75em;fill:rgba(0,0,0,0.6)}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .map-element{fill:#f8f8f8;stroke:rgba(0,0,0,0.6) !important}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .map-element .reactive{fill-opacity:inherit;stroke-opacity:inherit}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-0,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-0 a:visited{stroke:#00b2f0;fill:#00b2f0}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-1,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-1 a:visited{stroke:#43d9be;fill:#43d9be}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-2,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .color-2 a:visited{stroke:#0662ab;fill:#0662ab}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .text-overlay .color-0 text{fill:black}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .text-overlay .color-1 text{fill:black}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .text-overlay .color-2 text{fill:black}\n#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 text.no_data{text-anchor:middle}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .guide.line{fill:none}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .centered{text-anchor:middle}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .title{text-anchor:middle}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .legends .legend text{fill-opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.x text{text-anchor:middle}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.x:not(.web) text[transform]{text-anchor:start}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.x:not(.web) text[transform].backwards{text-anchor:end}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y text{text-anchor:end}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y text[transform].backwards{text-anchor:start}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y2 text{text-anchor:start}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y2 text[transform].backwards{text-anchor:end}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .guide.line{stroke-dasharray:4,4}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .major.guide.line{stroke-dasharray:6,6}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .horizontal .axis.y .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .horizontal .axis.y2 .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .vertical .axis.x .guide.line{opacity:0}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .horizontal .axis.always_show .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .vertical .axis.always_show .guide.line{opacity:1 !important}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y .guides:hover .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.y2 .guides:hover .guide.line,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis.x .guides:hover .guide.line{opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .axis .guides:hover text{opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .nofill{fill:none}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .subtle-fill{fill-opacity:.2}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .dot{stroke-width:1px;fill-opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .dot.active{stroke-width:5px}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .dot.negative{fill:transparent}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 text,#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 tspan{stroke:none !important}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .series text.active{opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip rect{fill-opacity:.95;stroke-width:.5}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .tooltip text{fill-opacity:1}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .showable{visibility:hidden}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .showable.shown{visibility:visible}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .gauge-background{fill:rgba(229,229,229,1);stroke:none}#chart-d1844ffa-80ce-4759-a684-a448557c1bf8 .bg-lines{stroke:#f0f0f0;stroke-width:2px}window.pygal = window.pygal || {};window.pygal.config = window.pygal.config || {};window.pygal.config['d1844ffa-80ce-4759-a684-a448557c1bf8'] = {\"allow_interruptions\": false, \"box_mode\": \"extremes\", \"classes\": [\"pygal-chart\"], \"css\": [\"file://style.css\", \"file://graph.css\"], \"defs\": [], \"disable_xml_declaration\": false, \"dots_size\": 2.5, \"dynamic_print_values\": false, \"explicit_size\": false, \"fill\": false, \"force_uri_protocol\": \"https\", \"formatter\": null, \"half_pie\": false, \"height\": 600, \"include_x_axis\": false, \"inner_radius\": 0, \"interpolate\": null, \"interpolation_parameters\": {}, \"interpolation_precision\": 250, \"inverse_y_axis\": false, \"js\": [\"//kozea.github.io/pygal.js/2.0.x/pygal-tooltips.min.js\"], \"legend_at_bottom\": true, \"legend_at_bottom_columns\": 3, \"legend_box_size\": 12, \"logarithmic\": true, \"margin\": 20, \"margin_bottom\": null, \"margin_left\": null, \"margin_right\": null, \"margin_top\": null, \"max_scale\": 16, \"min_scale\": 4, \"missing_value_fill_truncation\": \"x\", \"no_data_text\": \"No data\", \"no_prefix\": false, \"order_min\": null, \"pretty_print\": false, \"print_labels\": false, \"print_values\": false, \"print_values_position\": \"center\", \"print_zeroes\": true, \"range\": null, \"rounded_bars\": null, \"secondary_range\": null, \"show_dots\": true, \"show_legend\": true, \"show_minor_x_labels\": true, \"show_minor_y_labels\": true, \"show_only_major_dots\": false, \"show_x_guides\": false, \"show_x_labels\": true, \"show_y_guides\": true, \"show_y_labels\": true, \"spacing\": 10, \"stack_from_top\": false, \"strict\": false, \"stroke\": true, \"stroke_style\": null, \"style\": {\"background\": \"#f0f0f0\", \"ci_colors\": [], \"colors\": [\"#00b2f0\", \"#43d9be\", \"#0662ab\", \"#00668a\", \"#98eadb\", \"#97d959\", \"#033861\", \"#ffd541\", \"#7dcf30\", \"#3ecdff\", \"#daaa00\"], \"font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"foreground\": \"rgba(0, 0, 0, 0.9)\", \"foreground_strong\": \"rgba(0, 0, 0, 0.9)\", \"foreground_subtle\": \"rgba(0, 0, 0, 0.6)\", \"guide_stroke_dasharray\": \"4,4\", \"label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"label_font_size\": 10, \"legend_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"legend_font_size\": 14, \"major_guide_stroke_dasharray\": \"6,6\", \"major_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"major_label_font_size\": 10, \"no_data_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"no_data_font_size\": 64, \"opacity\": \".5\", \"opacity_hover\": \".9\", \"plot_background\": \"#f8f8f8\", \"stroke_opacity\": \".8\", \"stroke_opacity_hover\": \".9\", \"title_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"title_font_size\": 16, \"tooltip_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"tooltip_font_size\": 14, \"transition\": \"250ms ease-in\", \"value_background\": \"rgba(229, 229, 229, 1)\", \"value_colors\": [], \"value_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_font_size\": 16, \"value_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_label_font_size\": 10}, \"title\": \"Fibonacci Numbers\", \"tooltip_border_radius\": 0, \"tooltip_fancy_mode\": true, \"truncate_label\": null, \"truncate_legend\": null, \"width\": 800, \"x_label_rotation\": 0, \"x_labels\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"], \"x_labels_major\": null, \"x_labels_major_count\": null, \"x_labels_major_every\": null, \"x_title\": null, \"xrange\": null, \"y_label_rotation\": 0, \"y_labels\": null, \"y_labels_major\": null, \"y_labels_major_count\": null, \"y_labels_major_every\": null, \"y_title\": null, \"zero\": 1, \"legends\": [\"Fibonacci(n)\", \"2\\u207f\", \"n\\u00b2\"]}Fibonacci Numbers112244668810102020404060608080100100200200400400600600800800100010002000200040004000600060008000800010000100002000020000400004000060000600008000080000100000100000200000200000400000400000600000600000800000800000100000010000001234567891011121314151617181920Fibonacci Numbers113.615384615384615490.01149.445344129554655490.02285.27530364372468465.533121.10526315789473451.168418732331745156.93522267206478433.112761675259658192.76518218623482416.5613228.59514170040487399.3392269055432721264.4251012145749382.3882231419204834300.2550607287449365.3571603893667955336.085020242915348.356687018645861089371.914979757085331.344530941323211144407.7449392712551314.336837464663312233443.5748987854251297.3274394559701413377479.40485829959516280.318692524917714610515.2348178137652263.309696904968915987551.0647773279353246.30079627581827161597586.8947368421052229.2918593634248172584622.7246963562753212.2829363099989184181658.5546558704452195.27400796291863196765694.3846153846154178.265081637834520213.615384615384615465.51449.445344129554655441.02885.27530364372468416.5316121.10526315789473392.0432156.93522267206478367.5564192.76518218623482343.06128228.59514170040487318.57256264.4251012145749294.08512300.2550607287449269.591024336.085020242915244.99999999999997102048371.914979757085220.5114096407.7449392712551196.00000000000006128192443.5748987854251171.500000000000061316384479.40485829959516147.01432768515.2348178137652122.51565536551.064777327935398.016131072586.894736842105273.517262144622.724696356275349.0000000000000618524288658.554655870445224.500000000000057191048576694.3846153846154-5.684341886080802e-1420113.615384615384615490.01449.445344129554655441.02985.27530364372468412.33683746466335316121.10526315789473392.0425156.93522267206478376.2255233505192536192.76518218623482363.33683746466335649228.59514170040487352.4396088191774764264.4251012145749343.0881300.2550607287449334.67367492932679100336.085020242915327.225523350519210121371.914979757085320.4878506867724511144407.7449392712551314.336837464663312169443.5748987854251308.6784538110864613196479.40485829959516303.4396088191774714225515.2348178137652298.562360815182615256551.0647773279353294.016289586.8947368421052289.714320778733417324622.7246963562753285.673674929326718361658.5546558704452281.851551841264419400694.3846153846154278.2255233505192720Fibonacci(n)2\u207fn\u00b2The most basic algorithm, that comes to mind is a recursive scheme that\ntaps directly into the above definition of Fibonacci series.\ndef fibRecursive(n):\n    if n == 0:\n        return 0\n    if n == 1:\n        return 1\n    return fibRecursive(n-2)+fibRecursive(n-1)\nIf you analyze this scheme, this is in fact an exponential algorithm,\ni.e. fibRecursive( n ) is proportional to\n\\(2{0.694n} \\approx (1.6)n\\), so it takes 1.6 times longer to\ncompute \\(F_{n+1}\\) than \\(F_n\\). This is interesting. Recall,\nunder Moore's law, computers get roughly 1.6 times faster every 2 years.\nSo if we can reasonably compute \\(F_{100}\\) with this year's\ntechnology, then only after 2 years we will manage to get\n\\(F_{101}\\)! Only one more Fibonacci number every 2 years!\nLuckily, algorithms have grown at a much faster pace. Let's consider\nimprovements w.r.t to this current problem of finding \\(n{th}\\)\nFibonacci number, \\(F_n\\).\nFirst problem we should realize in the above recursive scheme is that we\nare recalculating lower \\(F_n\\) at each recursion level. Lets solve\nthis issue by storing each calculation and avoiding any re-calculation!\ndef fibN2(n):\n    a = 0\n    b = 1\n    if n == 0:\n        return 0\n\n    for i in range(1,n+1):\n        c = a + b\n        a = b\n        b = c\n\n    return b\nOn first glance this looks like an \\(\\mathcal{O}(n)\\) scheme, as we\nconsider each addition as one operation. However, we should realize that\nas \\(n\\) increases, addition can not be assumed as a single\noperation, rather every step of addition is an \\(\\mathcal{O}(n)\\)\noperation, recall first grade Math for adding numbers digit by digit!!\nHence, this algorithm is an \\(\\mathcal{O}(n2)\\) scheme. Can we do\nbetter?\nYou bet, we can! Lets consider the following scheme:\n\n\\begin{equation*}\n\\begin{pmatrix} 1&1 \\\\ 1&0 \\end{pmatrix}n = \\begin{pmatrix} F_{n+1}&F_n \\\\ F_n&F_{n-1} \\end{pmatrix}\n\\end{equation*}\n\nWe can use a recursive scheme to calculate this matrix power using a\ndivide and conquer scheme in \\(\\mathcal{O}(\\log{}n)\\) time.\ndef mul(A, B):\n    a, b, c = A\n    d, e, f = B\n    return a*d + b*e, a*e + b*f, b*e + c*f\n\ndef pow(A, n):\n    if n == 1:     return A\n    if n & 1 == 0: return pow(mul(A, A), n//2)\n    else:          return mul(A, pow(mul(A, A), (n-1)//2))\n\ndef fibLogN(n):\n    if n < 2: return n\n    return pow((1,1,0), n-1)[0]\nLets think a bit harder about this. Is it really an\n\\(\\mathcal{O}(\\log{}n)\\) scheme? It involves multiplication of\nnumbers, the method mul(A, B). What happens when \\(n\\) is very\nlarge? Sure, this will blow up, as typical multiplication would be an\n\\(\\mathcal{O}(n2)\\) operation. So, in fact, our new scheme is\n\\(\\mathcal{O}(n2 \\log{}n)\\)!\nLuckily, we can solve even large multiplications in\n\\(\\mathcal{O}(n{log_2{3}} \\approx n{1.585})\\), using Karatsuba\nmultiplication,\nwhich is again a divide and conquer scheme.\nHere is one simple implementation (Same as the above scheme, but with\nthe following mul(A,B) method):\n_CUTOFF = 1536\n\ndef mul(A, B):\n    a, b, c = A\n    d, e, f = B\n    return multiply(a,d) + multiply(b,e), multiply(a,e) + multiply(b,f), multiply(b,e) + multiply(c,f)\n\ndef multiply(x, y):\n    if x.bit_length() <= _CUTOFF or y.bit_length() <= _CUTOFF:\n        return x * y\n\n    else:\n        n = max(x.bit_length(), y.bit_length())\n        half = (n + 32) // 64 * 32\n        mask = (1 << half) - 1\n        xlow = x & mask\n        ylow = y & mask\n        xhigh = x >> half\n        yhigh = y >> half\n\n        a = multiply(xhigh, yhigh)\n        b = multiply(xlow + xhigh, ylow + yhigh)\n        c = multiply(xlow, ylow)\n        d = b - a - c\n        return (((a << half) + d) << half) + c\nSo, this final scheme is in \\(\\mathcal{O}(n{1.585}\\log{}n)\\) time.\nHere is one final way of solving this problem in the same\n\\(\\mathcal{O}(n{1.585}\\log{}n)\\) time, but using a somewhat simpler\nscheme!\nIf we know \\(F_K\\) and \\(F_{K+1}\\), then we can find,\n\n\\begin{equation*}\nF_{2K} = F_K \\left [ 2F_{K+1}-F_K \\right ]\n\\end{equation*}\n\n\n\\begin{equation*}\nF_{2K+1} = {F_{K+1}}2+{F_K}2\n\\end{equation*}\n\nWe can implement this using the Karatsuba multiplication as follows:\ndef fibFast(n):\n    if n <= 2:\n        return 1\n    k = n // 2\n    a = fibFast(k + 1)\n    b = fibFast(k)\n    if n % 2 == 1:\n        return multiply(a,a) + multiply(b,b)\n    else:\n        return multiply(b,(2*a - b))\nThat's it for today. We saw how far algorithms can go in speed for such\nsimple problems. Let me know in the comments below, if you have any\nfaster or alternate algorithms in mind. Have fun, May zero be with you!",
      "tags": "Algorithms,mathjax",
      "url": "https://sadanand-singh.github.io/posts/FibonacciNumbers/"
    },
    {
      "title": "Two Simple Math Puzzles",
      "text": "Here are two math puzzles, solve, comment and enjoy the discussion!\n\n\nTable of Contents\n\nPuzzle 1: Prime Numbers\nSolution\nPuzzle 2: Hopping on a Chess Board\nSolution\n\n\n\nPuzzle 1: Prime Numbers\nProve that \\(p2-1\\) is divisible by 24, where \\(p\\) is a prime\nnumber with \\(p>3\\).\nThis is a simple one - do not go by the technicality of the problem statement.\n\n\nSolution\n\\(p2-1 = (p-1)\\times (p+1)\\) Given, \\(p\\)\nis a prime number \\(>3\\), \\(p-1\\) and \\(p+1\\)\nare even. We can also\nwrite,\n\n\\begin{equation*}\np-1=2K, \\text{and } p+1=2K+2=2(K+1)\n\\end{equation*}\n\nGiven, \\(K \\in \\mathbb{N}\\), either \\(K\\)\nor \\(K+1\\) are also even.\nHence, \\((p-1)\\times (p+1)\\) is divisible by \\(2\\times 4 = 8\\).\nFurthermore, as \\(p\\) is prime, we can write it as either\n\\(p = 3s+1\\) or \\(p = 3s-1\\), where \\(s \\in \\mathbb{N}\\). In\neither case, one of \\(p-1\\) or \\(p+1\\) are divisible by 3 as\nwell.\nHence, \\(p2-1\\) is divisible by \\(8\\times 3 = 24\\).\n\n\nPuzzle 2: Hopping on a Chess Board\nOn a chess board, basically a \\(8\\times 8\\) grid, how many ways one\ncan go from the left most bottom corner to the right most upper corner?\nIn chess naming conventions, from \"a1\" to \"h8\".\n\n \n\nIn the original post this problem was ill defined.\n\nPlease solve this problem with the constraints that only up and right moves are allowed.\n\n\n\n\n\nChess Board\n\nCan you find the generic answer for the case of \\(N\\times M\\) grid.\n\n\nSolution\nCorrection:\nFor an \\(N\\times M\\) grid, we need only \\(N-1\\)\nright and \\(M-1\\) up moves.  Thank you Devin for pointing this\nout.\nGiven only forward moves are allowed, for any arbitrary grid of\n\\(N\\times M\\), a total of \\((N-1) + (M-1)\\) moves are needed.\nAny \\(N-1\\) of these moves can be of type right and \\(M-1\\) of\ntype up. In short, this is a combinatorial problem of distributing\n\\(N+M-2\\) objects into groups of \\(N-1\\) and \\(M-1\\). This\nis simply,\n\n\\begin{equation*}\n\\dbinom{N+M-2}{N-1} = \\frac{(N+M-2)!}{(N-1)! (M-1)!}\n\\end{equation*}\n\nIn the particular case of the chess board, \\(N = M = 8\\). Hence,\ntotal number of possible paths are:\n\n\\begin{equation*}\n\\text{No. of Paths} = \\frac{14!}{7! 7!} =3432\n\\end{equation*}\n\nThank you Rohit and Amber for posting quick solutions!\nLet me know if you have any other interesting alternative solutions to\nthese problems.",
      "tags": "Algorithms,mathjax,Puzzles",
      "url": "https://sadanand-singh.github.io/posts/PrimeNumberAndPath/"
    },
    {
      "title": "Plasma 5 Installation on Arch Linux",
      "text": "In my last post on Arch Installation Guide , We installed the base system and\nwe can now login into our new system as root using the password that we\nset.\n\n\nTable of Contents\n\nAdd New User\nPlasma 5 Desktop\nAudio Setup\nUseful Tips\n\n\n\n\nPlasma 5 Looks\n\nNow, we will proceed further to install the Plasma 5 desktop.\n\nAdd New User\nChoose $USER per your liking. I chose ssingh, so in future commands\nwhenever you see ssingh please replace it with your $USER.\n$ useradd -m -G wheel -s /bin/bash $USERNAME\n$ chfn --full-name \"$FULL_NAME\" $USERNAME\n$ passwd $USERNAME\n\n\nPlasma 5 Desktop\nNetwork should be setup at the start. Check the status of network using:\n$ ping google.com -c 2\n$\n$ PING google.com (10.38.24.84) 56(84) bytes of data.\n$ 64 bytes from google.com (10.38.24.84): icmp_seq=1 ttl=64 time=0.022 ms\n$ 64 bytes from google.com (10.38.24.84): icmp_seq=2 ttl=64 time=0.023 ms\n$\n$ --- google.com ping statistics ---\n$ 2 packets transmitted, 2 received, 0% packet loss, time 999ms\n$ rtt min/avg/max/mdev = 0.022/0.022/0.023/0.004 ms\n$\nIf you do not get this output, please follow the troubleshooting links\nat arch wiki on setting up network.\nI will be assuming you have an NVIDIA card for graphics installation.\nTo setup a graphical desktop, first we need to install some basic X\nrelated packages, and some essential packages (including fonts):\n$ pacman -S xorg-server xorg-server-utils nvidia nvidia-libgl\nTo avoid the possibility of forgetting to update your initramfs after\nan nvidia upgrade, you have to use a pacman hook like this:\n$ vim /etc/pacman.d/hooks/nvidia.hook\n$\n...\n[Trigger]\nOperation=Install\nOperation=Upgrade\nOperation=Remove\nType=Package\nTarget=nvidia\n\n[Action]\nDepends=mkinitcpio\nWhen=PostTransaction\nExec=/usr/bin/mkinitcpio -p linux\n...\n$\nNvidia has a daemon that is to be run at boot. To start the persistence\ndaemon at boot, enable the nvidia-persistenced.service.\n$ systemctl enable nvidia-persistenced.service\n$ systemctl start nvidia-persistenced.service\n\nImportant\nKWIN FLICKERING ISSUE\nTo avoid screen tearing in KDE (KWin), add following:\n$ vim /etc/profile.d/kwin.sh\n$\n...\nexport __GL_YIELD=\"USLEEP\"\n...\nIf this does not help please try adding the following instead -\n$ vim /etc/profile.d/kwin.sh\n$\n...\nexport KWIN_TRIPLE_BUFFER=1\n...\n\nWarning\n Do not have both of the above enabled at the same time. Please look at `Arch Wiki `__ for additional details.\n\n\nNow continue installing remaining important packages for the GUI.\n$ pacman -S mesa ttf-hack ttf-anonymous-pro\n$ pacman -S tlp tlp-rdw acpi_call bash-completion git meld\n$ pacman -S ttf-dejavu ttf-freefont ttf-liberation\nNow, we will install the packages related to Plasma 5:\n$ pacman -S plasma-meta kf5 kdebase kdeutils kde-applications\n$ pacman -S kdegraphics gwenview\nNow we have to setup a display manager. I chose recommended SDDM for\nplasma 5.\n$ pacman -S sddm sddm-kcm\n$ vim /etc/sddm.conf\n\n...\n[Theme]\n# Current theme name\nCurrent=breeze\n\n# Cursor theme\nCursorTheme=breeze_cursors\n...\n\n$ systemctl enable sddm\nAlso make sure that network manager starts at boot:\n$ systemctl disable dhcpcd.service\n$ systemctl enable NetworkManager\n\n\nAudio Setup\nThis is pretty simple. Install following packages and you should be\ndone:\n$ pacman -S alsa-utils pulseaudio pulseaudio-alsa libcanberra-pulse\n$ pacman -S libcanberra-gstreamer jack2-dbus kmix\n$ pacman -S mpv mplayer\n\n\nUseful Tips\nThis part is optional and you can choose as per your taste. Sync time using the systemd service:\n$ vim /etc/systemd/timesyncd.conf\n$\n...\n[Time]\nNTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org\nFallbackNTP=0.pool.ntp.org 1.pool.ntp.org 0.fr.pool.ntp.org\n...\n$\n$ timedatectl set-ntp true\n$ timedatectl status\n$\n...\n      Local time: Tue 2016-09-20 16:40:44 PDT\n  Universal time: Tue 2016-09-20 23:40:44 UTC\n        RTC time: Tue 2016-09-20 23:40:44\n       Time zone: US/Pacific (PDT, -0700)\n Network time on: yes\nNTP synchronized: yes\n RTC in local TZ: no\n ...\n$\nOn Plasma 5, It is recommended to enable no-bitmaps to improve the font\nrendering:\n$ sudo ln -s /etc/fonts/conf.avail/70-no-bitmaps.conf\n   /etc/fonts/conf.d\nIf you use vim as your primary editor, you may find\nthis vimrc quite useful.\nThat's It. You are done. Start playing your new beautiful desktop.\nPlease leave your comments with suggestions or any word of appreciation\nif this has been of any help to you.\nFollow this page for any additional suggestions or improvements in this guide.",
      "tags": "Linux",
      "url": "https://sadanand-singh.github.io/posts/plasmaInstall/"
    },
    {
      "title": "Arch Installation Guide",
      "text": "You must be thinking - yet another installation guide! There is no\ndearth of \"Installation\" guides of Arch on web. So why another one?\n\nWith advancements like BTRFS file system, UEFI motherboards and modern\nin-development desktop environment like Plasma 5; traditional Arch\nWiki guide\nand Arch Beginners'\nGuide can\nonly be of a limited help. After I got my new my new desktop , my goal\nwas to setup it with a modern setup. I decided to go with Arch Linux\nwith btrfs file system and Plasma 5 desktop. Coming from OSX, I just\nlove how far linux has come in terms of looks - quite close to OSX!\n\nTable of Contents\n\nInitial Setup\nEthernet/Wifi\nSystem Updates\nHard Drives\n\n\nBase Installation\nBasic Setup\nBootloader Setup\nNetwork Setup\nFirst Boot\n\n\n\n\nFor all of you who love installation videos-\n\n\nI will cover this in two parts. First in this post, I will install the\nbase system. Then, in a follow up post, I will discuss details of\nsetting up final working Plasma 5 desktop.\n\n\nPlasma 5 Looks\n\n\nInitial Setup\nDownload the latest iso from Arch website and create the uefi usb\ninstallation media. I used my mac to do this on terminal:\n$ diskutil list\n$ diskutil unmountDisk /dev/disk1\n$ dd if=image.iso of=/dev/rdisk1 bs=1m\n20480+0 records in\n20480+0 records out\n167772160 bytes transferred in 220.016918 secs (762542 bytes/sec)\n\n$ diskutil eject /dev/disk1\nUse this media to boot into your machine. You should boot into UEFI mode\nif you have a\nUEFI\nmotherboard and UEFI mode enabled.\nTo verify you have booted in UEFU mode, run:\n$ efivar -l\nThis should give you a list of set UEFI variables. Please look at the\nBegineers'\nGuide in\ncase you do not get any list of UEFI variables.\n\nEthernet/Wifi\nEthernet should have started by default on your machine.\nIf you do not plan to use wifi during installation, you can skip\nto the next section. If desired later, wifi will still be configurable after you are done\nwith all the installation.\nTo setup wifi simply run:\n$ wifi-menu\nThis is a pretty straight forward tool and will setup wifi for you for\nthis installation session.\nThis will also create a file at /etc/netctl/. We will use this file\nlater to enable wifi at the first session after installation.\n\n\nSystem Updates\nFor editing different configurations, I tend to use vim. So we will\nupdate our package cache and install vim.\n$ pacman -Syy\n$ pacman -S vim\n\n\nHard Drives\nIn my desktop, I have three hard drives, one 256 GB solid state drive\n(SDD), one 1 TB HDD and another 3TB HDD. I set up my drives as follows: -\nSDD for root(/), /boot, and /home partitions, 1st HDD for /data and\nthe 2nd HDD for /media partitions.\nFor UEFI machines, we need to use a GPT partition table and /boot\npartition has to be a fat32 partition with a minimum size of 512 MB. We\nwill format rest other partitions with BTRFS. See this\nlink\nfor benefits of using btrfs partitions.\nFirst list your hard drives with the following:\n$ lsblk\n$ cat /proc/partitions\nAssuming, my setup above, now create gpt partitions and format them.\n$ dd if=/dev/zero of=/dev/sda bs=1M count=5000\n$ gdisk /dev/sda\nFound invalid MBR and corrupt GPT. What do you want to do? (Using the\nGPT MAY permit recovery of GPT data.)\n 1 - Use current GPT\n 2 - Create blank GPT\nThen press 2 to create a blank GPT and start fresh\nZAP:\n$ press x - to go to extended menu\n$ press z - to zap\n$ press Y - to confirm\n$ press Y - to delete MBR\nIt might now kick us out of gdisk, so get back into it:\n$ gdisk /dev/sda\n\n$ Command (? for help): m\n$ Command (? for help): n\n\n$ Partition number (1-128, default 1):\n$ First sector (34-500118158, default = 2048) or {+-}size{KMGTP}:\n$ Last sector (2048-500118, default = 500118) or {+-}size{KMGTP}: 512M\n$ Current type is 'Linux filesystem'\n$ Hex code or GUID (L to show codes, Enter = 8300): ef00\n$ Changed type of partition to 'EFI System'\n\n$ Partition number (2-128, default 2):\n$ First sector (34-500118, default = 16779264) or {+-}size{KMGTP}:\n$ Last sector (16779264-500118, default = 500118) or {+-}size{KMGTP}:\n$ Current type is 'Linux filesystem'\n$ Hex code or GUID (L to show codes, Enter = 8300):\n$ Changed type of partition to 'Linux filesystem'\n\n$ Command (? for help): p\n$ Press w to write to disk\n$ Press Y to confirm\nRepeat the above procedure for /dev/sdb and /dev/sdc, but create just one partition\nwith all values as default. At the end we will have three partitions:\n/dev/sda1, /dev/sda2, /dev/sdb1 and /dev/sdc1\nNow we will format these partitions.\n$ mkfs.vfat -F32 /dev/sda1\n$ mkfs.btrfs -L arch /dev/sda2\n$ mkfs.btrfs -L data /dev/sdb1\n$ mkfs.btrfs -L media /dev/sdc1\nNow, we will create btrfs subvolumes and mount them properly for\ninstallation and final setup.\n$ mount /dev/sda2 /mnt\n$ btrfs subvolume create /mnt/ROOT\n$ btrfs subvolume create /mnt/home\n$ umount /mnt\n\n$ mount /dev/sdb1 /mnt\n$ btrfs subvolume create /mnt/data\n$ umount /mnt\n\n$ mount /dev/sdc1 /mnt\n$ btrfs subvolume create /mnt/media\n$ umount /mnt\nNow, once the sub-volumes have been created, we will mount them in\nappropriate locations with optimal flags.\n$SSD_MOUNTS=\"rw,noatime,nodev,compress=lzo,ssd,discard,\n    space_cache,autodefrag,inode_cache\"\n$ HDD_MOUNTS=\"rw,nosuid,nodev,relatime,space_cache\"\n$ EFI_MOUNTS=\"rw,noatime,discard,nodev,nosuid,noexec\"\n$ mount -o $SSD_MOUNTS,subvol=ROOT /dev/sda2 /mnt\n$ mkdir -p /mnt/home\n$ mkdir -p /mnt/data\n$ mkdir -p /mnt/media\n$ mount -o $SSD_MOUNTS,nosuid,subvol=home /dev/sda2 /mnt/home\n$ mount -o $HDD_MOUNTS,subvol=data /dev/sdb1 /mnt/data\n$ mount -o $HDD_MOUNTS,subvol=media /dev/sdc1 /mnt/media\n\n$ mkdir -p /mnt/boot\n$ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot\n\n\n\nBase Installation\nNow, we will do the actually installation of base packages.\n$ pacstrap /mnt base base-devel btrfs-progs\n$ genfstab -U -p /mnt >> /mnt/etc/fstab\nEdit the /mnt/ect/fstab file to add following /tmp mounts.\ntmpfs /tmp tmpfs rw,nodev,nosuid 0 0\ntmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0\n\nNote\nWIFI AT FIRST BOOT\n\n    Copy our current wifi setup file into the new system. This will enable\n    wifi at first boot. Next, chroot into our newly installed system:\n    \n$cp /etc/netctl/wl* /mnt/etc/netctl/\n\nFinally  bind root for installation.\n$ arch-chroot /mnt /bin/bash\n\nBasic Setup\nHere are some basic commands you need to run to get the installation started.\n$ pacman -Syy\n$ pacman -S sudo vim\n$ vim /etc/locale.gen\n\n...\n# en_SG ISO-8859-1\nen_US.UTF-8 UTF-8\n# en_US ISO-8859-1\n...\n\n$ locale-gen\n$ echo LANG=en_US.UTF-8 > /etc/locale.conf\n$ export LANG=en_US.UTF-8\n$ ls -l /usr/share/zoneinfo\n$ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime\n$ hwclock --systohc --utc\n$ sed -i \"s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\" /etc/sudoers\n$ HOSTNAME=euler\n$ echo $HOSTNAME > /etc/hostname\n$ pacman -S dosfstools efibootmgr\n$ sed -i 's/\\(HOOKS=.*fsck\\)\\(.*$\\)/\\1 btrfs\\2/g' /etc/mkinitcpio.conf\n$ mkinitcpio -p linux\n$ passwd\n\nNote\nWIFI PACKAGES\n\n    We also need to install following packages for wifi to work at first boot:\n    \n$ pacman -S iw wpa_supplicant\n\nWe will also add hostname to our /etc/hosts file:\n$ vim /etc/hosts\n...\n127.0.0.1       localhost.localdomain   localhost $HOSTNAME\n::1             localhost.localdomain   localhost $HOSTNAME\n...\n\n\nBootloader Setup\nsystemd-boot, previously called gummiboot, is a simple UEFI boot manager\nwhich executes configured EFI images. The default entry is selected by\na configured pattern (glob) or an on-screen menu.\nIt is included with the systemd, which is installed on an Arch systems by default.\nAssuming /boot is your boot drive, first run the following command to get started:\n$ bootctl --path=/boot install\nIt will copy the systemd-boot binary to your EFI System Partition\n( /boot/EFI/systemd/systemd-bootx64.efi and /boot/EFI/Boot/BOOTX64.EFI\n- both of which are identical - on x64 systems ) and add systemd-boot\nitself as the default EFI application (default boot entry) loaded by\nthe EFI Boot Manager.\nFinally to configure out boot loader, we will need the UUID of\nout root drive (/dev/sda2). You can find that by:\n$ lsblk -no NAME,UUID /dev/sda2\nNow, make sure that the following two files look as follows,\nwhere $UUID is the value obtained from above command:\n$ vim /boot/loader/loader.conf\n...\ntimeout 3\ndefault arch\n...\n$ vim /boot/loader/entries/arch.conf\n...\n\ntitle Arch Linux\nlinux /vmlinuz-linux\ninitrd /initramfs-linux.img\noptions root=UUID=$UUID rw rootfstype=btrfs rootflags=subvol=ROOT\n...\n\nImportant\n\n    Please  note that you will to need manually run bootctl command everytime systemd-boot gets updated.\n    \n$ bootctl update\n\n\n\nNetwork Setup\nFirst setup hostname using systemd:\n   $ hostnamectl set-hostname $HOSTNAME\n\nCheck the \"Ethernet controller\" entry (or similar) from the\n`lspci -v` output. It should tell you which kernel module contains\nthe driver for your network device. For example:\n\n.. code:: bash\n\n   $ lspci -v\n   $\n   ...\n   04:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 11)\n           Subsystem: ASUSTeK Computer Inc. Device 859e\n           Flags: bus master, fast devsel, latency 0, IRQ 29\n           I/O ports at d000 [size=256]\n           Memory at f7100000 (64-bit, non-prefetchable) [size=4K]\n           Memory at f2100000 (64-bit, prefetchable) [size=16K]\n           Capabilities: <access denied>\n           Kernel driver in use: r8169\n           Kernel modules: r8169\n   ...\n   $\nNext, check that the driver was loaded via dmesg | grep module_name. For example:\n$ dmesg | grep r8169\n$\n...\n[    3.215178] r8169 Gigabit Ethernet driver 2.3LK-NAPI loaded\n[    3.215185] r8169 0000:04:00.0: can't disable ASPM; OS doesn't have ASPM control\n[    3.220477] r8169 0000:04:00.0 eth0: RTL8168g/8111g at 0xffffc90000c74000, 78:24:af:d7:1d:3d, XID 0c000800 IRQ 29\n[    3.220481] r8169 0000:04:00.0 eth0: jumbo features [frames: 9200 bytes, tx checksumming: ko]\n[    3.226949] r8169 0000:04:00.0 enp4s0: renamed from eth0\n[    5.128713] r8169 0000:04:00.0 enp4s0: link down\n[    5.128713] r8169 0000:04:00.0 enp4s0: link down\n[    8.110869] r8169 0000:04:00.0 enp4s0: link up\n...\n$\nProceed if the driver was loaded successfully. Otherwise,\nyou will need to know which module is needed for your particular model.\nPlease follow the\nArch Wiki Networking guide\nfor further assistance.\nGet current device names via /sys/class/net or ip link. For example:\n$ ls /sys/class/net\n$\n...\nenp4s0  lo  wlp3s0\n...\n$\n$ ip link\n$\n...\n2: enp4s0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 78:24:af:d7:1d:3d brd ff:ff:ff:ff:ff:ff\n...\n$\nUsing this name of the device, we need to configure, enable following\ntwo systemd services: systemd-networkd.service\nand systemd-resolved.service.\nFor compatibility with resolv.conf, delete or rename the existing file and\ncreate the following symbolic link:\n$ ln -s /usr/lib/systemd/resolv.conf /etc/resolv.conf\nNetwork configurations are stored as *.network in /etc/systemd/network.\nWe need to create ours as follows.:\n$ vim /etc/systemd/network/wired.network\n$\n...\n[Match]\nName=enp4s0\n\n[Network]\nDHCP=ipv4\n\n...\n\n$\nNow enable these services:\n$ systemctl enable systemd-resolved.service\n$ systemctl enable systemd-networkd.service\nYour network should be ready for first use!\n\n\nFirst Boot\nNow we are ready for the first boot!\nRun the following command:\n$ exit\n$ umount -R /mnt\n$ reboot\nAwesome! We are ready to play with our new system. Alas!\nwhat you have is just a basic installation without any GUI.\nPlease see my next post for where to go next!",
      "tags": "Linux",
      "url": "https://sadanand-singh.github.io/posts/archInstall/"
    },
    {
      "title": "Palak Paneer Recipe",
      "text": "Last month or so has been all silent here. No puzzles, no math, no\ncomputers and most important, no Food! And as usual blame is on my work\nschedule.\n\n\nTable of Contents\n\nIngredients\nPreparation\nSpinach Puree\nCurry for Paneer\n\n\nInteresting Facts\n\n\nYes, the silence is finally over, and what's better than giving your\ntaste buds some rejuvenation after some long busy weeks at work.\nContinuing with my healthy but tasty choices, today I tried to cook\nPalak (Spinach) with Paneer.\nPalak Paneer is a common north Indian cuisine, Indian cottage cheese\ncooked in spinach puree. Its a bit involved than my last few dishes.\nNevertheless, I promise you - the \"yummy-ness\" of this one is worth all\nthe trouble!\n\n\nBoiling Spinach\n\n\nIngredients\n\n1 lb Paneer (Indian Cottage Cheese)\n1 Bunch Fresh Spinach Leaves\n1/2 Medium Onion Finely Cut\n1/2 inch Ginger Root Grated\n1 Medium Tomato Finely Chopped\n1 Clove of Garlic Minced\n1 Medium Bay Leaf\n1/2 tbsp Turmeric Powder\n1/4 tbsp Coriander Powder\n1 tbsp Cumin Seeds\n1 tbsp Garam Masala\n2 pinch Asafoetida Heeng\n2 tbsp Olive Oil\nSalt to taste\n1 cup Milk or Cream\n\n\n\nPaneer\n\n\n\nPreparation\n\nSpinach Puree\n\nWash the leaves thoroughly in running water.\nIn a deep pot, bring about 6 cup of water to boiling temperature. Add\na pinch of salt and 2 pinch asafoetida (Heeng) into the boiling\nwater.\nAdd Spinach leaves in boiling water and let it cook for 3 minutes.\nStrain boiled spinach leaves under cold water.\nIn a blender, make a puree of leaves along with ginger, garlic and\nmilk or cream.\n\n\n\nSpinach Puree\n\n\n\nCurry for Paneer\n\nCut Paneer into small cubes.\nHeat a deep pan on Medium for about 1 minute and then pour olive oil\ninto it.\nAfter about 30 seconds, Add cumin seeds, bay leaf and minced garlic.\nOnce cumin seeds start to pop, add finely cut onions.\nFry the onions until oil starts to separate (about 1-2 minutes).\nAdd finely chopped tomatoes and all the spices except Garam\nMasala.\nOnce oil start to separate out, add the spinach puree.\nStir well while cooking. Add salt and Garam Masala after about 2\nminutes.\nAfter about another 2 minutes add Paneer cubes. Cook for another\n3-4 minutes on low heat.\n\nLet it cool before serving. It can be served with Indian Breads or rice.\n\n\nMy Creation Before Entering My Tummy\n\n\n\n\nInteresting Facts\n\nNutritional\nValue of\nPaneer.\nMore about Paneer\nNutritional\nValue\nof Spinach.",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/PalakPaneerRecipe/"
    },
    {
      "title": "Tilapia Fish Recipe",
      "text": "Typically, hunger and laziness come to me as inseparable couples. To\nmake things worse, I have been trying to eat healthy.\nNevertheless, here is another recipe that solves all these at once. Its\nmy five minute Tilapia Fish Recipe.\n\n\nTable of Contents\n\nIngredients\nPreparation\nInteresting Facts\n\n\nTwo major ingredients - fish and veggies, of this recipe can be grabbed\nfrom the frozen section of any supermarket.\n\n\nFried Streamed Veggies\n\n\nIngredients\n\n2 frozen Boneless Tilapia Fish Pieces\n1 Packet Frozen Steamed Veggies\n2 tbsp Cooking Oil\n1/2 tbsp Parsley\n1 tbsp Gamram Masala\n1 tbsp Cumin Powder\n1/2 tbsp Turmeric Powder\n1/2 tbsp Garlic Powder\n1 tbsp Red Chili Powder\n1/2 Lime\nSalt to Taste\n\n\n\nPreparation\n\nThaw and wash fish pieces carefully.\nIn a skillet, put a tbsp of oil and add frozen veggies to it.\nAdd 1/2 of all the spices and salt to it.\nFry for about 3-5 minutes and keep it aside on a plate.\nIn the same skillet, add rest of the oil.\nAfter about a minute, add fish pieces on medium heat.\nSprinkle salt and half of the remaining spices on the top of the\nfish.\nAfter about 3 minutes, turn the fish and sprinkle rest of the\nspices.\nLet the fish cook slowly. Sprinkle some lime juice on the fish.\nOnce the fish is of mustard color, put fish pieces on veggies.\n\nServe and garnish it with lime juice and some cilantro for an exotic\nlunch/dinner.\n\n\nTilapia with Veggies\n\n\n\nInteresting Facts\n\nMore about Tilapia Fish\nNutritional Value of Tilapia\nFish",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/TilapiaFish/"
    },
    {
      "title": "Aloo Paratha Recipe",
      "text": "Today, I share one of my favorite dishes - Aloo Paratha. It is a dish\nof mashed potato stuffed bread from the Northern India. It is my\nfavorite breakfast dish when I am in India.\n\n\nTable of Contents\n\nIngredients\nFor Stuffing\nFor Bread\n\n\nPreparation\nStuffing\nBread (Paratha)\n\n\nInteresting Facts\n\n\nHere is my try in making some edible parathas. It might take few\ntrials in making a perfectly round-shaped Paratha. Please share your\nviews and inputs in comments below.\n\n\nAloo Paratha Stuffing\n\n\nIngredients\n\nFor Stuffing\n\n2 medium Potatoes\n2 tbsp Cooking Oil\n1/2 tbsp Coriander Powder\n1/2 tbsp Gamram Masala\n1/2 tbsp Cumin Powder\n1/2 tbsp Turmeric Powder\n1/2 tbsp Garlic Powder\n1 tbsp Red Chili Flakes\n1/2 Medium Onion\nSalt to Taste\n\n\n\nAloo Paratha Raw\n\n\n\nFor Bread\n\n4 cups of Whole Wheat Flour\n4-5 tbsp Butter or Ghee\n1 tbsp Black Indian Onion Seeds (Kalaunji)\n1 tbsp Ajwain (Carom Seeds)\nSalt to taste\n\n\n\n\nPreparation\n\nStuffing\n\nBoil and peel Potatoes.\nMash boiled Potatoes and mix salt and chili flakes.\nIn a skillet heat oil and add Garam Masala.\nOnce it start to sputter, add cut onions.\nAdd rest of spices and fry until golden brown.\nAdd mashed Potatoes mix and fry for another 2-3 minutes.\n\n\n\nBread (Paratha)\n\nKnead flour with Ajwain, Kalaunji and salt.\nMake small balls\nMake craters in the balls and fill them with the stuffing.\nRoll these into flat circular breads, typical Indian Bread shape.\nHeat these slowly on a skillet on each side.\nOnce almost cooked, apply some butter or Ghee.\n\nLet it cool before serving. It can be served with Indian spicy pickle\nand yogurt (Dahi).\n\n\nAloo Paratha Yummy\n\n\n\n\nInteresting Facts\n\nMore about Aloo\nParatha\nMore about Ajwain",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/AlooParatha/"
    },
    {
      "title": "MixVegPaneerRecipe",
      "text": "I am quite found of Paneer. However, cooking it can be a hassle.\nYou can get paneer generally at any Indian grocery store. For\nenthusiasts, Here is\na recipe for making Paneer from milk.\n\n\nTable of Contents\n\nIngredients\nFor Baking Paneer\nFor Veggies\n\n\nPreparation\nBake Paneer\nPrepare Veggies\n\n\nInteresting Facts\n\n\nHere is a version of recipe that I use quite often. It involves two\nparts - Baking Paneer and Cooking the veggies.\n\n\nBaked Paneer\n\n\nIngredients\n\nFor Baking Paneer\n\n400 g Paneer\n1 tbsp Coriander Powder\n1 tbsp Gamram Masala\n1/2 tbsp Cumin Powder\n1/2 tbsp Turmeric Powder\n1/2 tbsp Garlic Powder\n1/4 cup Milk or Yogurt\nSalt to Taste\n\n\n\nFor Veggies\n\n500 g Cut Cauliflowers\n1 cup Peas\n1/2 Medium Onion\n1 Medium Tomato\n1/2 tbsp Turmeric Powder\n1 tbsp Coriander Powder\n1 tbsp Mustard Black Seeds\n1 tbsp Cumin Seeds\n2 tbsp Olive Oil\nSalt to taste\n\n\n\n\nPreparation\n\nBake Paneer\n\nThaw Paneer and cut in cubes.\nPreheat the oven at 400 F\nMix Paneer with all the spices and milk/yogurt.\nPlace cubes in a baking sheet and bake it at 400F for about 30\nminutes until Paneer is well cooked.\n\n\n\nPrepare Veggies\n\nCut onion, tomatoes and cauliflowers.\nHeat oil in the pan and add cumin seeds.\nOnce Seeds start to pop, add cut onions.\nFry the onions until oil starts to separate. (about 1-2 minutes)\nAdd spices, peas and tomatoes. (about 1-2 minutes)\nAfter about a minute, add cauliflowers.\nAfter another minute or so, add salt, and keep stirring\noccasionally.\nOnce the cauliflowers are almost cooked, add baked Paneer pieces\n(about 3-5 minutes).\nAdd a little amount of water and cook for about a minute.\n(optional) Garnish with cilantro leaves.\n\nLet it cool before serving. It can be served either Indian breads or\nrice.\n\n\nMy Creation Before Entering My Tummy\n\n\n\n\nInteresting Facts\n\nNutritional\nValue of\nPaneer.\nBaked Paneer in this recipe is also referred as Paneer Tikka\nMore about Paneer",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/MixedVegPaneer/"
    },
    {
      "title": "My New Desktop",
      "text": "It has been long due. Just built a new desktop. Here are different parts\nI used to build this beauty. (Updated Cost)\n\n\ni7 4790 3.6 GHz (Haswell) - Can't Reveal - (Market Price $300)\nAsus Micro MATX B85-G R2.0 Intel LGA1150 - $50\nADATA XPG V1.0 DDR3 1866 2x4 GB RAM - $50\nOCZ Vertex 460A Series 2.5\" 240 GB - $60\nWD Blue 1TB 3.5\" 7200 RPM, 64MB Cache - $45\nUltra LSP V2 650 Watt PSU - $30\nUltra XBlaster Pro U12-42350 Case - $15\nAsus BW-12B1ST/BLK/G/AS Blue Ray Burner - $20\nDas Keyboard V4\nEvoluent Ergo Right Mouse\n\nTotal Value - $580 (Excluding Keyboard/Mouse)\nIt is up and running Arch Linux with BTFRS file system, Plasma 5 Desktop\nand so on. Over the next few posts, I will be posting my installation\nordeals.\nDo not forget to add your comments below. Please add your alternative\nsolutions to anything I did.",
      "tags": "Linux",
      "url": "https://sadanand-singh.github.io/posts/myNewCompSpecs/"
    },
    {
      "title": "Indian Vermicelli Recipe",
      "text": "This is for all the lazy souls like me - in a mood to eat something\ntasty, but in no mood to cook for long.\n\nVermicelli, or also known as seviyan in Hindi, is commonly cooked as a\nsweet dish in Indian subcontinent. As I try to be away from all things\nsweet, I came across this recipe which uses this in a quite spicy\nflavor.\n\nTable of Contents\n\nIngredients\nPreparation\nInteresting Facts\n\n\nSo, here is my super quick and tasty recipe for these noodles. It takes\nless than 12 minutes ( 4 Minutes Preparation + 8 Minutes Cooking).\n\n\nRaw Materials\n\n\nIngredients\n\n300 g Vermicelli Seviyan\n1 cup Peas\n1/2 Medium Onion\n1 Medium Tomato\n1 Clove of Garlic\n4 Curry Leaves\n1/2 tbsp Turmeric Powder\n1 tbsp Coriander Powder\n1 tbsp Mustard Black Seeds\n1 tbsp Cumin Seeds\n2 tbsp Olive Oil\nSalt to taste\n1/2 cup Peanuts optional\n\n\n\nPreparation\n\nWash the leaves thoroughly in running water and chop them (about 1\ninch cuts).\nHeat a deep pan on Medium for about 1 minute and then pour olive oil\ninto it.\nAfter about 30 seconds, Add mustard seeds and cumin seeds after\nanother 30 seconds.\nOnce Seeds start to pop, add garlic and cut onions.\nFry the onions until oil starts to separate (about 1-2 minutes)\nAdd spices, curry leaves, peas and tomatoes.\nAfter about a minute, add noodles.\nAfter another minute or so, add salt, and keep stirring\noccasionally.\nOnce the noodles have become light golden brown in color, add 1/2\ncup of water (about 2-3 minutes).\nKeep stirring, so that noodles do not stick with each other, for\nabout another 2 minutes\n\nLet it cool before serving. You can use some ketchup or sauce with it. I\nlike adding peanuts to it while frying onions.\n\n\nMy Creation Before Entering My Tummy\n\n\n\nInteresting Facts\n\nThe sweet dish seviyan made out of this is also known as shemai\nin Bengali, sev in Gujarati, shavige in Kannada, sevalu or\nsemiya in Telugu, and semiya in Tamil and Malayalam.\nThe recipe above is also commonly known as upma in various parts of\nIndia.\nMore about Vermicelli",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/DesiNoodlesRecipe/"
    },
    {
      "title": "Shortest Non-repeating Substring",
      "text": "Given an alphanumeric string, find the shortest substring that occurs\nexactly once as a (contiguous) substring in it. Overlapping occurrences\nare counted as distinct. If there are several candidates of the same\nlength, you must output all of them in the order of occurrence. The\n\"space\" is NOT considered as a valid non-repeating substring.\n\nExample:\nConsider the following cases:\nCase 1: If the given string is asdfsasa, the answer should be\n['d', 'f']\nCase 2: If the given string is sadanands,\nthe answer should be ['sa', 'ad', 'da', 'na', 'nd', 'ds']\nCase 3: If the given string is wwwwwwww, the answer should be\n['wwwwwwww']\n\nMy Solution\nHere is my solution in Python.\nIt is quite brute force. I am not sure about the order of find() and\nrfind() built-in methods in Python. Assuming these are O(n), my\nalgorithm is in O(n `3`:sup: ). Please put your answers in comments below, if\nyour answer has a better scaling.\nThe function definition that I use for finding non-empty non-repeating\nstrings is recursive.\ndef findNsubString(s,n):\n    subS = []\n    for index in range(len(s)+1) :\n        x = s[index:index+n]\n        if s.find(x)==s.rfind(x) :\n            subS.append(x)\n    if subS :\n        return subS\n    else :\n        return findNsubString(s,n+1)\nI call this method as follows to get the desired results:\n#! /usr/bin/python\nimport argparse\n# Parse Command Line Arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\"-s\", \"--string\", default = \"asdfdfa\", help=\"String Input\")\nargs = parser.parse_args()\ns = args.string\n# Call Method to find smallest non-repeating sub-string\nans = findNsubString(s,1)\nprint ans\nA similar solution can also be written in JAVA or C++. The corresponding\nfind() and rfind() methods in JAVA are called indexOf() and\nlastIndexOf(), respectively. In C++, these methods are called same as\nin Python.\nPlease feel free to put your method definition in any programming\nlanguage.",
      "tags": "Algorithms,Python",
      "url": "https://sadanand-singh.github.io/posts/shortestSubstring/"
    },
    {
      "title": "Lal Saag Recipe",
      "text": "I have started eating a lot of greens these days. As a kid, I always\nloved a vegetable made by my mom, which was made of red leaves, called\n\"Laal Saag\" in Hindi. I could never find what exactly was the\nEnglish/American name for those leaves.\n\n\nTable of Contents\n\nIngredients\nPreparation\nInteresting Facts\n\n\nRecently, at one of the Korean grocery stores, I came across red\nAmaranth leaves. Those looked strikingly similar to the red leaves that\nI used to have back home in India.\nSo, here is my super fast and healthy recipe for these red leaves. It\ntook me just 10 minutes ( 5 Minutes Preparation + 5 Minutes Cooking).\n\n\nRed Amaranth\n\n\nIngredients\n\n1 Bunch Red Amaranth Leaves\n1/2 tbsp Turmeric Powder\n1 tbsp Coriander Powder\n1 tbsp Garam Masala\n1 tbsp Mustard Black Seeds\n1 tbsp Cumin Seeds\n2 tbsp Olive Oil\nSalt to taste\n\n\n\nPreparation\n\nWash the leaves thoroughly in running water and chop them (about 1\ninch cuts).\nHeat a deep pan on Medium for about 1 minute and then pour olive oil\ninto it.\nAfter about 30 seconds, Add mustard seeds and cumin seeds after\nanother 30 seconds.\nOnce Seeds start to pop, add cut leaves into the pan.\nAdd salt, close the lid and keep stirring occasionally.\nOnce the leaves have lost water, add all the spices and keep frying\nuntil things are dry (about 2-3 minutes).\n\nYou can serve this with any Indian bread, or with rice and lentils\n(Daal).\n\n\nMy Creation Before Entering My Tummy\n\n\n\nInteresting Facts\n\nAmaranth is also known as \"Chauli\" or \"Chavli\" in Hindi.\nNutritional\nValue\nof Amaranth\nHistory of\nAmaranth",
      "tags": "Food,Recipe",
      "url": "https://sadanand-singh.github.io/posts/LaalSaagRecipe/"
    },
    {
      "title": "Puzzle 2",
      "text": "Here is another puzzle starring a monkey, transportation and money! Short summary - avoid dealing with fools!\n\nProblem Statement\nThe owner of an apple plantation has a monkey. He wants to transport his\n10000 apples to the market, which is located after the forest. The\ndistance between his apple plantation and the market is about 1000\nkilometer. So he decided to take his monkey to carry the apples. The\nmonkey can carry at the maximum of 2000 apples at a time, and it eats\none apple and throws another one for every kilometer it travels.\nWhat is the largest number of apples that can be delivered to the\nmarket?\nPlease give your solutions in the comments below.\nSolution\nIf the owner lets the monkey carry apples all the way to the end of the\nforest, in the first trip itself, all of 2000 apples will be lost and\nthe monkey will never return back, as the monkey will be out of any food\nand play.\nLets approach this in a different way. Lets break the monkey's journey\nby per unit distance. To carry 10000 apples for 1 km, monkey has to make\n2 x 5 - 1 = 9 trips! On each trip, the owner will loose 2 apples. Hence,\nfor each km distance traveled until number of apples is greater than\n8000, monkey requires 9 x 2 = 18 apples. So, distance traveled by\nmonkey until number of apples is less than 8000 is int(2000 / 18) + 1 =\n112 km, and by this time, 10,000 - 112 x 18 = 7984 apples are left.\nSimilarly, until 6000 apples are left, the monkey will require 2 x 4 -\n1 = 7 trips for every km. Hence, the total distance traveled until no.\nof apples left is less than 6000 is 112 + int(2000 / 14) + 1 = 255 km,\nand by this time no. of apples left is 7984 - 143 x 14 = 5982. Now\nproceeding in a similar manner, until 4000 apples are left, the monkey\nwill require 5 trips for each km. Total distance traveled until no. of\napples left is less than 4000 is 255 + int(2000/10) = 455 km. By this\ntime, number of apples left is 5982 - 200 x 10 = 3982. Until 2000 apples\nare left, the monkey will require 3 trips per km traveled. Total\ndistance traveled till this time is 455 + int(2000/6) + 1 = 789 km. By\nthis time, no. of apples left is 3982 - 334 x 6 = 1978. Below 2000\napples, the monkey will require only one trip to reach to market.\nDistance left is 1000 - 789 = 211 km. Number of apples required by the\nmonkey to travel this distance is 211 x 2 = 422. Hence, total number of\napples that can reach the market is just 1978 - 422 = 1556! That's just\n15.56% of all apples.\nThat's quite bad monkey. Moral of the story is, avoid dealing with fools\n:)\nThank you all who tried this.\nDisclaimer: This puzzle has been inspired by a problem at the\nBlog on Technical Interviews.",
      "tags": "Algebra,mathjax,Puzzles",
      "url": "https://sadanand-singh.github.io/posts/ConsumeTransportProblem/"
    },
    {
      "title": "Puzzle 1",
      "text": "As promised in the intro post, here is the first puzzle!\n\nProblem Statement\nA man needs to go through a train tunnel to reach the other side. He\nstarts running through the tunnel in an effort to reach his destination\nas soon as possible. When he is 1/4th of the way through the tunnel, he\nhears the train whistle behind him. Assuming the tunnel is not big\nenough for him and the train, he has to get out of the tunnel in order\nto survive. We know that the following conditions are true.\n\nIf he runs back, he will make it out of the tunnel by a whisker.\nIf he continues running forward, he will still make it out through\nthe other end by a whisker.\n\nWhat is the speed of the train compared to that of the man?\nPlease give your solutions in the comments below.\nSolution\nIf the man decided to go back, he would have met the train at the\nentrance of tunnel (i.e The man would have traveled 0.25 of the\ntunnel). Instead, if man went forward- by the time train would have\nreached at the entrance of tunnel, the man would have been at the\n0.25+0.25 = 0.5 of tunnel. Hence, according to the second condition, the\ntime taken by man to travel half of tunnel is same as the time taken by\nthe train to travel all of the tunnel. Therefore, the train is traveling\nat twice the speed of the man.\nDisclaimer: This puzzle has been copied from a\nBlog on Technical Interviews.",
      "tags": "Algebra,Puzzles",
      "url": "https://sadanand-singh.github.io/posts/TrainSpeedProblem/"
    },
    {
      "title": "Welcome",
      "text": "This is Sadanand Singh. I am a process engineer, a physicist, a\nprogrammer, an Indian and a human being; with interests in world\npolitics, economics, and society.\n\n Tweets by sadanandsingh  \n\n\n\nThis space is for my personal notes on different subjects. I plan to\nshare my thoughts on following topics from time to time.\n\nTechnology\nStatistics\nMachine Learning\nNews\nEconomics\nEducation\nPolitics, Society & Education\nIndian Food\nPuzzles\n\nIf you have interests in any of these topics, you are welcome to have a peek\ninto my world. Please drop me your views through comments or any of the social\nmedia links.\nHAPPY WEB SURFING!!!",
      "tags": "Introduction",
      "url": "https://sadanand-singh.github.io/posts/FirstPost/"
    }
  ]
}