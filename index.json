{"pages":[{"tags":"Blog, Hugo, golang","text":"I have been using Nikola to build this Blog. Its a great static site build system that is based on Python. However, It has some crazy amount of dependencies (to have reasonable looking site). It uses restructured text (rst) as the primary language for content creation. Personally, I use markdown for almost every thing else - taking notes, making diary, code documentation etc. Furthermore, given Nikola tries to support almost everything in a static site builder, lately its is becoming more and more bloated.\n\nCase in point, recently it got support for shortcodes and although that did enable me to write posts in Markdown, but it is so difficult to develop them (It does not help to have almost no documentation/guide for their development). They are heavily tied to the plugin system with light support for template based shortcodes.\nI really have nothing against Nikola. However, I did not feel at home - I wanted a light system that focused on markdown and had all the flexibilities that I wanted. My research soon brought me to Hugo.\n Hugo - the blazing fast site generator Hugo is a light weight, fast and modern static website engine written in go. It literally takes just milliseconds to build your entire site. For the given lightness, it is highly flexible as well. You can organize your content however you want with any URL structure, group your content using your own indexes and categories and define your own metadata in any format: YAML, TOML or JSON. I was impressed! Keep in my mind, python is still my primary language of programming for scripting and machine learning. And, I have almost no programming experience with go.\nI chose YAML for all the configuration as well as metadata. In my next step to make this move, I had to choose the theme for my blog. If you have been following me, I have been using several flavors of Bootswatch themes. So my first goal was implement my heavily modified version of bootswatch theme in Hugo.\nBootswatch Theme Developing a theme from scratch (Well, the implementation from scratch, as I all I am trying to do is mimic/improvise my current theme from Nikola) turned out to be a great adventure and learning exercise. It helped me understand the Hugo architecture in great detail. It did help to have some good documentation written for developers, not users! Although, Hugo\u0026rsquo;s documentation can surely help itself with some cleaning and some fresher looks!\nHugo uses go templates with many extra functions and set of variables provided by Hugo. I personally feel Hugo\u0026rsquo;s template-ing system to be more flexible and easier than Mako - the one used by default by Nikola.\nI converted almost all of Mako theme from Nikola website to Hugo\u0026rsquo;s format and architecture with additions (copied features and code) from a nice theme called TranqilPeak. In particular, I liked their fonts, search feature for taxonomies pages. Copying these features also meant I had to learn a bit of javascript and css. You can find a working copy of my theme in the src branch of gihub repository of my blog. I plan to release this theme as a standalone theme in near future though.\nShortcodes Given I am using a bootstrap based theme, I like having a lot of its features available to me when I am writing in Markdown. The powerful template based shortcodes in Hugo provide a great way to write custom HTML code inside markdown. I feel Hugo shortcodes are so powerful, you could develop your own grammar of markup language in it! 😋\nSome of my shortcodes are basically based on bootstrap classes like panel, label, emphasis, highlighted text, and block quotes. I also liked the figure command provided by restructured text in Nikola. Luckily, same features are available in Hugo using a default shortcode called figure. Hugo also provides several other useful default shortcodes like youtube, ref/relref for referencing other posts etc.\nI have also some additional shortcodes for code-blocks and math. I will be detailing about them in a bit more detail in the next section. All of my shortcodes are available with the theme in the same github repo.\nOther Caveats and Fixes While converting to Hugo was almost fun, there were some caveats. The issues I faced were mainly with home page, site search, and ipython notebook posts.\nHome Page with Content and Post Lists Getting home page to work was very simple. Hugo documents page provides a very clear details about order in which various templates are looked. For home page, you will need to provide a template for index.html. Then Inside the content folder, you can put the metadata and the content for the home page in a file named _index.md.\nI also added following template code in the index.html file to get list of posts with certain tags:\n{{ $.Scratch.Add \u0026#34;mlposts\u0026#34; slice }} {{ $tags := (slice \u0026#34;Machine Learning\u0026#34; \u0026#34;EDA\u0026#34; \u0026#34;Kaggle\u0026#34; \u0026#34;ML\u0026#34; \u0026#34;Deep Learning\u0026#34; \u0026#34;DL\u0026#34; \u0026#34;Data Science\u0026#34;) }} {{ range .Site.RegularPages }} {{ $page := . }} {{ $has_common_tags := intersect $tags .Params.tags | len | lt 0 }} {{ if $has_common_tags }} {{ $.Scratch.Add \u0026#34;mlposts\u0026#34; $page }} {{ end }} {{ end }} {{ $cand := .Scratch.Get \u0026#34;mlposts\u0026#34; }} {{ range first 10 $cand }} {{ .Render \u0026#34;li\u0026#34;}} {{ end }} \r\rtipue Search Hugo has support for several output formats, including HTML and JSON. For implementing tipue search, we need to generate a JSON file with site content. This can be done by adding following to the configuration file:\n# Output formats outputs: home: [ \u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] page: [ \u0026#34;HTML\u0026#34;] \r\rand, using the following index.json template:\n{{- $.Scratch.Add \u0026#34;index\u0026#34; slice -}} {{- range where .Site.RegularPages \u0026#34;Type\u0026#34; \u0026#34;not in\u0026#34; (slice \u0026#34;page\u0026#34; \u0026#34;json\u0026#34; \u0026#34;nosearch\u0026#34;) -}} {{- $.Scratch.Add \u0026#34;index\u0026#34; (dict \u0026#34;url\u0026#34; .Permalink \u0026#34;title\u0026#34; .Title \u0026#34;text\u0026#34; .Plain \u0026#34;tags\u0026#34; (delimit .Params.tags \u0026#34;, \u0026#34;)) -}} {{- end -}} {\u0026#34;pages\u0026#34;: {{- $.Scratch.Get \u0026#34;index\u0026#34; | jsonify -}}} \r\rNow, include the following css in the \u0026lt;head\u0026gt; of your pages:\n\u0026lt;link href=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; \r\rThe following modal code is needed to display the search results, preferably at the end of the body of the HTMLpage:\n\u0026lt;div id=\u0026#34;search-resuts\u0026#34; class=\u0026#34;modal fade\u0026#34; role=\u0026#34;dialog\u0026#34; style=\u0026#34;height: 80%;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-dialog\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-header\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;close\u0026#34; data-dismiss=\u0026#34;modal\u0026#34;\u0026gt;×\u0026lt;/button\u0026gt; \u0026lt;h4 class=\u0026#34;modal-title\u0026#34;\u0026gt;Search Results:\u0026lt;/h4\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;modal-body\u0026#34; id=\u0026#34;tipue_search_content\u0026#34; style=\u0026#34;max-height: 600px; overflow-y: auto;\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;modal-footer\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default\u0026#34; data-dismiss=\u0026#34;modal\u0026#34;\u0026gt;Close\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{\\\u0026lt; panel warning \u0026gt;}} Hi there {{\\\u0026lt; /panel \u0026gt;}} \r\rFinally, the following javascript in the lower end of the body of HTML pages:\n\u0026lt;script\u0026gt; $(document).ready(function() { var url1 = \u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch_set.js\u0026#34;; var url2 = \u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch.min.js\u0026#34;; $.when( $.getScript( url1 ), $.getScript( url2 ), $.Deferred(function( deferred ){ $( deferred.resolve ); }) ).done(function() { $(\u0026#39;#tipue_search_input\u0026#39;).tipuesearch({ \u0026#39;mode\u0026#39;: \u0026#39;json\u0026#39;, \u0026#39;contentLocation\u0026#39;: \u0026#39;/index.json\u0026#39; }); $(\u0026#39;#tipue_search_input\u0026#39;).keyup(function (e) { if (e.keyCode == 13) { $(\u0026#39;#search-results\u0026#39;).modal() } }); }); }); \u0026lt;/script\u0026gt; \r\rAnd, of course you will need a form/input for performing the search:\n\u0026lt;span class=\u0026#34;navbar-form navbar-right\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;tipue_search_input\u0026#34; class=\u0026#34;form-control\u0026#34; placeholder=\u0026#34;Search\u0026#34;\u0026gt; \u0026lt;/span\u0026gt; \r\rCode Highlighting Although, by default Hugo provides code highlighting using the pygments, I prefer to use client-side highlighting using prism.js. I also use the following plugins of prism.js for line numbers, highlighting and cleanup of white space:\n Line Highlight Line Numbers Normalize Whitespace  Finally, I create a shortcode called code-block to add relevant classes and variables around \u0026lt;code\u0026gt; and \u0026lt;pre\u0026gt; tags so that prism could highlight code correctly.\njupyter Notebooks as Posts One of the advantages of using Nikola is that, it provides native support for writing Blog posts in jupyter notebooks.\nBut, on some google search, I found this neat solution at the following Blog.\nIn summary, the solution is very simple - Use the linked jupyter.css file in your template. Then for any jupyter notebook, convert it to basic HTML using the following command:\njupyter nbconvert --to html --template basic *source_file.ipynb* \r\rThen, finally, create a markdown file for your post, where simply put the contents of this HTML file as markdown supports including raw HTML code!\nLatex Math Equations I used katex for using math in markdown. I was having some issue with the multi-line display math equations, so I created shortcode called tex to write HTML code explicitly so that katex could handle that easily.\nSo there you have it. I have my Blog now up and running with Hugo. Hope I will be more active here, since it now takes only seconds to deploy once I have a post written. No excuses now! 😜\n","title":"Switching to Hugo from Nikola","url":"https://sadanand-singh.github.io/posts/nikola2hugo/"},{"tags":"Data Science, EDA","text":" One of the first tasks involved in any data science project is to get to understand the data. This can be extremely beneficial for several reasons:\n Catch mistakes in data See patterns in data Find violations of statistical assumptions Generate hypotheses etc.  We can think of this task as an exercise in summarization of the data. To summarize the main characteristics of the data, often two methods are used: numerical and graphical.\nThe numerical summary of data is done through descriptive statistics. While the graphical summary of the data is done through exploratory data analysis (EDA). In this post, we will look at both of these fundamental data science techniques in more detail using some examples.\n Descriptive Statistics Descriptive statistics are statistics that quantitatively describe or summarize features of a collection of information. Some measures that are commonly used to describe a data set are:\n Measures of Central Tendency or Measure of Location, such as mean Measures of Variability or Dispersion, such as standard deviation Measure of the shape of the distribution, such as skewness or kurtosis Relative Standing Measures, such as z-score, Quartiles etc.  Measures of Central Tendency Central tendency (or measure of central tendency) is a central or typical value for a probability distribution. Measures of central tendency are often called averages. The most common measures of central tendency are the arithmetic mean, the median and the mode.\nMean The arithmetic mean (or mean or average) is the most commonly used and readily understood measure of central tendency. In statistics, however, the term average refers to any of the measures of central tendency. If we have a data set containing the values \\(a_{1},a_{2},\\ldots ,a_{n}\\)   , then the arithmetic mean, \\(A\\)    is defined by the formula:\n$$A = \\frac{1}{n}\\sum_{i=1}^{n} a_i = \\frac{a_1 \u0026#43; a_2 \u0026#43; \\ldots \u0026#43; a_n}{n}$$     If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean. If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.\nAlthough, arithmetic mean is the most common definition of mean, several other types are means also common. Some examples are: Weighted mean, Geometric mean, Harmonic mean and Trimmed mean etc.\nMedian The median is the midpoint of the data set. This midpoint value is the point at which half the observations are above the value and half the observations are below the value. The median is determined by ranking the observations and finding the observation that are at the number \\(\\frac{[N \u0026#43; 1]}{2}\\)    in the ranked order. If the number of observations are even, then the median is the average value of the observations that are ranked at numbers \\(\\frac{[N]}{2}\\)    and \\(\\frac{[N \u0026#43; 1]}{2} \u0026#43; 1\\)   .\nMean vs Median The median and the mean both measure central tendency. But unusual values, called outliers, affect the median less than they affect the mean. When you have unusual values, you can compare the mean and the median to decide which is the better measure to use. If your data are symmetric, the mean and median are similar.  The concept of median can be generalized as quartiles. Quartiles are the three values – the first quartile at 25% ($Q_1$), the second quartile at 50% ($Q_2$ or median), and the third quartile at 75% ($Q_3$) – that divide a sample of ordered data into four equal parts.\nMode The mode is the value that appears most often in a set of data. The mode of a discrete probability distribution is the value x at which its probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled.\nThe mode can be used with mean and median to provide an overall characterization of your data distribution. The mode can also be used to identify problems in your data. For example, a distribution that has more than one mode may identify that your sample includes data from two populations. If the data contain two modes, the distribution is bimodal. If the data contain more than two modes, the distribution is multi-modal.\nMinimum and Maximum Many a times looking at the smallest and largest data and their relative positioning wrt to other central tendencies are also quite helpful.\nUse the maximum/minimum to identify a possible outliers or any data- entry errors. One of the simplest ways to assess the spread of your data is to compare the minimum and maximum. If the maximum value is very high, even when you consider the center, the spread, and the shape of the data, investigate the cause of the extreme value.\nMeasures of Variability or Dispersion Dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched. A measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse. Some common examples of dispersion measures are: Standard Deviation, Interquartile Range (IQR), Mean Absolute Difference and Median Absolute Difference etc.\nStandard Deviation The standard deviation is a measure of how spread out the data are about the mean. The symbol $\\sigma$ is often used to represent the standard deviation of a population, while $s$ is used to represent the standard deviation of a sample.\nIf we have a data set containing the values \\(a_{1},a_{2},\\ldots ,a_{n}\\)   , then the standard deviation, $\\sigma$ is defined by the formula:\n$$\\sigma = \\sqrt{\\frac{1}{n}\\Big[\\big(a_1 - A\\big)^2 + \\big(a_2 - A\\big)^2 + \\ldots + \\big(a_n - A\\big)^2\\Big]}, \\text{ where } A \\text{ is the Mean}$$\nA higher standard deviation value indicates greater spread in the data. A good rule of thumb for a [normal distribution][normal] is that approximately 68% of the values fall within one standard deviation of the mean, 95% of the values fall within two standard deviations, and 99.7% of the values fall within three standard deviations.\nInterquartile Range (IQR) The interquartile range (IQR) is the distance between the first quartile ($Q_1$) and the third quartile ($Q_3$). 50% of the data are within this range.\n$$IQR = Q_3 - Q_1$$\nThe interquartile range can be used to describe the spread of the data. As the spread of the data increases, the IQR becomes larger. It is also used to build box plots.\nRange The range is the difference between the largest and smallest data values in the sample. The range represents the interval that contains all the data values.\nThe range can be used to understand the amount of dispersion in the data. A large range value indicates greater dispersion in the data. A small range value indicates that there is less dispersion in the data. Because the range is calculated using only two data values, it is more useful with small data sets.\nMeasure of the Shape of the Distribution Generally speaking, a moment is a specific quantitative measure, used in both mechanics and statistics, of the shape of a set of points. If the points represent probability density, then the zeroth moment is the total probability (i.e. one), the first moment is the mean, the second central moment is the variance, the third central moment is the skewness, and the fourth central moment (with normalization and shift) is the kurtosis.\nWe have already seen the use of first and second moments in describing statistics. The shape of distributions are further described using higher moments as described below.\nSkewness skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its measure of central tendency. The skewness value can be positive or negative, or even undefined.\nFor a unimodal distribution, negative skew indicates that the tail on the left side of the probability density function is longer or fatter than the right side – it does not distinguish these two kinds of shape. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In multi-modal distributions and discrete distributions, skewness is very difficult to interpret.\nThere are two common definitions of skewness:\nA. Pearson Moment Coefficient of Skewness: Pearson Moment Coefficient of Skewness refers to the third standardized moment, defined as:\n$$S_{Pearson}=\\frac{E\\big[X^3\\big]-3\\mu\\sigma^2-\\mu^3}{\\sigma^3}$$\nwhere, $\\mu$ is the mean, $\\sigma$ is the standard deviation, $E$ is the expectation operator, and $X$ refers to the data points.\nB. Bowley Skewness:\nBowley skewness is a way to measure skewness purely from quartiles. One of the most popular ways to find skewness is the Pearson Mode Skewness formula. However, in order to use it you must know the mean, mode and standard deviation for your data. Sometimes you might not have that information; Instead you might have information about your quartiles.\nBowley skewness is an important quantity, if you have extreme data values (outliers) or if you have an open-ended distribution.\nMathematically, Bowley Skewness is defined as :\n$$S_{Bowley} = \\frac{Q_3 + Q_1 - 2Q_2}{Q_3 - Q_1}$$\nwhere, $Q_1$, $Q_2$ and $Q_3$, represent, first, second and third quartiles, respectively. Bowley Skewness is an absolute measure of skewness. In other words, it’s going to give you a result in the units that your distribution is in. That’s compared to the Pearson Mode Skewness, which gives you results in a dimensionless unit — the standard deviation. This means that you cannot compare the skewness of different distributions with different units using Bowley Skewness.\nKurtosis Kurtosis indicates how the peak and tails of a distribution differ from the normal distribution. Mathematically, it is the fourth standardized moment, defined as,\n$$Kurtosis = \\frac{E\\Big[\\big(X-\\mu\\big)^4\\Big]}{\\sigma^4} - 3$$\nwhere, $\\mu$ is the mean, $\\sigma$ is the standard deviation, $E$ is the expectation operator, and $X$ refers to the data points.\nUse kurtosis to initially understand general characteristics about the distribution of your data. Normally distributed data establish the baseline for kurtosis. A kurtosis value of 0 indicates that the data follow the normal distribution perfectly. A kurtosis value that significantly deviates from 0 may indicate that the data are not normally distributed.\nA distribution that has a positive kurtosis value indicates that the distribution has heavier tails and a sharper peak than the normal distribution. For example, data that follow a t-distribution have a positive kurtosis value.\nA distribution with a negative kurtosis value indicates that the distribution has lighter tails and a flatter peak than the normal distribution. For example, data that follow a beta distribution with first and second shape parameters equal to 2 have a negative kurtosis value.\nMeasures of Relative Standing A measure of relative standing is a measure of where a data value stands relative to the distribution of the whole data set. With an idea of relative standing, we can say things like, “You got a really high score compared to the rest of the class” or, “that basketball player is unusually short” etc. Some of the common measures of relative standings are: z-score, quartile and percentile.\nz-scores The z-score (or standard score) is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. Observed values above the mean have positive standard scores, while values below the mean have negative standard scores.\nMathematically, z-score of a raw score $x$ is given by,\n$$z = \\frac{x - \\mu}{\\sigma}$$\nwhere, $\\mu$ is the mean and $\\sigma$ is the standard deviation of the population.\nThe z-score is often used in the z-test in standardized testing – the analog of the Student\u0026rsquo;s t-test for a population whose parameters are known, rather than estimated. As it is very unusual to know the entire population, the t-test is much more widely used.\nQuartiles and Percentiles A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value (or score) below which 20 percent of the observations may be found. The term percentile and the related term, percentile rank, are often used in the reporting of scores from norm-referenced tests. For example, if a score is in the 86th percentile, it is higher than 86% of the other scores. The 25th percentile is also known as the first quartile ($Q_1$), the 50th percentile as the median or second quartile ($Q_2$), and the 75th percentile as the third quartile ($Q_3$).\nCorrelations Often the data that we deal with is multi-dimensional in nature. Correlation most often refers to the extent to which two variables have a linear relationship with each other. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.\nThe most familiar measure of dependence between two quantities is the Pearson product-moment correlation coefficient, or \u0026ldquo;Pearson\u0026rsquo;s correlation coefficient\u0026rdquo;, commonly called simply \u0026ldquo;the correlation coefficient\u0026rdquo;.\nThe population correlation coefficient $\\rho_{X, Y}$ between two variates $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$ is defined as:\n$$ \\rho_{X, Y} = \\frac{cov(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{E\\Big[\\big(X-\\mu_X\\big)\\big(Y-\\mu_Y\\big)\\Big]}{\\sigma_X \\sigma_Y} $$\nwhere $E$ is the expectation operator, and $cov$ means covariance.\nThere are additional alternative ways to measures of correlations. Some common examples are: Rank Correlation, Distance Correlation, polychoric correlation and correlation ratio etc. Each of such measures capture different aspects of the data and should be used with care depending on the situation.\nMost correlation measures are sensitive to the manner in which $X$ and $Y$ are sampled. Dependencies tend to be stronger if viewed over a wider range of values. Sensitivity to the data distribution can be used to an advantage. For example, scaled correlation is designed to use the sensitivity to the range in order to pick out correlations between fast components of time series.\n🔥Correlation does not imply causation.🔥 If a strong correlation is observed between two variables A and B, there are several possible explanations: (a) A influences B; (b) B influences A; \u0026copy; A and B are influenced by one or more additional variables; (d) the relationship observed between A and B was a chance error.\nSmall correlation values do not necessarily indicate that two variables are unassociated. For example, Pearson\u0026rsquo;s coefficients will underestimate the association between two variables that show a quadratic relationship. You should always examine the scatter plot in the EDA.\nThe correlation of two variables that both have been recorded repeatedly over time can be misleading and spurious. Time trends should be removed from such data before attempting to measure correlation. Caution should be used in interpreting results of correlation analysis when large numbers of variables have been examined, resulting in a large number of correlation coefficients.\nExploratory Data Analysis (EDA) Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. The objectives of EDA are to:\n Suggest hypotheses about the causes of observed phenomena Assess assumptions on which statistical inference will be based Support the selection of appropriate statistical tools and techniques Provide a basis for further data collection through surveys or experiments  Typical graphical techniques used in EDA are:\n Box Plot Histogram Multi-Vari Chart Run Chart Pareto Chart Scatter Plot Stem-and-Leaf Plot Parallel Coordinates Odd Ratio Multidimensional Scaling Targeted Projection Pursuit Principal Component Analysis (PCA) Multi-linear PCA Dimensionality Reduction Nonlinear Dimensionality Reduction (NLDR)  Typical quantitative techniques used in EDA are:\n Median Polish Trimean Ordination  I have already covered some examples of many of these techniques in my past posts on EDA of Single Variable , Two variables and Multiple Variables.\nI will be going through mathematical details of some of others in future posts.\n","title":"Descriptive Statistics","url":"https://sadanand-singh.github.io/posts/descriptivestats/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":" Arch Linux is a general purpose GNU/Linux distribution that provides most up-to-date software by following the rolling-release model. Arch Linux allows you to use updated cutting-edge software and packages as soon as the developers released them. KDE Plasma 5 is the current generation of the desktop environment created by KDE primarily for Linux systems.\nIn this post, we will do a complete installation of Arch Linux with Plasma 5 as the desktop environment. Our setup will also involve encryption of the root partition that will be formatted in btrfs. This post is an updated and a more complete version of my previous posts on Arch Linux and Plasma 5 Installation.\n  System Details For reference, my installation system is a slightly upgraded form of my original desktop:\n i7 4790 3.6 GHz (Haswell) ASRock Z97 Extreme6 LGA 1150 Intel Z97 HDMI SATA USB 3.0 ADATA XPG V1.0 DDR3 1866 4x4 GB RAM OCZ Vertex 460A Series 2.5\u0026rdquo; 240 GB WD Blue 1TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache WD Blue 3TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache Ultra LSP V2 650 Watt PSU Cooler Master - MasterCase Pro 5 Asus BW-12B1ST/BLK/G/AS Blue Ray Burner Samsung U28E590D 28-Inch UHD LED-Lit 4K Monitor Nvidia GeForce GTX 750 Ti GPU  Base Installation NOTE I do not wish to repeat Arch Installation Guide here.\nDo not forget about Arch Wiki, the best documentation in the world! Most of the content in this post has been compiled from the Arch wiki.  Before beginning this guide, I would assume that you have a bootable USB of the latest Arch Linux Installer. If not, please follow the Arch wiki guide to create one for you.\nOnce you login in the installer disk, You will be logged in on the first virtual console as the root user, and presented with a zsh shell prompt. I will assume you have an Ethernet connection and hence will be connected to Internet by default. If you have to rely on wifi, please refer to the Wireless Network Configuration wiki page for the detailed setup. You must have Internet connection at this stage before proceeding any further.\nYou should boot into UEFI mode if you have a UEFI motherboard and UEFI mode enabled.\nTo verify you have booted in UEFU mode, run:\n$ efivar -l \r\rThis should give you a list of set UEFI variables. Please look at the Arch Installation Guide in case you do not get any list of UEFI variables.\nThe very first thing that annoys me in the virtual console is how tiny all the fonts are. We will fix that by running the following commands:\n$ pacman -Sy $ pacman -S terminus-font $ setfont ter-132n \r\rWe are all set to get started with the actual installation process.\nHDDs Partitioning First find the hard drive that you will be using as the main/root disk.\n$ cat /proc/partitions # OUTPUT eg. # major minor #blocks name # 8 0 268435456 sda # 9 0 268435456 sdb # 19 0 268435456 sdc # 11 0 759808 sr0 # 7 0 328616 loop0 \r\rSay, we will be using /dev/sda as the main disk and /dev/sdb as /data and /dev/sdc as /media .\nBecause we are creating an encrypted file system it’s a good idea to overwrite it with random data.\nWe’ll use badblocks for this. Another method is to use dd if=/dev/urandom of=/dev/xxx, the dd method is probably the best method, but is a lot slower. The following step should take about 20 minutes on a 240 GB SSD.\n$ badblocks -c 10240 -s -w -t random -v /dev/sda \r\rNext, we will create GPT partitions on all disks using gdisk command.\n$ dd if=/dev/zero of=/dev/sda bs=1M count=5000 $ gdisk /dev/sda Found invalid MBR and corrupt GPT. What do you want to do? (Using the GPT MAY permit recovery of GPT data.) 1 - Use current GPT 2 - Create blank GPT \r\rThen press 2 to create a blank GPT and start fresh\nZAP: $ press x - to go to extended menu $ press z - to zap $ press Y - to confirm $ press Y - to delete MBR \r\rIt might now kick us out of gdisk, so get back into it:\n$ gdisk /dev/sda $ Command (? for help): m $ Command (? for help): n $ Partition number (1-128, default 1): $ First sector (34-500118158, default = 2048) or {\u0026#43;-}size{KMGTP}: $ Last sector (2048-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: 512M $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): ef00 $ Changed type of partition to \u0026#39;EFI System\u0026#39; $ Partition number (2-128, default 2): $ First sector (34-500118, default = 16779264) or {\u0026#43;-}size{KMGTP}: $ Last sector (16779264-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): $ Changed type of partition to \u0026#39;Linux filesystem\u0026#39; $ Command (? for help): p $ Press w to write to disk $ Press Y to confirm \r\rRepeat the above procedure for /dev/sdb and /dev/sdc, but create just one partition with all values as default. At the end we will have three partitions: /dev/sda1, /dev/sda2, /dev/sdb1 and /dev/sdc1.\nSetup Disk Encryption Our /boot partition will be on /dev/sda1, while the main installation will be on /dev/sda2. In this setup, we will be enabling full encryption on /dev/sda2 only.\nIn order to enable disk encryption, we will first create a root luks volume, open it and then format it.\n# first, we need to prepare the encrypted (outer) volume $ cryptsetup --cipher aes-xts-plain64 --hash sha512 --use-random --verify-passphrase luksFormat /dev/sda2 # I really hope I don\u0026#39;t have to lecture you on NOT LOSING this # password, lest all of your data will be forever inaccessible, # right? # then, we actually open it as a block device, and format the # inner volume later $ cryptsetup luksOpen /dev/sda2 root \r\rAutomatic Key Login from an USB/SD Card If you want to automatically login the encrypted disk password from an externally attached USB or SD card, you will first need to create a key file.\n$ dd bs=512 count=4 if=/dev/urandom of=KEYFILE   Then, add this key to the luks container, so that it can be later used to open the encrypted drive.\n$ cryptsetup luksAddKey /dev/sda2 KEYFILE   Note that the KEYFILE here should be kept on a separate USB drive or SD card. The recommended way of using such a disk would be as follows:\n# assuming our USB of interest is /dev/sdd and can be format # # Format the drive $ dd if=/dev/zero of=/dev/sdd bs=1M # Create partitions using gdisk # $ gdisk /dev/sdd # # Follow along to create one partition (/dev/sdd1) of type 0700 # # format /dev/sdd1 $ mkfs.fat /dev/sdd1 # mount the newly format disk on /mnt and then copy the KEYFILE $ mount /dev/sdd1 /mnt $ mv KEYFILE /mnt/KEYFILE $ umount /mnt   We will be later using this KEYFILE in boot loader setup.  Format HDDs At this point, we have following drives ready for format: /dev/sda1, /dev/mapper/root, /dev/sdb1 and /dev/sdc1.\nThese can be format as follows:\n$ mkfs.vfat -F32 /dev/sda1 $ mkfs.btrfs -L arch /dev/mapper/root $ mkfs.btrfs -L data /dev/sdb1 $ mkfs.btrfs -L media /dev/sdc1 \r\rNow, we will create btrfs subvolumes and mount them properly for installation and final setup.\n$ mount /dev/mapper/root /mnt $ btrfs subvolume create /mnt/ROOT $ btrfs subvolume create /mnt/home $ umount /mnt $ mount /dev/sdb1 /mnt $ btrfs subvolume create /mnt/data $ umount /mnt $ mount /dev/sdc1 /mnt $ btrfs subvolume create /mnt/media $ umount /mnt \r\rNow, once the sub-volumes have been created, we will mount them in appropriate locations with optimal flags.\n$ SSD_MOUNTS=\u0026#34;rw,noatime,nodev,compress=lzo,ssd,discard, space_cache,autodefrag,inode_cache\u0026#34; $ HDD_MOUNTS=\u0026#34;rw,nosuid,nodev,relatime,space_cache\u0026#34; $ EFI_MOUNTS=\u0026#34;rw,noatime,discard,nodev,nosuid,noexec\u0026#34; $ mount -o $SSD_MOUNTS,subvol=ROOT /dev/mapper/root /mnt $ mkdir -p /mnt/home $ mkdir -p /mnt/data $ mkdir -p /mnt/media $ mount -o $SSD_MOUNTS,nosuid,subvol=home /dev/mapper/root /mnt/home $ mount -o $HDD_MOUNTS,subvol=data /dev/sdb1 /mnt/data $ mount -o $HDD_MOUNTS,subvol=media /dev/sdc1 /mnt/media $ mkdir -p /mnt/boot $ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot \r\rSave the current /etc/resolv.conf file for future use! $ cp /etc/resolv.conf /mnt/etc/resolv.conf \r\rBase System Installation Now, we will do the actually installation of base packages.\n$ pacstrap /mnt base base-devel btrfs-progs $ genfstab -U -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab \r\rInitial System Setup Edit the /mnt/ect/fstab file to add following /tmp mounts.\ntmpfs /tmp tmpfs rw,nodev,nosuid 0 0 tmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0 \r\rFinally bind root for installation.\n$ arch-chroot /mnt \u0026#34;bash\u0026#34; $ pacman -Syy $ pacman -Syu $ pacman -S sudo vim $ vim /etc/locale.gen ... # en_SG ISO-8859-1 en_US.UTF-8 UTF-8 # en_US ISO-8859-1 ... $ locale-gen $ echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf $ export LANG=en_US.UTF-8 $ ls -l /usr/share/zoneinfo $ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime $ hwclock --systohc --utc $ sed -i \u0026#34;s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\u0026#34; /etc/sudoers $ HOSTNAME=euler $ echo $HOSTNAME \u0026gt; /etc/hostname $ passwd \r\rWe will also add hostname to our /etc/hosts file:\n$ vim /etc/hosts ... 127.0.0.1 localhost.localdomain localhost ::1 localhost.localdomain localhost 127.0.0.1 $HOSTNAME.localdomain $HOSTNAME ... \r\rWe also need to fix the mkinitcpio.conf to contain what we actually need.\n$ vi /etc/mkinitcpio.conf # on the MODULES section, add \u0026#34;vfat aes_x86_64 crc32c-intel\u0026#34; # (and whatever else you know your hardware needs. Mine needs i915 too) # on the BINARIES section, add \u0026#34;/usr/bin/btrfsck\u0026#34;, since it\u0026#39;s useful # to have in case your filesystem has troubles # on the HOOKS section: # - add \u0026#34;encrypt\u0026#34; before \u0026#34;filesystems\u0026#34; # - remove \u0026#34;fsck\u0026#34; and # - add \u0026#34;btrfs\u0026#34; at the end # # re-generate your initrd images mkinitcpio -p linux \r\rBoot Manager Setup systemd-boot, previously called gummiboot, is a simple UEFI boot manager which executes configured EFI images. The default entry is selected by a configured pattern (glob) or an on-screen menu. It is included with the systemd, which is installed on an Arch systems by default.\nAssuming /boot is your boot drive, first run the following command to get started:\n$ bootctl --path=/boot install \r\rIt will copy the systemd-boot binary to your EFI System Partition ( /boot/EFI/systemd/systemd-bootx64.efi and /boot/EFI/Boot/BOOTX64.EFI - both of which are identical - on x64 systems ) and add systemd-boot itself as the default EFI application (default boot entry) loaded by the EFI Boot Manager.\nFinally to configure out boot loader, we will need the UUID of some of our hard drives. These can be easily done using the blkid command.\n$ blkid /dev/sda1 \u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/sda2 \u0026gt;\u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/mapper/root \u0026gt;\u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/sdd1 \u0026gt;\u0026gt; /boot/loader/entries/arch.conf # for this example, I\u0026#39;m going to mark them like this: # /dev/sda1 LABEL=\u0026#34;EFI\u0026#34; UUID=11111111-1111-1111-1111-111111111111 # /dev/sda2 LABEL=\u0026#34;arch\u0026#34; UUID=33333333-3333-3333-3333-333333333333 # /dev/mapper/root LABEL=\u0026#34;Arch Linux\u0026#34; UUID=44444444-4444-4444-4444-444444444444 # /dev/sdd1 LABEL=\u0026#34;USB\u0026#34; UUID=0000-0000 # this is the drive where KEYFILE exists \r\rNow, make sure that the following two files look as follows, where UUIDs is the value obtained from above commands.\nDo not forget to modify UUIDs and KEYFIL entries! $ vim /boot/loader/loader.conf ... timeout 3 default arch ... $ vim /boot/loader/entries/arch.conf ... title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options ro cryptdevice=UUID=33333333-3333-3333-3333-333333333333:luks-33333333-3333-3333-3333-333333333333 root=UUID=44444444-4444-4444-4444-444444444444 rootfstype=btrfs rootflags=subvol=ROOT cryptkey=UUID=0000-0000:vfat:KEYFILE ... \r\rNetwork Setup At first we will need to figure out the Ethernet controller on which cable is connected.\n$ networkctl # # IDX LINK TYPE OPERATIONAL SETUP # 1 lo loopback carrier unmanaged # 2 enp3s0 ether no-carrier unmanaged # 3 wlp6s0 wlan no-carrier unmanaged # 4 enp0s25 ether routable configured # \r\rIn my case, the name of the device is enp0s25.\nUsing this name of the device, we need to configure, and enable the systemd-networkd.service service.\nNote that we will using the resolv.conf that we saved from this session.\nNetwork configurations are stored as *.network in /etc/systemd/network. We need to create ours as follows.:\n$ vim /etc/systemd/network/50-wired.network $ ... [Match] Name=enp0s25 [Network] DHCP=ipv4 ... $ \r\rNow enable the networkd services:\nsystemctl enable systemd-networkd.service \r\rYour network should be ready for the first use!\nSync time automatically using the systemd service:\n$ vim /etc/systemd/timesyncd.conf $ ... [Time] NTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org FallbackNTP=0.pool.ntp.org 1.pool.ntp.org 0.fr.pool.ntp.org ... $ $ timedatectl set-ntp true $ timedatectl status $ ... Local time: Tue 2016-09-20 16:40:44 PDT Universal time: Tue 2016-09-20 23:40:44 UTC RTC time: Tue 2016-09-20 23:40:44 Time zone: US/Pacific (PDT, -0700) Network time on: yes NTP synchronized: yes RTC in local TZ: no ... $ \r\rAvahi is a tool that allows programs to publish and discover services and hosts running on a local network with no specific configuration. For example you can plug into a network and instantly find printers to print to, files to look at and people to talk to.\nWe can easily set it up it as follows:\n$ pacman -S avahi nss-mdns $ systemctl enable avahi-daemon.service \r\rWe will also install terminus-font on our system to work with proper fonts on first boot.\n$ pacman -S terminus-font \r\rFirst Boot Installations Now we are ready for the first boot! Run the following command:\n$ exit $ umount -R /mnt $ reboot \r\rAfter your new system boots, Network should be setup at the start. Check the status of network using:\n# Set readable font first! setfont ter-132n ping google.com -c 2 # # PING google.com (10.38.24.84) 56(84) bytes of data. # 64 bytes from google.com (10.38.24.84): icmp_seq=1 ttl=64 time=0.022 ms # 64 bytes from google.com (10.38.24.84): icmp_seq=2 ttl=64 time=0.023 ms # # --- google.com ping statistics --- # 2 packets transmitted, 2 received, 0% packet loss, time 999ms # rtt min/avg/max/mdev = 0.022/0.022/0.023/0.004 ms # \r\rIf you do not get this output, please follow the troubleshooting links at Arch Wiki on setting up network.\nAdding New User Choose $USERNAME per your liking. I chose ssingh, so in future commands whenever you see ssingh please replace it with your $USERNAME.\n$ pacman -S zsh $ useradd -m -G wheel -s usr/bin/zsh $USERNAME $ chfn --full-name \u0026#34;$FULL_NAME\u0026#34; $USERNAME $ passwd $USERNAME \r\rGUI Installation with nvidia I will be assuming you have an NVIDIA card for graphics installation.\nTo setup a graphical desktop, first we need to install some basic X related packages, and some essential packages (including fonts):\n$ pacman -S xorg-server nvidia nvidia-libgl nvidia-settings mesa \r\rTo avoid the possibility of forgetting to update your initramfs after an nvidia upgrade, you have to use a pacman hook like this:\n$ vim /etc/pacman.d/hooks/nvidia.hook $ ... [Trigger] Operation=Install Operation=Upgrade Operation=Remove Type=Package Target=nvidia [Action] Depends=mkinitcpio When=PostTransaction Exec=/usr/bin/mkinitcpio -p linux ... $ \r\rNvidia has a daemon that is to be run at boot. To start the persistence daemon at boot, enable the nvidia-persistenced.service.\n$ systemctl enable nvidia-persistenced.service $ systemctl start nvidia-persistenced.service \r\rHow to Avoid Screen Tearing Tearing can be avoided by forcing a full composition pipeline, regardless of the compositor you are using.\nIn order to make this change permanent, We will need to edit nvidia configuration file. Since, by default there aren\u0026rsquo;t any, we will first need to create one.\n$ nvidia-xconfig $ mv /etc/X11/xorg.cong /etc/X11/xorg.conf.d/20-nvidia.conf # # Edit this file as follows: vim /etc/X11/xorg.conf.d/20-nvidia.conf # ------------------------------------------- # Section \u0026#34;Screen\u0026#34; # Identifier \u0026#34;Screen0\u0026#34; # Option \u0026#34;metamodes\u0026#34; \u0026#34;nvidia-auto-select \u0026#43;0\u0026#43;0 { ForceFullCompositionPipeline = On }\u0026#34; # Option \u0026#34;AllowIndirectGLXProtocol\u0026#34; \u0026#34;off\u0026#34; # Option \u0026#34;TripleBuffer\u0026#34; \u0026#34;on\u0026#34; # EndSection [...] # Section \u0026#34;Device\u0026#34; # [...] # Option \u0026#34;TripleBuffer\u0026#34; \u0026#34;True\u0026#34; # [...] # EndSection # [...] # ------------------------------------------------     Specific for Plasma 5, we will also create the following file to avoid any tearing in Plasma.\n$ vim /etc/profile.d/kwin.sh $ ... export KWIN_TRIPLE_BUFFER=1 ... \r\rHow to Enable Better Resolution During Boot The kernel compiled in efifb module supports high-resolution nvidia console on EFI systems. This can enabled by enabling the DRM kernel mode setting.\nFirst, we will need to add following to MODULES section of the mkinitcpio.conf file:\n nvidia nvidia_modeset nvidia_uvm nvidia_drm  We will also need to pass the nvidia-drm.modeset=1 kernel parameter during the boot.\n$ vim /etc/mkinitcpio.conf $ ... MODULES=\u0026#34;vfat aes_x86_64 crc32c-intel nvidia nvidia_modeset nvidia_uvm nvidia_drm\u0026#34; ... $ $ vim /boot/loader/entries/arch.conf $ ... options ro cryptdevice=UUID=:luks- root=UUID= rootfstype=btrfs rootflags=subvol=ROOT cryptkey=UUID=:vfat:deepmind20170602 nvidia-drm.modeset=1 ... $ $ mkinitcpio -p linux     Plasma 5 Installation and Setup We can now proceed with the installation of Plasma 5. In the process, we will also install some useful fonts.\n$ pacman -S ttf-hack ttf-anonymous-pro $ pacman -S ttf-dejavu ttf-freefont ttf-liberation $ pacman -S plasma-meta dolphin kdialog kfind $ pacman -S konsole gwenview okular spectacle kio-extras $ pacman -S kompare dolphin-plugins kwallet kwalletmanager $ pacman -S ark yakuake flite \r\rWe will also need to select proper themes for the Plasma 5 display manager sddm and then enable its systemd service.\n$ vim /etc/sddm.conf .... [Theme] # Current theme name Current=breeze # Cursor theme used in the greeter CursorTheme=breeze_cursors ... $ systemctl enable sddm $ reboot \r\rOnce, we boot into the new system, we should have a basic Plasma 5 desktop waiting for you. In the following section, we will be do installation and modifications to the system that I prefer.\nPost Installation Setup Plasma 5 provides a handy network manager applet. However, in order to use it properly we will need the NetworkManager service to be enabled. This applet allows user specific enabling of wifi, ethernet or even VPN connections.\n$ sudo pacman -S networkmanager $ systemctl enable NetworkManager.service $ systemctl start NetworkManager.service \r\rWe can also automate the hostname setup using the following systemd command:\n$ hostnamectl set-hostname $HOSTNAME \r\rSelecting pacman Mirrors The pacman package provides a \u0026ldquo;bash\u0026rdquo; script, /usr/bin/rankmirrors, which can be used to rank the mirrors according to their connection and opening speeds to take advantage of using the fastest local mirror.\nWe will do this only on the US based mirrors. First make a copy of the mirrors list file and then delete all non-US mirrors. We will then rankmirrors script on the modified list to get the top 6 mirrors for our regular use.\n$ cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup $ cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.us $ vim /etc/pacman.d/mirrorlist.us .... # Delete all non-US servers .... $ rankmirrors -n 6 /etc/pacman.d/mirrorlist.us \u0026gt; /etc/pacman.d/mirrorlist \r\rSetup AUR AUR is a community-driven repository for Arch users. This allows you to install many popular packages that are otherwise not available through core repositories.\nIn order to make all types of installations uniform, I use pacaur as the preferred tool for installing all packages. One the biggest advantages of pacaur is that is uses exactly the same options that regular pacman uses.\nIn order to install pacuar, first install dependencies.\n$ sudo pacman -S expac yajl curl gnupg --noconfirm \r\rCreate a temp directory for building packages:\n$ mkdir ~/temp $ cp ~ temp \r\rInstall cower first and then pacaur:\n$ gpg --recv-keys --keyserver hkp://pgp.mit.edu 1EB2638FF56C0C53 $ curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=cower $ makepkg -i PKGBUILD --noconfirm $ curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=pacaur $ makepkg -i PKGBUILD --noconfirm # Finally cleanup and remove the temp directory $ cd ~ $ rm -r ~/temp \r\rAudio Setup This is pretty simple. Install following packages and you should be done:\n$ sudo pacaur -S alsa-utils pulseaudio pulseaudio-alsa mpv $ sudo pacaur -S libcanberra-pulse libcanberra-gstreamer $ sudo pacaur -S vlc-qt5 \r\rNow start the pulseaudio service.\n$ systemctl --user enable pulseaudio.socket \r\rWeb Browsers My preferred choice of browsers is google chrome. However, it is also good to have the KDE native qupzilla.\n$ sudo pacaur -S google-chrome qupzilla \r\rProfile-sync-daemon (psd) is a tiny pseudo-daemon designed to manage browser profile(s) in tmpfs and to periodically sync back to the physical disc (HDD/SSD). This is accomplished by an innovative use of rsync to maintain synchronization between a tmpfs copy and media-bound backup of the browser profile(s). These features of psd leads to following benefits:\n Transparent user experience Reduced wear to physical drives, and Speed  To setup. first install the profile-sync-daemon package.\nsudo pacaur -S profile-sync-daemon \r\rRun psd the first time which will create a configuration file at \\$XDG_CONFIG_HOME/psd/psd.conf which contains all settings.\n$ psd # First time running psd so please edit # /home/$USERNAME/.config/psd/psd.conf to your liking and run again. \r\rIn the config file change the BROWSERS variables to google-chrome qupzilla. Also, enable the use of overlayfs to improve sync speed and to use a smaller memory footprint. Do this in the USE_OVERLAYFS=\u0026ldquo;yes\u0026rdquo; variable.\nNote: USE_OVERLAYFS feature requires a Linux kernel version of 3.18.0 or greater to work. In order to use the OVERLAYFS feature, you will also need to give sudo permissions to psd-helper as follows (replace $USERNAME accordingly):\n$ vim /etc/sudoers ... $USERNAME ALL=(ALL) NOPASSWD: /usr/bin/psd-overlay-helper ... \r\rVerify the working of configuration using the preview mode of psd:\npsd p \r\rGoogle Chrome by default uses kdewallet to manage passwords, where as Qupzilla does not. You can change that in its settings.\ngit Setup Install git and setup some global options as below:\n$ sudo pacaur -S git $ $ vim ~/.gitconfig ... [user] name = Sadanand Singh email = EMAIL_ADDRESS [color] ui = auto [status] showuntrackedfiles = no [alias] gist = log --graph --oneline --all --decorate --date-order find = log --graph --oneline --all --decorate --date-order --regexp-ignore-case --extended-regexp --grep rfind = log --graph --oneline --all --decorate --date-order --regexp-ignore-case --extended-regexp --invert-grep --grep search = grep --line-number --ignore-case -E -I [pager] status = true [push] default = matching [merge] tool = meld [diff] tool = meld [help] autocorrect = 1 ... \r\rssh Setup To get started first install the openssh package.\nsudo pacaur -S openssh \r\rThe ssh server can be started using the systemd service. Before starting the service, however, we want to generate ssh keys and setup the server for login based only on keys.\n$ ssh-keygen -t ed25519 $ # Create a .ssh/config file for rmate usage in sublime text $ vim ~/.ssh/config ... RemoteForward 52698 localhost:52698 ... $ # Create ~/.ssh/authorized_keys file with list of machines that # are allowed to login to this machine. $ touch ~/.ssh/authorized_keys $ # Finally edit the /etc/ssh/sshd_config # file to disable Password based logins $ sudo vim /etc/ssh/sshd_config ... PasswordAuthentication no ... \r\rFurthermore, before enabling the sshd service, please also ensure to copy your keys to all your relevant other servers and places like github.\nWe can now use systemd to start the ssh service.\n$ systemctl enable sshd.socket $ systemctl start sshd.socket \r\rzsh Setup During the user creation, we already installed the zsh shell. We have also activated a basic setup at first login by the user.\nIn this section, we will be installing my variation of zprezto package to manage zsh configurations.\nFirst install the main zprezto package:\n$ git clone --recursive https://github.com/sorin-ionescu/prezto.git \u0026#34;${ZDOTDIR:-$HOME}/.zprezto\u0026#34; $ $ setopt EXTENDED_GLOB $ for rcfile in \u0026#34;${ZDOTDIR:-$HOME}\u0026#34;/.zprezto/runcoms/^README.md(.N); do ln -sf \u0026#34;$rcfile\u0026#34; \u0026#34;${ZDOTDIR:-$HOME}/.${rcfile:t}\u0026#34; done $ \r\rNow, We will add my version of prezto to the same git repo.\n$ cd ~/.zprezto $ git remote add personal git@github.com:sadanand-singh/My-Zprezto.git $ git pull personal arch $ git checkout arch $ git merge master \r\rAnd we are all setup for using zsh!\ngpg Setup We have already installed the gnupg package during the pacaur installation. We will first either import our already existing private keys(s) or create one.\nOnce We have our keys setup, edit keys to change trust level.\nOnce all keys are setup, we need to gpg-agent configuration file:\n$ vim ~/.gnupg/gpg-agent.conf .. enable-ssh-support default-cache-ttl-ssh 10800 default-cache-ttl 10800 max-cache-ttl-ssh 10800 ... $ \r\rAlso, add following to your .zshrc or .\u0026ldquo;bash\u0026rdquo;rc file. If you are using my zprezto setup, you already have this!\n$ vim ~/.zshrc ... # set GPG TTY export GPG_TTY=$(tty) # Refresh gpg-agent tty in case user switches into an X Session gpg-connect-agent updatestartuptty /bye \u0026gt;/dev/null # Set SSH to use gpg-agent unset SSH_AGENT_PID if [ \u0026#34;${gnupg_SSH_AUTH_SOCK_by:-0}\u0026#34; -ne $$ ]; then export SSH_AUTH_SOCK=\u0026#34;/run/user/$UID/gnupg/S.gpg-agent.ssh\u0026#34; fi ... $ \r\rNow, simply start the following systemd sockets as user:\n$ systemctl --user enable gpg-agent.socket $ systemctl --user enable gpg-agent-ssh.socket $ systemctl --user enable dirmngr.socket $ systemctl --user enable gpg-agent-browser.socket $ $ systemctl --user start gpg-agent.socket $ systemctl --user start gpg-agent-ssh.socket $ systemctl --user start dirmngr.socket $ systemctl --user start gpg-agent-browser.socket \r\rFinally add your ssh key to ssh agent.\n$ ssh-add ~/.ssh/id_ed25519 \r\rUser Wallpapers You can store your own wallpapers at the following location. A good place to get some good wallpapers are KaOS Wallpapers.\n$ mkdir -p $ $HOME/.local/wallpapers $ cp SOME_JPEG $HOME/.local/wallpapers/ \r\rconky Setup First installed the conky package with lua and nvidia support:\n$ paci conky-lua-nv \r\rThen, copy your conky configuration at \\$HOME/.config/conky/conky.conf.\n$ mkdir -p $HOME/.config/conky # Generate sample conky config file $ conky -C \u0026gt; $HOME/.config/conky/conky.conf $ # start conky in background $ conky \u0026amp; \r\rHere, I have also put my simple configuration file:\nconky.config = { background = true, use_xft = true, xftalpha = 0.2, update_interval = 1, total_run_times = 0, own_window_argb_visual = true, own_window = true, own_window_type = \u0026#39;dock\u0026#39;, own_window_transparent = true, own_window_hints = \u0026#39;undecorated,below,sticky,skip_taskbar,skip_pager\u0026#39;, double_buffer = true, draw_shades = false, draw_outline = false, draw_borders = false, draw_graph_borders = false, stippled_borders = 0, border_width = 0, default_color = \u0026#39;white\u0026#39;, default_shade_color = \u0026#39;#000000\u0026#39;, default_outline_color = \u0026#39;#000000\u0026#39;, minimum_width = 2500, minimum_height = 3500, maximum_width = 2500, gap_x = 2980, gap_y = 0, alignment = \u0026#39;top_left\u0026#39;, no_buffers = true, uppercase = false, cpu_avg_samples = 2, net_avg_samples = 2, --short_units = true, text_buffer_size = 2048, use_spacer = \u0026#39;none\u0026#39;, override_utf8_locale = true, color1 = \u0026#39;#424240\u0026#39;, color2 = \u0026#39;2a2b2f\u0026#39;, color3 = \u0026#39;#FF4B4C\u0026#39;,--0E87E4 color4 = \u0026#39;#73bcca\u0026#39;, own_window_argb_value = 0, --own_window_colour = \u0026#39;#000000\u0026#39;, --lua_load rings-v1.2.1.lua lua_draw_hook_pre = \u0026#39;ring_stats\u0026#39;, --lua_load lilas_rings.lua lua_draw_hook_post = \u0026#39;main\u0026#39;, }; conky.text = [[ ${goto 200}${voffset 100}${color2}${font Nothing You Could Do:size=50}${time %I:%M}${font Nothing You Could Do:size=20}${time %p} ${goto 185}${voffset 10}${color4}${font Bad Script:size=30}${time %A} ${goto 185}${voffset -35}${font Bad Script:size=18}${time %d %B, %Y} ${goto -80}${voffset -35}${font Pompiere:size=11}${color 3eafe8}//${color4} CPU: ${execi 1000 cat /proc/cpuinfo | grep \u0026#39;model name\u0026#39; | sed -e \u0026#39;s/model name.*: //\u0026#39;| uniq | cut -c 19-25} ${color ff3d3d}${hwmon 0 temp 1}°C ${color 3eafe8}//${color4} Load: ${color ff3d3d} ${cpu cpu0}% ${color 3eafe8}// RAM:${color ff3d3d} ${memperc}% / $memmax ${color 3eafe8}// ${goto -80}${voffset -35}${font Pompiere:size=11}${color 3eafe8}//${color4} GPU: ${execi 1000000 nvidia-smi --query-gpu=\u0026#34;name,driver_version\u0026#34; --format=\u0026#34;csv,noheader\u0026#34; | cut -c 9-18} ${color ff3d3d} ${nvidia temp}°C ${color 3eafe8}//${color4} Load: ${color ff3d3d}${exec nvidia-smi --query-gpu=\u0026#34;utilization.gpu\u0026#34; --format=\u0026#34;csv,noheader\u0026#34;} ${color 3eafe8}// Free: ${color ff3d3d} ${exec nvidia-smi --query-gpu=\u0026#34;memory.free\u0026#34; --format=\u0026#34;csv,noheader\u0026#34;} ${color 3eafe8}// ]]; \r\rSoftware Installations Here is a running list of other common softwares that I install.\n$ paci spotify tmux tree dropbox thesilver_searcher $ paci digikam imagemagick \r\rI also add the following repository to install the Sublime Text editor. Refer to my previous post  for details on setting up Sublime Text.\n$ curl -O https://download.sublimetext.com/sublimehq-pub.gpg $ sudo pacman-key --add sublimehq-pub.gpg $ sudo pacman-key --lsign-key 8A8F901A $ rm sublimehq-pub.gpg $ $ echo -e \u0026#34;\\n[sublime-text]\\nServer = https://download.sublimetext.com/arch/dev/x86_64\u0026#34; | sudo tee -a /etc/pacman.conf \r\rNow we can install sublime-text as:\n$ paci sublime-text/sublime-text \r\rThis brings us to the conclusion of this installation guide. Hope many of you find it useful. Please drop your comments below if you have any suggestions for improvements etc.\n","title":"My Arch Linux Setup with Plasma 5","url":"https://sadanand-singh.github.io/posts/completesetuparchplasma/"},{"tags":"Editor","text":"I have been using Sublime text as my primary editor for some time now. Here I wanted to share my current setup for the editor including all settings, packages, shortcut keys and themes.\n\n  Packages First thing you will need to install is the Package Control. This can be easily done by following the directions at their installation instructions.\nOnce you have installed the package manager and restarted sublime text, now you can install all other packages using the powerful command pallet. Hit ctrl + shift + P and type Install, choose Package Control : Install Package. Now you can search for any package that you wish to install, and then press Enter to install it.\nHere is a list of packages that I currently use:\n Alignment Bracket Highlighter C++11 Column Select DocBlockr_Python GitGutter MagicPython rsub Search In Project SublimeLinter SublimeLinter-flake8  Alignment provides a simple key-binding for aligning multi-line and multiple selections. Bracket Highlighter, as the name suggests, matches a variety of brackets such as: [], (), {}, \u0026quot;\u0026quot;, '', \u0026lt;tag\u0026gt;\u0026lt;/tag\u0026gt;, and even custom brackets. C++11 provides better coloring scheme and syntax highlighting for C++11 syntax.\nColumn Select plug-in provides an alternate behavior for Sublime keyboard column selection. The differences are:\n Allows reversing direction (go down too far, just go back up). Added PageUp/PageDown, Home/End, and mouse selection. Skip rows that are too short. If you start at the end of a line, then it will stay at the end of each line.  DocBlockr_Python makes writing documentation a breeze for python code. GitGutter is a handy plug-in to show information about files in a git repository. Main Features are:\n Gutter Icons indicating inserted, modified or deleted lines Diff Popup with details about modified lines Status Bar Text with information about file and repository Jumping Between Changes to easily navigate between modified lines  MagicPython is a package with preferences and syntax highlighter for cutting edge Python 3. It is meant to be a drop-in replacement for the default Python package. MagicPython correctly highlights all Python 3.5 and 3.6 syntax features, including type annotations, f-strings and regular expressions. It is built from scratch for robustness with an extensive test suite.\nrsub is an implementation of TextMate 2\u0026rsquo;s rmate feature for Sublime Text, allowing files to be edited on a remote server using ssh port forwarding / tunneling. Please make sure you have installed a version of rmate and are using correct port forwarding.\nSearch in Project lets you use your favorite search tool (grep, ack, ag, pt, rg, git grep, or findstr) to find strings across your entire current Sublime Text project. I personally use the silver_seracher (ag) for this purpose.\nSublimeLinter and SublimeLinter-flake8 is plug-in that provides an interface to flake8. It will be used with files that have the Python syntax.\nShortcut Keys Here is a summary of my key map:\n[ { \u0026#34;keys\u0026#34;: [\u0026#34;shift\u0026#43;alt\u0026#43;a\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;find_all_under\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;control\u0026#43;v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste_and_indent\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;control\u0026#43;shift\u0026#43;v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;;\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;alignment\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;up\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;lines\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;down\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;lines\u0026#34;, \u0026#34;forward\u0026#34;: true}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;pageup\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;pages\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;pagedown\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;pages\u0026#34;, \u0026#34;forward\u0026#34;: true}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;home\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;end\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;forward\u0026#34;: true}} ] \r\rTheme and Color Scheme I like using the material theme. In particular, I use the \u0026ldquo;Materialize\u0026rdquo; theme. You can use this by installing the following packages:\n Materialize Materialize-Appbar Materialize-White-Panels  With these installation, you will also get a lot of color schemes. I prefer to use the Material Oceanic Next color scheme. All other settings for this theme can be seen in my settings below.\nUser Settings / Preferences Here is my complete set of settings for Sublime Text. Please feel free to leave comments below for any questions or suggestions.\n{ \u0026#34;always_show_minimap_viewport\u0026#34;: true, \u0026#34;auto_complete\u0026#34;: true, \u0026#34;bold_folder_labels\u0026#34;: true, \u0026#34;caret_extra_width\u0026#34;: 1.5, \u0026#34;color_scheme\u0026#34;: \u0026#34;Material Oceanic Next (SL).tmTheme\u0026#34;, \u0026#34;default_line_ending\u0026#34;: \u0026#34;unix\u0026#34;, \u0026#34;drag_text\u0026#34;: false, \u0026#34;draw_white_space\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;enable_tab_scrolling\u0026#34;: false, \u0026#34;font_face\u0026#34;: \u0026#34;Hack\u0026#34;, \u0026#34;font_options\u0026#34;: [ \u0026#34;directwrite\u0026#34;, \u0026#34;gray_antialias\u0026#34;, \u0026#34;subpixel_antialias\u0026#34; ], \u0026#34;font_size\u0026#34;: 13, \u0026#34;ignored_packages\u0026#34;: [ \u0026#34;C\u0026#43;\u0026#43;\u0026#34;, \u0026#34;Python\u0026#34;, \u0026#34;Vintage\u0026#34; ], \u0026#34;indent_guide_options\u0026#34;: [ \u0026#34;draw_normal\u0026#34;, \u0026#34;draw_active\u0026#34; ], \u0026#34;line_padding_bottom\u0026#34;: 1, \u0026#34;line_padding_top\u0026#34;: 1, \u0026#34;material_theme_bold_tab\u0026#34;: true, \u0026#34;material_theme_compact_panel\u0026#34;: true, \u0026#34;material_theme_compact_sidebar\u0026#34;: false, \u0026#34;material_theme_contrast_mode\u0026#34;: true, \u0026#34;material_theme_disable_fileicons\u0026#34;: false, \u0026#34;material_theme_disable_folder_animation\u0026#34;: true, \u0026#34;material_theme_disable_tree_indicator\u0026#34;: true, \u0026#34;material_theme_panel_separator\u0026#34;: true, \u0026#34;material_theme_small_statusbar\u0026#34;: true, \u0026#34;material_theme_small_tab\u0026#34;: true, \u0026#34;material_theme_tabs_autowidth\u0026#34;: true, \u0026#34;material_theme_tabs_separator\u0026#34;: true, \u0026#34;material_theme_tree_headings\u0026#34;: true, \u0026#34;overlay_scroll_bars\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;rulers\u0026#34;: [ 80 ], \u0026#34;scroll_past_end\u0026#34;: true, \u0026#34;soda_classic_tabs\u0026#34;: true, \u0026#34;soda_folder_icons\u0026#34;: true, \u0026#34;tab_completion\u0026#34;: false, \u0026#34;tab_size\u0026#34;: 4, \u0026#34;theme\u0026#34;: \u0026#34;Material Oceanic Next.sublime-theme\u0026#34;, \u0026#34;translate_tabs_to_spaces\u0026#34;: true, \u0026#34;trim_trailing_white_space_on_save\u0026#34;: true, \u0026#34;word_wrap\u0026#34;: true, \u0026#34;hot_exit\u0026#34;: false, \u0026#34;remember_open_files\u0026#34;: false } \r","title":"Sublime Text Setup","url":"https://sadanand-singh.github.io/posts/sublimetext/"},{"tags":"Algorithms, Machine Learning","text":"In this post we will explore a class of machine learning methods called Support Vector Machines also known commonly as SVM.\n\n Introduction SVM is a supervised machine learning algorithm which can be used for both classification and regression.\n In the simplest classification problem, given some data points each belonging to one of the two classes, the goal is to decide which class a new data point will be in. A simple linear solution to this problem can be viewed in a framework where a data point is viewed as a $p$-dimensional vector, and we want to know whether we can separate such points with a ($p-1$)-dimensional hyperplane.\nThere are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability.\nThe figure on the right is a binary classification problem (points labeled $y_i = \\pm 1$) that is linearly separable in space defined by the vector x. Green and purple line separate two classes with a small margin, whereas yellow line separates them with the maximum margin.\n Mathematically, for the linearly separable case, any point x lying on the separating hyperplane satisfies: \\(\\mathbf{x}^T\\mathbf{w} \u0026#43; b = 0\\)   , where $\\mathbf{w}$ is the vector normal to the hyperplane, and $b$ is a constant that describes how much plane is shifted relative to the origin. The distance of the hyperplane from the origin is \\(\\frac{b}{\\lVert \\mathbf{w} \\rVert}\\)   .\nNow draw parallel planes on either side of the decision boundary, so we have what looks like a channel, with the decision boundary as the central line, and the additional planes as gutters. The margin, i.e. the width of the channel, is \\((d_\u0026#43; \u0026#43; d_-)\\)    and is restricted by the data points closest to the boundary, which lie on the gutters. The two bounding hyperplanes of the channel can be represented by a constant shift in the decision boundary. In other words, these planes ensure that all the points are at least a signed distance d away from the decision boundary. The channel region can be also represented by the following equations:\n$$\\begin{aligned} \u0026amp; \\mathbf{x}_i^T\\mathbf{w} \u0026#43; b \\ge \u0026#43;a, \\text{for } y_i = \u0026#43;1 \\\\ \u0026amp; \\mathbf{x}_i^T\\mathbf{w} \u0026#43; b \\le -a, \\text{for } y_i = -1 \\end{aligned}$$     These conditions can be put more succinctly as:\n$$y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge a, \\forall i$$     Using the formulation of distance from origin of three hyper planes, we can show that, the margin, M is equivalent to $d_+ + d_- = 2a / \\lVert \\mathbf{w} \\rVert$. Without any loss of generality, we can set \\(a = 1\\)   , since it only sets the scale (units) of \\(b\\)    and $\\mathbf{w}$. So to maximize the margin, we have to maximize \\(1 / \\lVert \\mathbf{w} \\rVert\\)   . Such a non-convex objective function can be avoided if we choose in stead to minimize \\({\\lVert \\mathbf{w} \\rVert}^2\\)   .\nIn summary, for a problem with m numbers of training data points, we need to solve the following quadratic programming problem:\n$$\\begin{aligned} \u0026amp; {\\text{maximize }} M \\\\\u0026amp; \\text{subject to } y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge M, \\forall \\text{ } i = 1 \\ldots m \\end{aligned}$$     This can be more conveniently put as:\n$$\\begin{aligned} \u0026amp; {\\text{minimize }} f(w) \\equiv \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \\\\ \u0026amp; \\text{subject to } g(\\mathbf{w}, b) \\equiv -y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \u0026#43; 1 \\le 0, i = 1 \\ldots m \\end{aligned}$$     The maximal margin classifier is a very natural way to perform classification, if a separating hyper plane exists. However, in most real-life cases no separating hyper plane exists, and so there is no maximal margin classifier.\nSupport Vector Classifier  We can extend the concept of a separating hyper plane in order to develop a hyper plane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier.\nAssuming the classes overlap in the given feature space. One way to deal with the overlap is to still maximize M, but allow for some points to be on the wrong side of the margin. In order to allow these, we can define the slack variables as, $\\xi = ( \\xi_1, \\xi_2 \\ldots \\xi_m)$. Now, keeping the above optimization problem as a convex problem, we can modify the constraints as:\n$$\\begin{aligned} \u0026amp; y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge M(1-\\xi_i), \\forall \\text{ } i = 1 \\ldots m, \\\\ \u0026amp; \\xi_i \\ge 0 \\text{ and } \\sum_{i=1}^{m}\\xi_i \\le C \\text{ }\\forall \\text{ } i = 1 \\ldots m, \\end{aligned}$$     We can think of this formulation in the following context. The value $\\xi_i$ in the constraint $y_i (\\mathbf{x}_i^T\\mathbf{w} + b) \\ge M(1-\\xi_i)$ is the proportional amount by which the prediction $f(x_i)=x_i^T\\mathbf{w} + b$ is on the wrong side of its margin. Hence by bounding the sum $\\sum \\xi_i$, we can bound the total proportional amount by which predictions fall on the wrong side of their margin. Mis-classifications occur when $\\xi_i \u0026gt; 1$, so bounding $\\sum \\xi_i$ at a value K say, bounds the total number of training mis-classifications at K.\nSimilar to the case of maximum margin classifier, we can rewrite the optimization problem more conveniently as,\n$$\\begin{aligned} \u0026amp; {\\text{minimize }} \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \u0026#43; C \\sum_{i}^{m} \\xi_i\\\\ \u0026amp; \\text{subject to } y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge 1 - \\xi_i, \\text{ } \\text{ and } \\xi_i \\ge 0, \\text{ } i = 1 \\ldots m \\end{aligned}$$     Now, the question before us is to find a way to solve this optimization problem efficiently.\nThe problem above is quadratic with linear constraints, hence is a convex optimization problem. We can describe a quadratic programming solution using Lagrange multipliers and then solving using the Wolfe dual problem.\nThe Lagrange (primal) function for this problem is:\n$$L_P = \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \u0026#43; C \\sum_{i}^{m} \\xi_i - \\sum_{i=1}^{m} \\alpha_i[y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i)] - \\sum_{i=1}^{m} \\mu_i \\xi_i,$$     which we can minimize w.r.t. $\\mathbf{w}$, b, and $\\xi_i$. Setting the respective derivatives to zero, we get,\n$$\\begin{aligned} \u0026amp; \\mathbf{w} = \\sum_{i=1}^{m} \\alpha_i y_i \\mathbf{x_i} \\\\ \u0026amp; 0 = \\sum_{i=1}^{m} \\alpha_i y_i \\\\ \u0026amp; \\alpha_i = C - \\mu_i, \\forall i, \\end{aligned}$$     as well as the positivity constraints, $\\alpha_i$, $\\mu_i$, $\\xi_i \\ge 0, \\text{ } \\forall i$. By substituting these conditions back into the Lagrange primal function, we get the Wolfe dual of the problem as,\n$$L_D = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j x_i^T x_j$$     which gives a lower bound on the original objective function of the quadratic programming problem for any feasible point. We maximize \\(L_D\\)    subject to \\(0 \\le \\alpha_i \\le C\\)    and \\(\\sum_{i=1}^{m} \\alpha_i y_i = 0\\)   . In addition to above constraints, the Karush-Kuhn-Tucker (KKT) conditions include the following constraints,\n$$\\begin{aligned} \u0026amp; \\alpha_i[y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i)] = 0, \\\\ \u0026amp; \\mu_i \\xi_i = 0, \\\\ \u0026amp; y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i) \\ge 0, \\end{aligned}$$     for $i = 1 \\ldots m$. Together these equations uniquely characterize the solution to the primal and the dual problem.\nLet us look at some special properties of the solution. We can see that the solution for $\\mathbf{w}$ has the for\n$$\\mathbf{\\hat{w}} = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x_i}$$     with nonzero coefficients $\\hat{\\alpha}_i$ only for those i for which $y_i (\\mathbf{x}_i^T\\mathbf{w} + b) - (1 - \\xi_i) = 0$. These i observations are called \u0026ldquo;support vectors\u0026rdquo; since $\\mathbf{w}$ is represented in terms of them alone. Among these support points, some will lie on the edge of the margin $(\\hat{\\xi}_i = 0)$, and hence characterized by $0 \u0026lt; \\hat{\\alpha}_i \u0026lt; C$; the remainder $(\\hat{\\xi}_i \u0026gt; 0)$ have $\\hat{\\alpha}_i = C$. Any of these margin points can be used to solve for b. Typically, once can use an average value from all of the solutions from the support points.\nIn this formulation, C is model hyper parameter and can be used as a regularizer to control the capacity and generalization error of the model.\nThe Kernel Trick The support vector classifier described so far finds linear boundaries in the input feature space. As with other linear methods, we can make the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines. Generally linear boundaries in the enlarged space achieve better training-class separation, and translate to nonlinear boundaries in the original space. Once the basis functions $h_i(x), i=1 \\ldots m$ are selected, the procedure remains same as before.\nNow recall that in calculating the actual classifier, we needed only support vector points, i.e. we need smaller amount of computation if data has better training-class separation. Furthermore, if one looks closely, we can find an additional trick. The separating plane can be given by the function:\n$$\\begin{aligned} f(x) \u0026amp; = \\mathbf{x}^T \\mathbf{w} \u0026#43; b \\\\ \u0026amp; = \\mathbf{x}^T \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x_i} \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x}^T \\mathbf{x}_i \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\langle\\mathbf{x} \\mathbf{x}_i\\rangle \u0026#43; b \\end{aligned}$$     where, $\\langle \\mathbf{x} \\mathbf{y} \\rangle$ denotes inner product of vectors $\\mathbf{x}$ and $\\mathbf{y}$. This shows us that we can rewrite training phase operations completely in terms of inner products!\nIf we were to replace linear terms with a predefined non-linear operation $h(x)$, the above formulation of the separating plane will simply modify into:\n$$\\begin{aligned} f(x) \u0026amp; = h(\\mathbf{x})^T \\mathbf{w} \u0026#43; b \\\\ \u0026amp; = h(\\mathbf{x})^T \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i h(\\mathbf{x}_i) \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i h(\\mathbf{x})^T h(\\mathbf{x}_i) \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\langle h(\\mathbf{x}) h(\\mathbf{x}_i) \\rangle \u0026#43; b \\end{aligned}$$     As before, given $\\hat{\\alpha_i}$, b can be determined by solving $y_i f(\\mathbf{x}_i) = 1$ for any (or all) $x_i$ for which $0 \u0026lt; \\hat{\\alpha}_i \u0026lt; C$. More importantly, this tells us that we do not need to specify the exact nonlinear transformation $h(x)$ at all, rather only the knowledge of the Kernel function \\(K(x, x\u0026#39;) = \\langle h(x)h(x\u0026#39;) \\rangle\\)    that computes inner products in the transformed space is enough. Note that for the dual problem to be convex quadratic programming problem, $K$ would need to be symmetric positive semi-definite.\nSome common choices of kernels are:\n $d^{th}$ degree polynomial: \\(K(x, x\u0026#39;) = (1\u0026#43;\\langle x x\u0026#39; \\rangle )^d\\)   \n Radial basis: \\(K(x, x\u0026#39;) = \\exp (-\\gamma \\lVert \\mathbf{x - x\u0026#39;} \\rVert^2 )\\)   \n Neural network: \\(K(x, x\u0026#39;) = \\tanh (\\kappa_1 \\langle x x\u0026#39; \\rangle \u0026#43; \\kappa_2)\\)   \nThe role of the hyper-parameter $C$ is clearer in an enlarged feature space, since perfect separation is often achievable there. A large value of $C$ will discourage any positive $\\xi_i$, and lead to an over-fit wiggly boundary in the original feature space; a small value of $C$ will encourage a small value of $\\lVert w \\rVert$, which in turn causes $f(x)$ and hence the boundary to be smoother, potentially at the cost of more points as support vectors.\nCurse of Dimensionality\u0026hellip;. huh!!! With m training examples, $p$ predictors and M support vectors, the SVM requires $M^3 + Mm + mpM$ operations. This suggests the choice of the kernel and hence number of support vectors M will play a big role in feasibility of this method. For a really good choice of kernel that leads to very high training-class separation, i.e. $M \u0026lt;\u0026lt;\u0026lt; m$, the method can be viewed as linear in m. However, for a bad choice case, $M \\approx m$ we will be looking at an $O (m^3)$ algorithm.\nThe modern incarnation of deep learning was designed to overcome these limitations (large order of computations and clever problem-specific choice of kernels) of kernel machines. We will look at the details of a generic deep learning algorithm in a future post.\n","title":"Support Vector Machines","url":"https://sadanand-singh.github.io/posts/svmmodels/"},{"tags":"Programming, Python","text":"In the Week 1 we got started with Python. Now that we can interact with python, lets dig deeper into it.\nThis week we will go over some additional fundamental things common in any program - interactive input from users, adding comments to your code, use of conditional logic i.e. if - else conditions, loops, formatted output with strings and print() statements.\n\nPython Week 2 User Inputs There are hardly any programs without any input. Input can come in various ways, for example from a database, another computer, mouse clicks and movements or from the internet. Yet, in most cases the input stems from the keyboard. For this purpose, Python provides the function input(). input() has an optional parameter, which is the prompt string, i.e. the text that will be shown when asking for input.\nIn\u0026nbsp;[1]: name = input(\u0026quot;What\u0026#39;s your name? \u0026quot;) print(\u0026quot;Nice to meet you \u0026quot; + name + \u0026quot;!\u0026quot;) age = input(\u0026quot;Your age? \u0026quot;) print(\u0026quot;So, you are already \u0026quot; + age + \u0026quot; years old, \u0026quot; + name + \u0026quot;!\u0026quot;)    \n What\u0026#39;s your name? Sadanand Nice to meet you Sadanand! Your age? 30 So, you are already 30 years old, Sadanand!     \n  What if you try to do some mathematical operation on the age? You will get a TypeError as follows:\n   In\u0026nbsp;[2]: age = 12 + age \n  \n  --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-2-3d9ce720d6f3\u0026gt; in \u0026lt;module\u0026gt;() ----\u0026gt; 1age = 12 + age TypeError: unsupported operand type(s) for +: \u0026#39;int\u0026#39; and \u0026#39;str\u0026#39;    \n  This says that by default all data is read as raw input i.e. strings. If we want numbers we need to convert them ourselves. For example:\n   In\u0026nbsp;[3]:  cities_canada = input(\u0026quot;Largest cities in Canada: \u0026quot;) \n  \n Largest cities in Canada: [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;]     \n In\u0026nbsp;[4]:  print(cities_canada, type(cities_canada)) \n  \n [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;] \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[5]: cities_canada = eval(input(\u0026quot;Largest cities in Canada: \u0026quot;)) \n  \n Largest cities in Canada: [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;]     \n In\u0026nbsp;[6]:  print(cities_canada, type(cities_canada)) \n  \n [\u0026#39;Montreal\u0026#39;, \u0026#39;Ottawa\u0026#39;, \u0026#39;Calgary\u0026#39;, \u0026#39;Toronto\u0026#39;] \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;     \n In\u0026nbsp;[7]: population = input(\u0026quot;Population of Portland? \u0026quot;) \n  \n Population of Portland? 604596     \n In\u0026nbsp;[8]:  print(population, type(population)) \n  \n 604596 \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[9]: population = int(input(\u0026quot;Population of Portland? \u0026quot;)) \n  \n Population of Portland? 604596     \n In\u0026nbsp;[10]:  print(population, type(population)) \n  \n 604596 \u0026lt;class \u0026#39;int\u0026#39;\u0026gt;     \n In\u0026nbsp;[13]: pi = input(\u0026quot;Value of PI is?\u0026quot;) \n  \n Value of PI is?3.14     \n In\u0026nbsp;[14]:  print(pi, type(pi)) \n  \n 3.14 \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[15]: pi = float(input(\u0026quot;Value of PI is?\u0026quot;)) \n  \n Value of PI is?3.14     \n In\u0026nbsp;[16]:  print(pi, type(pi)) \n  \n 3.14 \u0026lt;class \u0026#39;float\u0026#39;\u0026gt;     \n\nNotice the use of various methods like eval(), int() and float() to get user input in correct formats. In summary, eval() is used to get data into various native python formats, e.g. lists, dictionaries etc. We will look at these in more detail in next few tutorials. int() is used to convert input to integer numbers (numbers without decimals), while float() is used to get floating point numbers.\nAlso, of interest above is the type() method used in print statements. You can get the type of any variable in python using this method. In the output of this we see something like: \u0026lt; class \u0026lsquo;float\u0026rsquo;\u0026gt; - if variable is of float type. For the time being we will ignore the \u0026ldquo;class\u0026rdquo; in this.\nIndentation Blocks Python programs get structured through indentation, i.e. code blocks are defined by their indentation (The amount of blank space before any line). This principle makes it easier to read and understand other people\u0026rsquo;s Python code.\nAll statements with the same distance to the right belong to the same block of code, i.e. the statements within a block line up vertically. The block ends at a line less indented or the end of the file. If a block has to be more deeply nested, it is simply indented further to the right.\nIn the following sections below we will see extensive use of such indentation blocks. Consider the following example to calculate Pythagorean triples. You do not need to understand the full code right here. We will revisit this code at the end of this tutorial.\nIn\u0026nbsp;[18]: from math import sqrt n = input(\u0026quot;Maximum Number? \u0026quot;) n = int(n)+1 for a in range(1,n): for b in range(a,n): c_square = a**2 + b**2 c = int(sqrt(c_square)) if ((c_square - c**2) == 0): print(a, b, c)    \n Maximum Number? 10 3 4 5 6 8 10     \n\nIn the above code, we see three indentation blocks, first and second \u0026ldquo;for\u0026rdquo; loops and the third \u0026ldquo;if\u0026rdquo; condition. There is another aspect of structuring in Python, which we haven\u0026rsquo;t mentioned so far, which you can see in the example. Loops and Conditional statements end with a colon \u0026ldquo;:\u0026rdquo; - the same is true for functions and other structures introducing blocks. So, we should have said Python structures by colons and indentation.\nComments in Python Python has two ways to annotate/comment Python code. One is by using comments to indicate what some part of the code does. Single-line comments begin with the hash character (\u0026ldquo;#\u0026rdquo;) and are terminated by the end of line. Here is an example:\nIn\u0026nbsp;[19]: # This is a comment in Python before print statement print(\u0026quot;Hello World\u0026quot;) #This is also a comment in Python    \n Hello World     \n\nConditionals Conditionals, - mostly in the form of if statements - are one of the essential features of a programming language. A decision has to be taken when the script or program comes to a point where it has a choice of actions, i.e. different computations, to choose from.\nThe decision depends in most cases on the value of variables or arithmetic expressions. These expressions are evaluated to the Boolean values True or False. The statements for the decision taking are called conditional statements. Alternatively they are also known as conditional expressions or conditional constructs.\nConditional statements in Python use indentation blocks to conditionally execute certain code. The general form of the if statement in Python looks like this:\nIn\u0026nbsp;[12]: if condition_1: statement_block_1 elif condition_2: statement_block_2 ... elif another_condition: another_statement_block else: else_block    \n     \n\nIf the condition \u0026ldquo;condition_1\u0026rdquo; is True, the statements of the block statement_block_1 will be executed. If not, condition_2 will be evaluated. If condition_2 evaluates to True, statement_block_2 will be executed, if condition_2 is False, the other conditions of the following elif conditions will be checked, and finally if none of them has been evaluated to True, the indented block below the else keyword will be executed.\nTypical examples of \u0026ldquo;condition\u0026rdquo; statements follow some of following operations: mathematical comparisons like, \u0026ldquo;\u0026lt;\u0026rdquo;, \u0026ldquo;\u0026gt;\u0026rdquo;, \u0026ldquo;\u0026lt;=\u0026ldquo;, \u0026ldquo;\u0026gt;=\u0026ldquo;, \u0026ldquo;==\u0026rdquo; object comparisons like \u0026ldquo;is\u0026rdquo; i.e. this is exactly something or not. boolean logic operators like \u0026ldquo;not\u0026rdquo;, \u0026ldquo;or\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;xor\u0026rdquo; etc.\nThe following objects are evaluated by Python as False:\n numerical zero values (0, 0L, 0.0, 0.0+0.0j), the Boolean value False, empty strings, empty lists and empty tuples, empty dictionaries. the special value None.  All other values are considered to be True. Let us try to solve this simple DNA sequence problem: Given the an input DNA sequence, print the sequence if its length is less than equal to 20. Print \u0026ldquo;Error\u0026rdquo; if the sequence is empty or its length is larger than 25. If length is between 21 and 25, print the last 5 bases only.\n\nIn\u0026nbsp;[20]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNNAATTCCGG\u0026quot; if len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;)    \n ERROR!     \n In\u0026nbsp;[21]: dna = \u0026quot;ATGCAATGCN\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n ATGCAATGCN     \n In\u0026nbsp;[22]: dna = \u0026quot;\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n ERROR!     \n In\u0026nbsp;[23]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n CCNNN     \n\nif else conditions can also be combined in a regular assignment expression to assign values. For example, In the DNA case, we want to store length of DNA. However, we want length to number only if length of sequence is between 1 and 25. In all other cases, we want to store the length of sequence as -1. A typical way to do this would be:\nIn\u0026nbsp;[24]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; length = -1 if 0 \u0026lt; len(dna) \u0026lt;= 20: length = len(dna) print(length)    \n -1     \n In\u0026nbsp;[25]: dna = \u0026quot;CCGGGAACCTCACG\u0026quot; length = -1 if 0 \u0026lt; len(dna) \u0026lt;= 20: length = len(dna)\nprint(length) \n  \n 14     \n  This example can be written in a much shorter fashion as well. Such conditions are commonly called as ternary if statements.\n   In\u0026nbsp;[26]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; length = len(dna) if 0 \u0026lt; len(dna) \u0026lt;= 20 else -1 print(length) \n  \n -1     \n In\u0026nbsp;[27]: dna = \u0026quot;CCGGGAACCTCACG\u0026quot; length = len(dna) if 0 \u0026lt; len(dna) \u0026lt;= 20 else -1 print(length) \n  \n 14     \n\nLoops Many algorithms make it necessary for a programming language to have a construct which makes it possible to carry out a sequence of statements repeatedly. The code within the loop, i.e. the code carried out repeatedly, is called the body of the loop.\nThere are two types of loops in Python -\n while Loop for Loop  The while Loop\nThese are a type of loop called \u0026ldquo;Condition-controlled loop\u0026rdquo;. As suggested by the name, the loop will be repeated until a given condition changes, i.e. changes from True to False or from False to True, depending on the kind of loop.\nLet us consider the following example of DNA sequence: We want to print every base of a given sequence, until we have found 2 A\u0026rsquo;s.\nIn\u0026nbsp;[28]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; countA = 0 index = 0 while countA \u0026lt; 2: print(dna[index]) if dna[index] == \u0026#39;A\u0026#39;: countA = countA + 1 index = index + 1    \n A T G C C G A     \n\nIn the above example, the loop (code under the while block) was executed until countA \u0026lt; 2 statement remained true.\nThe loops can be made to exit before its actual completion using the break statements. Consider the following example of DNA sequence. We want to print every base of a given sequence, until we have found 2 A\u0026rsquo;s. However, we want to stop printing as soon as we have found an N base.\nIn\u0026nbsp;[29]: dna = \u0026quot;ATGCNCGATTTATCGGGAACCNNN\u0026quot; countA = 0 index = 0 while countA \u0026lt; 2: if dna[index] == \u0026#39;N\u0026#39;: break if dna[index] == \u0026#39;A\u0026#39;: countA = countA + 1 print(dna[index]) index = index + 1    \n A T G C     \n\nNow, let us consider another case while looping over something. We want to skip over a part of code at certain condition. In such cases, continue statement comes handy.\nConsider the following example wrt to DNA sequencing. Given a sequence of dna, we do NOT want to print the base name if it is \u0026lsquo;N\u0026rsquo;\nIn\u0026nbsp;[30]: dna = \u0026quot;ATGCNCN\u0026quot; index = 0 while index \u0026lt; len(dna): index = index + 1 if dna[index-1] == \u0026#39;N\u0026#39;: continue print(dna[index-1])    \n A T G C C     \n\nThe for Loop\nA for loop is similar to while loop, except it is used to loop over certain elements, unlike while loop that continues until certain condition is satisfied. In the case DNA sequences, say, one case of for loop would be to loop over all bases in a sequence.\nConsider the following example: Given a DNA sequence, we want to count the number of all \u0026lsquo;A\u0026rsquo;, and \u0026rsquo;T bases.\nIn\u0026nbsp;[31]: dna = \u0026quot;ATGCNCGATTTATCGGGAACCNNN\u0026quot; count = 0 for base in dna: if base == \u0026#39;A\u0026#39; or base == \u0026#39;T\u0026#39;: count += 1 print(\u0026quot;Number of A, T bases is:\u0026quot;, count)    \n Number of A, T bases is: 10     \n\nSimilar to while loops, we can use break and continue statements with for loops as well.\nLet us look at somewhat complicated use of for loop:\nGiven a DNA sequence, we want to count the number of doublets of bases, i.e. no. of times certain bases come twice exactly. If some base occur more than twice, we do not want to count that.\nIn\u0026nbsp;[32]: dna = \u0026quot;ATGGCNCGAATTTAAATCGGGAACCNNN\u0026quot; countPairs = 0 pairFound = 0 prevBase = \u0026#39;\u0026#39; for base in dna: if (base == prevBase): pairFound += 1 else: if pairFound == 1: countPairs += 1 pairFound = 0 prevBase = base print(\u0026quot;Number of paired bases is:\u0026quot;, countPairs)    \n Number of paired bases is: 4     \n\nFormatting of Output Final topic for this week is the formatting of text in the print statements. Consider the following case:\nWe have following variables: name = \u0026quot;Sadanand\u0026quot;, age = 30, and gender = \u0026quot;male\u0026quot;\nWe would like to print a quite cumbersome statement like as follows. This can be quite easily done using the format method.\nIn\u0026nbsp;[33]: name = \u0026quot;Sadanand\u0026quot; age = 30 gender = \u0026quot;male\u0026quot; msg = \u0026quot;Hi {0}, You are a {1}, and you have seen {2}winters as you are {2}years old! Thanks {0}!\u0026quot; print(msg.format(name, gender, age))    \n Hi Sadanand, You are a male, and you have seen 30 winters as you are 30 years old! Thanks Sadanand!     \n\nThus format method provides us with easy way to mix different types of variables in the strings.\nThats it for this week. Next we will look at strings and lists in Python in more detail.\nExercise\nGiven the following sequence of dna - \u0026ldquo;ATGGCNCGAATTTAAATCGGGAACCNNN\u0026rdquo;,\n Write a program to count number of all triplets in it. Write a program that prints all non \u0026rsquo;T\u0026rsquo; bases that come after \u0026rsquo;T\u0026rsquo;, but stops when two or more continuous \u0026rsquo;T\u0026rsquo; has been found. Write a program to generate new sequence with every 3rd base from the above sequence. Write a program to calculate sum of all numbers from 1 to 10. HINT: Please take a look at the range method. ","title":"Python Tutorial - Week 2","url":"https://sadanand-singh.github.io/posts/pythontutorialweek2/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In the last post we looked at some initial cleanup of the data. We will start from there by loading the pickled dataframe.\n\nIn\u0026nbsp;[1]: import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns %matplotlib inline df = pd.read_pickle(\u0026quot;/home/ssingh/LendingClubData/Part1.pickle\u0026quot;)    \n  Lets first check what all columns are remaining in our dataframe. As there are still more than 100 variables left, we will initially focus on the first 25 ones only.\n   In\u0026nbsp;[2]: print(df.columns) print(df.columns.shape) \n  \n Index([\u0026#39;loan_amnt\u0026#39;, \u0026#39;funded_amnt\u0026#39;, \u0026#39;funded_amnt_inv\u0026#39;, \u0026#39;term\u0026#39;, \u0026#39;int_rate\u0026#39;, \u0026#39;installment\u0026#39;, \u0026#39;grade\u0026#39;, \u0026#39;sub_grade\u0026#39;, \u0026#39;emp_length\u0026#39;, \u0026#39;home_ownership\u0026#39;, ... \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;, \u0026#39;pct_tl_nvr_dlq\u0026#39;, \u0026#39;percent_bc_gt_75\u0026#39;, \u0026#39;pub_rec_bankruptcies\u0026#39;, \u0026#39;tax_liens\u0026#39;, \u0026#39;tot_hi_cred_lim\u0026#39;, \u0026#39;total_bal_ex_mort\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;total_il_high_credit_limit\u0026#39;], dtype=\u0026#39;object\u0026#39;, length=111) (111,)     \n  From the data dictionary, we can see that funded_amnt is total amount committed till now, and funded_amnt_inv is the amount funded by investors. It is difficult to think of a direct correlation between the charged interest rate and the actual funded amount. However, this amount can give us a range of risk that one will be taking when investing. Given the two committed amounts are very similar, we will drop the the \u0026ldquo;funded_amnt\u0026rdquo; column. The installment column gives us feel of how much burden the loan will be on the borrower. However, this will be direct function of term and rate of the loan and hence should be dropped from any further analysis. the \u0026ldquo;grade\u0026rdquo; and \u0026ldquo;sub_grade\u0026rdquo; are LC assigned grades to the loan. We can keep these as secondary variables to check the liability of models used by LC.\n   In\u0026nbsp;[3]: df.drop([\u0026#39;funded_amnt\u0026#39;, \u0026#39;installment\u0026#39;, \u0026quot;pymnt_plan\u0026quot;],1, inplace=True) \n  \n In\u0026nbsp;[4]: df.ix[:4,11:19] \n  \nOut[4]:    url desc purpose title zip_code addr_state dti delinq_2yrs     0 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 235xx VA 12.03 0   1 https://www.lendingclub.com/browse/loanDetail.... NaN credit_card Credit card refinancing 937xx CA 14.92 0   2 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 850xx AZ 34.81 0   3 https://www.lendingclub.com/browse/loanDetail.... NaN car Car financing 953xx CA 8.31 1   4 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 077xx NJ 25.81 0      \n \n  For our purpose, we will not be going into any kind of natural language processing, hence, the description and the url variables are of no use to us.\n   In\u0026nbsp;[5]: df.drop([\u0026#39;url\u0026#39;, \u0026#39;desc\u0026#39;],1, inplace=True) \n  \n  Let us check what are typical \u0026ldquo;purpose\u0026rdquo; used for requesting loans. We can view this as a histogram plot.\n   In\u0026nbsp;[6]: sns.set() sns.set_context(\u0026quot;notebook\u0026quot;, font_scale=1.5, rc={\u0026quot;lines.linewidth\u0026quot;: 2.5}) total = float(len(df.index)) ax = sns.countplot(x=\u0026quot;purpose\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); ax.set(yscale = \u0026quot;log\u0026quot;) plt.xticks(rotation=90) plt.show() \n  \n  \n \n  We can also look for any kind of correlation between the purpose and the interest rate of loan using a box plot. We can clearly see this could be useful for building our model!\n   In\u0026nbsp;[7]: sns.boxplot(x=\u0026quot;purpose\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) plt.xticks(rotation=90) plt.show() \n  \n  \n \n  Let also look for any kind of correlations between \u0026ldquo;employment length\u0026rdquo;, \u0026ldquo;rate\u0026rdquo; and \u0026ldquo;status\u0026rdquo; of loans. Status here, if you remember from the previous post refers to the risk factor involved with the loan.\n   In\u0026nbsp;[8]: sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;emp_length\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[8]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e52116278\u0026gt;  \n  \n \n  As expected, we find good loans to have larger employment length. Interestingly, interest rate tends to be all over the place for high risk loans. But, if you think about it, that is what we are trying to fix here!\nAnalyzing tile of loans could be tricky. Again, due to lack of any kind of natural language processing, let us drop this as well.\nThe location address of borrowers can say interesting pattern about the interest rates. First three letters of zip code can give much more information than states. However, if the zip info is missing, state can provide a reasonable approx. of the data. Lets check if we have any data where zip data is missing. If none, we can simply drop the state information.\n   In\u0026nbsp;[9]: df[\u0026#39;zip_code\u0026#39;] = df[\u0026#39;zip_code\u0026#39;].str.replace(\u0026#39;xx\u0026#39;,\u0026#39;\u0026#39;) \n  \n In\u0026nbsp;[10]: df.drop([\u0026#39;title\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[11]: df.zip_code.isnull().sum() \n  \nOut[11]: 0  \n \n In\u0026nbsp;[12]: df.drop([\u0026#39;addr_state\u0026#39;],1, inplace=True) \n  \n  The \u0026ldquo;dti\u0026rdquo; column in the data dictionary has been described as - \u0026ldquo;A ratio calculated using the borrower\u0026rsquo;s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower\u0026rsquo;s self-reported monthly income\u0026rdquo;. Based on this information, Debt_to_Income ratio is a direct measure of the loan risk.\nLets check effects of delinquency over last 2 years on interest rate using a box plot:\n   In\u0026nbsp;[13]: sns.boxplot(x=\u0026quot;delinq_2yrs\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) \n  \nOut[13]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e502c3a58\u0026gt;  \n  \n \n  We can see visualize effects of delinquency over last 2 years. Let us bin this data into three bins - Low, Medium and High. We will now move on to the next set of columns.\n   In\u0026nbsp;[14]: df[\u0026quot;delinq_2yrs\u0026quot;] = pd.cut(df.delinq_2yrs, bins=3, labels=[\u0026quot;Low\u0026quot;, \u0026quot;Medium\u0026quot;, \u0026quot;High\u0026quot;], include_lowest = True) \n  \n In\u0026nbsp;[15]: df.ix[:4,15:23] \n  \nOut[15]:    earliest_cr_line fico_range_low fico_range_high inq_last_6mths mths_since_last_delinq mths_since_last_record open_acc pub_rec     0 Aug-1994 750 754 0 NaN NaN 6 0   1 Sep-1989 710 714 2 42.0 NaN 17 0   2 Aug-2002 685 689 1 NaN NaN 11 0   3 Oct-2000 665 669 0 17.0 NaN 8 0   4 Nov-1992 680 684 0 NaN NaN 12 0      \n \n  Earliest credit line should play an important role in determining the rate. We will replace this column by something more quantitative - credit_age.\n   In\u0026nbsp;[16]: now = pd.Timestamp(\u0026#39;20160501\u0026#39;) df[\u0026quot;credit_age\u0026quot;] = pd.to_datetime(df.earliest_cr_line, format=\u0026quot;%b-%Y\u0026quot;) df[\u0026#39;credit_age\u0026#39;] = (now - df[\u0026#39;credit_age\u0026#39;]).dt.days.divide(30).astype(\u0026quot;int64\u0026quot;) df.drop([\u0026#39;earliest_cr_line\u0026#39;],1, inplace=True) \n  \n  Let us try to find a trend between interest rate, fico ranges and loan status.\n   In\u0026nbsp;[17]: sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;fico_range_low\u0026quot;, \u0026quot;fico_range_high\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[17]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e5239cef0\u0026gt;  \n  \n \n  We find 2 FICO scores to be highly collinear. Further, high risk loans have much larger lower values of fico scores. We can safely replace these with the mean values of fico scores.\n   In\u0026nbsp;[18]: df[\u0026#39;fico\u0026#39;] = 0.5*(df[\u0026#39;fico_range_high\u0026#39;] + df[\u0026#39;fico_range_low\u0026#39;]) df.drop([\u0026#39;fico_range_high\u0026#39;],1, inplace=True) df.drop([\u0026#39;fico_range_low\u0026#39;],1, inplace=True) \n  \n  Similar to the 2 year delinquency, let us also look at the 6 month inquiry data. Other data like mths_since_last_delinq and mths_since_last_record can be safely removed, as they will be correlated to 2 year delinquency data.\n   In\u0026nbsp;[19]: sns.boxplot(x=\u0026quot;inq_last_6mths\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) \n  \nOut[19]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e50ff59b0\u0026gt;  \n  \n \n  Let us find correlations between many of these similar variables.\n   In\u0026nbsp;[20]: sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;pub_rec\u0026quot;, \u0026quot;open_acc\u0026quot;, \u0026quot;inq_last_6mths\u0026quot;], hue=\u0026quot;delinq_2yrs\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[20]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e50dea1d0\u0026gt;  \n  \n \n  Both open_acc and inq_last_6_mnths have a strong correlation with delinq_2year, and hence can be safely dropped. pub_rec too has a distinct shape for each levels of delinq_2yrs showing interdependence and hence we can drop this as well.\n   In\u0026nbsp;[21]: df.drop([\u0026#39;pub_rec\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_acc\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_last_6mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_delinq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_record\u0026#39;],1, inplace=True) \n  \n  We will now move on to the next set of columns.\n   In\u0026nbsp;[22]: df.ix[:4,15:25] \n  \nOut[22]:    revol_bal revol_util total_acc initial_list_status out_prncp out_prncp_inv total_pymnt total_pymnt_inv total_rec_prncp total_rec_int     0 138008 29% 17 w 12484.99 12484.99 4364.64 4364.64 2515.01 1849.63   1 6133 31.6% 36 w 6892.58 6892.58 4163.94 4163.94 3507.42 656.52   2 16822 91.9% 20 f 0.00 0.00 2281.98 2281.98 704.38 339.61   3 5753 100.9% 13 w 10868.67 10868.67 4117.57 4117.57 1931.33 2186.24   4 16388 59.4% 44 f 0.00 0.00 9973.43 9973.43 9600.00 373.43      \n \n  Revolving balance and revolving utilization, is a measure of \u0026ldquo;how leveraged your credit cards are\u0026rdquo;. revol_util should provide a relative measure of leverage, whereas revol_bal should provide an absolute measurement. Before we proceed, we need to convert \u0026lsquo;%\u0026rsquo; data to fraction.\n   In\u0026nbsp;[23]: df.revol_util = pd.Series(df.revol_util).str.replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;).astype(float) df.revol_util = df.revol_util * 0.01 \n  \n In\u0026nbsp;[24]: g = sns.pairplot(df, vars=[\u0026quot;revol_bal\u0026quot;, \u0026quot;revol_util\u0026quot;, \u0026quot;total_acc\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n  None of these variables seem to make any direct correlation with the risk levels of the loan. Given their direct use in the FICO score calculation, we will keep these in our analysis.\n    Let us take a look at the initial listing status of the loan. Then, we can find a correlation between these and the risk level.\n   In\u0026nbsp;[25]: sns.countplot(x=\u0026quot;initial_list_status\u0026quot;, hue=\u0026quot;loan_status\u0026quot;, data=df) \n  \nOut[25]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e4fe23898\u0026gt;  \n  \n \n  For high risk loans as well low risk ones, there does not seem to be any significant difference among two types of initial listing of the loan and hence we can drop it.\n   In\u0026nbsp;[26]: df.drop([\u0026#39;initial_list_status\u0026#39;],1, inplace=True) \n  \n  Following variables remaining in the list refer to the current state of the loan and hence will not be playing any effect on the general state or risk level of the loan, therefore should be dropped from our analysis. We will also not consider any joint data for this analysis.\n   In\u0026nbsp;[27]: df.drop([\u0026#39;out_prncp\u0026#39;],1, inplace=True) df.drop([\u0026#39;out_prncp_inv\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_pymnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_pymnt_inv\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_prncp\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_int\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_late_fee\u0026#39;],1, inplace=True) df.drop([\u0026#39;recoveries\u0026#39;],1, inplace=True) df.drop([\u0026#39;collection_recovery_fee\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_pymnt_d\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_pymnt_amnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;next_pymnt_d\u0026#39;],1, inplace=True) df.drop([\u0026#39;policy_code\u0026#39;],1, inplace=True) df.drop([\u0026#39;application_type\u0026#39;],1, inplace=True) df.drop([\u0026#39;annual_inc_joint\u0026#39;],1, inplace=True) df.drop([\u0026#39;dti_joint\u0026#39;],1, inplace=True) df.drop([\u0026#39;verification_status_joint\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[28]: df.ix[:4,18:24] \n  \nOut[28]:    last_credit_pull_d last_fico_range_high last_fico_range_low collections_12_mths_ex_med mths_since_last_major_derog acc_now_delinq     0 Feb-2016 684 680 0 NaN 0   1 Feb-2016 679 675 0 59.0 0   2 Dec-2015 539 535 0 NaN 0   3 Feb-2016 704 700 0 36.0 0   4 Feb-2016 684 680 0 NaN 0      \n \n  First we need to convert, last credit pull day to a numeric value as days since lst credit pull. Let us find if there are any NA values.\n   In\u0026nbsp;[29]: print(\u0026quot;No. of Data with NA values = {}\u0026quot;.format(len(df.last_credit_pull_d) - df.last_credit_pull_d.count())) \n  \n No. of Data with NA values = 27     \n  We will replace these NA values with, Day corresponding with the oldest date of their account, i.e. now - credit history date.\n   In\u0026nbsp;[30]: df.last_credit_pull_d.fillna(\u0026quot;Jan-1980\u0026quot;, inplace=True) \n  \n In\u0026nbsp;[31]: df[\u0026quot;last_credit_pull_d\u0026quot;] = pd.to_datetime(df.last_credit_pull_d, format=\u0026quot;%b-%Y\u0026quot;) df[\u0026#39;last_credit_pull_d\u0026#39;] = (now - df[\u0026#39;last_credit_pull_d\u0026#39;]).dt.days.divide(30).astype(\u0026quot;int64\u0026quot;) df[df[\u0026#39;last_credit_pull_d\u0026#39;] \u0026gt;= 7000].last_credit_pull_d = df[df[\u0026#39;last_credit_pull_d\u0026#39;] \u0026gt;= 7000].credit_age \n  \n  Let us compare last fico score to the overall fico score.\n   In\u0026nbsp;[32]: sns.pairplot(df, vars=[\u0026quot;last_fico_range_high\u0026quot;, \u0026quot;last_fico_range_low\u0026quot;, \u0026quot;fico\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[32]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e5004d828\u0026gt;  \n  \n \n  As before, last fico high and low scores are correlated, and also with overall fico score, and hence we can get rid of these.\n   In\u0026nbsp;[33]: df.drop([\u0026#39;last_fico_range_high\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_fico_range_low\u0026#39;],1, inplace=True) \n  \n  We can also get of \u0026ldquo;collections_12_mths_ex_med\u0026rdquo; column as this corresponds only to the current state of loan. Other two variables, \u0026ldquo;mths_since_last_major_derog\u0026rdquo; and \u0026ldquo;acc_now_delinq\u0026rdquo; should have no additional impact than ones already considered.\n   In\u0026nbsp;[34]: df.drop([\u0026#39;collections_12_mths_ex_med\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_major_derog\u0026#39;],1, inplace=True) df.drop([\u0026#39;acc_now_delinq\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[35]: df.ix[:4,19:29] \n  \nOut[35]:    tot_coll_amt tot_cur_bal open_acc_6m open_il_6m open_il_12m open_il_24m mths_since_rcnt_il total_bal_il il_util open_rv_12m     0 0 149140 NaN NaN NaN NaN NaN NaN NaN NaN   1 0 162110 NaN NaN NaN NaN NaN NaN NaN NaN   2 0 64426 NaN NaN NaN NaN NaN NaN NaN NaN   3 0 261815 NaN NaN NaN NaN NaN NaN NaN NaN   4 0 38566 NaN NaN NaN NaN NaN NaN NaN NaN      \n \n  Again, we can go on and delete all the columns that are related to only the current states of loans, including the ones with large amount of missing data.\n   In\u0026nbsp;[36]: df.drop([\u0026#39;tot_coll_amt\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_acc_6m\u0026#39;],1, inplace=True) df.drop([\u0026#39;tot_cur_bal\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_6m\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_12m\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_24m\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_rcnt_il\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_bal_il\u0026#39;],1, inplace=True) df.drop([\u0026#39;il_util\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_rv_12m\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[37]: df.ix[:4,19:29] \n  \nOut[37]:    open_rv_24m max_bal_bc all_util total_rev_hi_lim inq_fi total_cu_tl inq_last_12m acc_open_past_24mths avg_cur_bal bc_open_to_buy     0 NaN NaN NaN 184500 NaN NaN NaN 5 29828.0 9525.0   1 NaN NaN NaN 19400 NaN NaN NaN 7 9536.0 7599.0   2 NaN NaN NaN 18300 NaN NaN NaN 6 5857.0 332.0   3 NaN NaN NaN 5700 NaN NaN NaN 2 32727.0 0.0   4 NaN NaN NaN 27600 NaN NaN NaN 8 3214.0 6494.0      \n \n  Out of these variables, only \u0026ldquo;avg_cur_bal\u0026rdquo; is viable additional feature for our model. We will also look at the distribution of average current balance. However, in order to use it correctly, we need to if there are any NA values in to and replace them correctly.\n   In\u0026nbsp;[38]: df.drop([\u0026#39;open_rv_24m\u0026#39;],1, inplace=True) df.drop([\u0026#39;max_bal_bc\u0026#39;],1, inplace=True) df.drop([\u0026#39;all_util\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_fi\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_cu_tl\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_last_12m\u0026#39;],1, inplace=True) df.drop([\u0026#39;acc_open_past_24mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;bc_open_to_buy\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rev_hi_lim\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[39]: print(\u0026quot;No. of Data with NA values = {}\u0026quot;.format(len(df.avg_cur_bal) - df.avg_cur_bal.count())) \n  \n No. of Data with NA values = 6     \n In\u0026nbsp;[40]: df.avg_cur_bal.fillna(df.avg_cur_bal.min(), inplace=True) \n  \n In\u0026nbsp;[41]: g = sns.pairplot(df, vars=[\u0026quot;avg_cur_bal\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n In\u0026nbsp;[42]: df.ix[:4,20:28] \n  \nOut[42]:    bc_util chargeoff_within_12_mths delinq_amnt mo_sin_old_il_acct mo_sin_old_rev_tl_op mo_sin_rcnt_rev_tl_op mo_sin_rcnt_tl mort_acc     0 4.7 0 0 103.0 244 1 1 0   1 41.5 0 0 76.0 290 1 1 1   2 93.2 0 0 137.0 148 8 8 0   3 103.2 0 0 16.0 170 21 16 5   4 69.2 0 0 183.0 265 23 3 0      \n \n  Similar to before, we can again get rid of variables that will not make significant impact on our model. Then look at the pair-wise effect of rest of them. We will also replace NAs with the mean values.\n   In\u0026nbsp;[43]: df.drop([\u0026#39;chargeoff_within_12_mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;delinq_amnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_old_il_acct\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_old_rev_tl_op\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_rcnt_rev_tl_op\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_rcnt_tl\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[44]: df.bc_util.fillna(df.bc_util.min(), inplace=True) \n  \n In\u0026nbsp;[45]: sns.pairplot(df, vars=[\u0026quot;bc_util\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[45]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4e1ab550\u0026gt;  \n  \n \n In\u0026nbsp;[46]: df.ix[:4,21:27] \n  \nOut[46]:    mort_acc mths_since_recent_bc mths_since_recent_bc_dlq mths_since_recent_inq mths_since_recent_revol_delinq num_accts_ever_120_pd     0 0 47.0 NaN NaN NaN 0   1 1 5.0 42.0 1.0 42.0 4   2 0 17.0 NaN 3.0 NaN 0   3 5 21.0 17.0 1.0 17.0 1   4 0 24.0 NaN 17.0 NaN 0      \n \n  In this list only variable of our interest is number of mortgage accounts.\n   In\u0026nbsp;[47]: sns.pairplot(df, vars=[\u0026quot;mort_acc\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[47]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4dd0ed30\u0026gt;  \n  \n \n In\u0026nbsp;[48]: df.drop([\u0026#39;mths_since_recent_bc\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_bc_dlq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_inq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_revol_delinq\u0026#39;],1, inplace=True) df.drop([\u0026#39;num_accts_ever_120_pd\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[49]: df.ix[:4,22:31] \n  \nOut[49]:    num_actv_bc_tl num_actv_rev_tl num_bc_sats num_bc_tl num_il_tl num_op_rev_tl num_rev_accts num_rev_tl_bal_gt_0 num_sats     0 1 4 1 2 8 5 9 4 6   1 6 9 7 18 2 14 32 9 17   2 1 4 1 4 12 4 8 4 11   3 3 5 3 5 1 5 7 5 8   4 4 7 5 16 17 8 26 7 12      \n \n  All of these variables are related to some kind of number of accounts. Lets take a look at their inter-dependence.\n   In\u0026nbsp;[50]: sns.pairplot(df, vars=[\u0026quot;num_actv_bc_tl\u0026quot;, \u0026quot;num_actv_rev_tl\u0026quot;, \u0026quot;num_bc_sats\u0026quot;, \u0026quot;num_bc_tl\u0026quot;, \u0026quot;num_op_rev_tl\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[50]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4de95208\u0026gt;  \n  \n \n In\u0026nbsp;[51]: sns.pairplot(df, vars=[\u0026quot;num_op_rev_tl\u0026quot;, \u0026quot;num_il_tl\u0026quot;, \u0026quot;num_rev_accts\u0026quot;, \u0026quot;num_rev_tl_bal_gt_0\u0026quot;, \u0026quot;num_sats\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[51]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4caaedd8\u0026gt;  \n  \n \n  Lets also look at the remaining \u0026ldquo;num\u0026rdquo; of account variables.\n   In\u0026nbsp;[52]: sns.pairplot(df, vars=[\u0026quot;num_sats\u0026quot;, \u0026#39;num_tl_30dpd\u0026#39;, \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[52]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4bf1b2b0\u0026gt;  \n  \n \n  We see that first 9 sets of variables are all quite strongly correlated. The best set to consider for our model could be a sum of a subset of these. Lets consider a new variable, i.e. sum of all types of opened loan accounts, consisting of num_actv_bc_tl, num_actv_rev_tl, num_rev_tl_bal_gt_0, and num_tl_90g_dpd_24m. Additionally, we should also keep no. of accounts open in last year as a variable, i.e. num_tl_op_past_12m.\n   In\u0026nbsp;[53]: df[\u0026#39;num_accounts\u0026#39;] = df[\u0026#39;num_actv_bc_tl\u0026#39;] + df[\u0026#39;num_actv_rev_tl\u0026#39;] + df[\u0026#39;num_rev_tl_bal_gt_0\u0026#39;] + df[\u0026#39;num_tl_90g_dpd_24m\u0026#39;] \n  \n In\u0026nbsp;[54]: dropped_vars=[\u0026quot;num_actv_bc_tl\u0026quot;, \u0026quot;num_actv_rev_tl\u0026quot;, \u0026quot;num_bc_sats\u0026quot;, \u0026quot;num_bc_tl\u0026quot;, \u0026quot;num_op_rev_tl\u0026quot;, \u0026quot;num_il_tl\u0026quot;, \u0026quot;num_rev_accts\u0026quot;, \u0026quot;num_rev_tl_bal_gt_0\u0026quot;, \u0026quot;num_sats\u0026quot;, \u0026#39;num_tl_30dpd\u0026#39;, \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_120dpd_2m\u0026#39;] df.drop(dropped_vars,1, inplace=True) \n  \n In\u0026nbsp;[55]: df.ix[:4,24:32] \n  \nOut[55]:    percent_bc_gt_75 pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort total_bc_limit total_il_high_credit_limit credit_age     0 0.0 0 0 196500 149140 10000 12000 264   1 14.3 0 0 179407 15030 13000 11325 324   2 100.0 0 0 82331 64426 4900 64031 167   3 100.0 0 0 368700 18007 4400 18000 189   4 60.0 0 0 52490 38566 21100 24890 286      \n \n  Out of these 8 remaining features, lets first focus on first two percentages. We will first examining their role of loan status. percent_bc_gt_75 has very high amount of missing data and hence we will remove it from consideration.\n   In\u0026nbsp;[56]: sns.pairplot(df, vars=[\u0026quot;pct_tl_nvr_dlq\u0026quot;, \u0026#39;int_rate\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[56]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4b63e860\u0026gt;  \n  \n \n  We find even pct_tl_nvr_dlq to be not of much help!\n   In\u0026nbsp;[57]: df.drop([\u0026#39;pct_tl_nvr_dlq\u0026#39;, \u0026#39;percent_bc_gt_75\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[58]: sns.pairplot(df, vars=[\u0026quot;pub_rec_bankruptcies\u0026quot;, \u0026#39;tax_liens\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[58]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4b357e10\u0026gt;  \n  \n \n  This clearly shows a strong relationship between loan status and these features and hence need to be an integral part of the model.\nFinally let us take a look at the remaining total credit/balance related features.\n   In\u0026nbsp;[59]: g = sns.pairplot(df, vars=[\u0026quot;tot_hi_cred_lim\u0026quot;, \u0026#39;total_bal_ex_mort\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;total_il_high_credit_limit\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n  We can see a clear effect of outliers on low status of loan on all these variables. However, we can also see a strong correlation between total_il_high_credit_limit, tot_hi_cred_lim and total_bal_ex_mort. Let us choose only tot_hi_cred_lim to be in out feature set.\n   In\u0026nbsp;[60]: df.drop([\u0026#39;total_il_high_credit_limit\u0026#39;, \u0026#39;total_bal_ex_mort\u0026#39;],1, inplace=True) \n  \n  Let us take a final look at all the remaining variables in our data.\n   In\u0026nbsp;[61]: print(df.columns) print(df.columns.shape) \n  \n Index([\u0026#39;loan_amnt\u0026#39;, \u0026#39;funded_amnt_inv\u0026#39;, \u0026#39;term\u0026#39;, \u0026#39;int_rate\u0026#39;, \u0026#39;grade\u0026#39;, \u0026#39;sub_grade\u0026#39;, \u0026#39;emp_length\u0026#39;, \u0026#39;home_ownership\u0026#39;, \u0026#39;annual_inc\u0026#39;, \u0026#39;verification_status\u0026#39;, \u0026#39;loan_status\u0026#39;, \u0026#39;purpose\u0026#39;, \u0026#39;zip_code\u0026#39;, \u0026#39;dti\u0026#39;, \u0026#39;delinq_2yrs\u0026#39;, \u0026#39;revol_bal\u0026#39;, \u0026#39;revol_util\u0026#39;, \u0026#39;total_acc\u0026#39;, \u0026#39;last_credit_pull_d\u0026#39;, \u0026#39;avg_cur_bal\u0026#39;, \u0026#39;bc_util\u0026#39;, \u0026#39;mort_acc\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;, \u0026#39;pub_rec_bankruptcies\u0026#39;, \u0026#39;tax_liens\u0026#39;, \u0026#39;tot_hi_cred_lim\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;credit_age\u0026#39;, \u0026#39;fico\u0026#39;, \u0026#39;num_accounts\u0026#39;], dtype=\u0026#39;object\u0026#39;) (30,)     \n  Let us stop here for this post. We will continue our model creation. We will save our pandas object as pickle and then catch up from there.\n   In\u0026nbsp;[62]: df.to_pickle(\u0026quot;/home/ssingh/LendingClubData/Part2.pickle\u0026quot;) \n  \n\n","title":"EDA of Lending Club Data - II","url":"https://sadanand-singh.github.io/posts/lceda2/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"We will first look at various aspects of the LendingClub data using techniques of Exploratory Data Analysis (EDA). Please look at my past post for finding further details on EDA techniques. Different data files for this analysis have already been downloaded in the current folder. In\u0026nbsp;[4]: import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import numpy as np from matplotlib import pyplot as plt %matplotlib inline    \n  Let\u0026rsquo;s also take a quick look at the data via shell scripts (file size, head, line count, column count).\n   In\u0026nbsp;[2]: !du -h /home/ssingh/LendingClubData/LoanStats3c_securev1.csv #!tail -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv #!head -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv \n  \n 181M /home/ssingh/LendingClubData/LoanStats3c_securev1.csv     \n  Examining the data we see that most of feature names are intuitive. We can get the specifics from the provided data dictionary.\n   In\u0026nbsp;[3]: !wc -l \u0026lt; /home/ssingh/LendingClubData/LoanStats3c_securev1.csv !head -2 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv | sed \u0026#39;s/[^,]//g\u0026#39; | wc -c \n  \n 235633 116     \n  Based on the above analysis, we find that we have a total of 235633-2-2 = 235659 rows and 116 - 1 = 115 columns of data! Let us first look at the detailed description of columns from the dictionary of the data.\n   In\u0026nbsp;[5]: df = pd.read_csv(\u0026quot;/home/ssingh/LendingClubData/LoanStatsDict.csv\u0026quot;, sep=\u0026quot;,\u0026quot;, engine=\u0026#39;c\u0026#39;, encoding = \u0026quot;ISO-8859-1\u0026quot;, na_filter=False) df = df.ix[1:,0:2] from IPython.display import HTML HTML(df.to_html()) \n  \nOut[5]:   LoanStatNew Description     1 acc_open_past_24mths Number of trades opened in past 24 months.   2 addr_state The state provided by the borrower in the loan...   3 all_util Balance to credit limit on all trades   4 annual_inc The self-reported annual income provided by th...   5 annual_inc_joint The combined self-reported annual income provi...   6 application_type Indicates whether the loan is an individual ap...   7 avg_cur_bal Average current balance of all accounts   8 bc_open_to_buy Total open to buy on revolving bankcards.   9 bc_util Ratio of total current balance to high credit/...   10 chargeoff_within_12_mths Number of charge-offs within 12 months   11 collection_recovery_fee post charge off collection fee   12 collections_12_mths_ex_med Number of collections in 12 months excluding m...   13 delinq_2yrs The number of 30+ days past-due incidences of ...   14 delinq_amnt The past-due amount owed for the accounts on w...   15 desc Loan description provided by the borrower   16 dti A ratio calculated using the borrowerÕs total ...   17 dti_joint A ratio calculated using the co-borrowers' tot...   18 earliest_cr_line The month the borrower's earliest reported cre...   19 emp_length Employment length in years. Possible values ar...   20 emp_title The job title supplied by the Borrower when ap...   21 fico_range_high The upper boundary range the borrowerÕs FICO a...   22 fico_range_low The lower boundary range the borrowerÕs FICO a...   23 funded_amnt The total amount committed to that loan at tha...   24 funded_amnt_inv The total amount committed by investors for th...   25 grade LC assigned loan grade   26 home_ownership The home ownership status provided by the borr...   27 id A unique LC assigned ID for the loan listing.   28 il_util Ratio of total current balance to high credit/...   29 initial_list_status The initial listing status of the loan. Possib...   30 inq_fi Number of personal finance inquiries   31 inq_last_12m Number of credit inquiries in past 12 months   32 inq_last_6mths The number of inquiries in past 6 months (excl...   33 installment The monthly payment owed by the borrower if th...   34 int_rate Interest Rate on the loan   35 issue_d The month which the loan was funded   36 last_credit_pull_d The most recent month LC pulled credit for thi...   37 last_fico_range_high The upper boundary range the borrowerÕs last F...   38 last_fico_range_low The lower boundary range the borrowerÕs last F...   39 last_pymnt_amnt Last total payment amount received   40 last_pymnt_d Last month payment was received   41 loan_amnt The listed amount of the loan applied for by t...   42 loan_status Current status of the loan   43 max_bal_bc Maximum current balance owed on all revolving ...   44 member_id A unique LC assigned Id for the borrower member.   45 mo_sin_old_il_acct Months since oldest bank installment account o...   46 mo_sin_old_rev_tl_op Months since oldest revolving account opened   47 mo_sin_rcnt_rev_tl_op Months since most recent revolving account opened   48 mo_sin_rcnt_tl Months since most recent account opened   49 mort_acc Number of mortgage accounts.   50 mths_since_last_delinq The number of months since the borrower's last...   51 mths_since_last_major_derog Months since most recent 90-day or worse rating   52 mths_since_last_record The number of months since the last public rec...   53 mths_since_rcnt_il Months since most recent installment accounts ...   54 mths_since_recent_bc Months since most recent bankcard account opened.   55 mths_since_recent_bc_dlq Months since most recent bankcard delinquency   56 mths_since_recent_inq Months since most recent inquiry.   57 mths_since_recent_revol_delinq Months since most recent revolving delinquency.   58 next_pymnt_d Next scheduled payment date   59 num_accts_ever_120_pd Number of accounts ever 120 or more days past due   60 num_actv_bc_tl Number of currently active bankcard accounts   61 num_actv_rev_tl Number of currently active revolving trades   62 num_bc_sats Number of satisfactory bankcard accounts   63 num_bc_tl Number of bankcard accounts   64 num_il_tl Number of installment accounts   65 num_op_rev_tl Number of open revolving accounts   66 num_rev_accts Number of revolving accounts   67 num_rev_tl_bal_gt_0 Number of revolving trades with balance \u0026gt;0   68 num_sats Number of satisfactory accounts   69 num_tl_120dpd_2m Number of accounts currently 120 days past due...   70 num_tl_30dpd Number of accounts currently 30 days past due ...   71 num_tl_90g_dpd_24m Number of accounts 90 or more days past due in...   72 num_tl_op_past_12m Number of accounts opened in past 12 months   73 open_acc The number of open credit lines in the borrowe...   74 open_acc_6m Number of open trades in last 6 months   75 open_il_12m Number of installment accounts opened in past ...   76 open_il_24m Number of installment accounts opened in past ...   77 open_il_6m Number of currently active installment trades   78 open_rv_12m Number of revolving trades opened in past 12 m...   79 open_rv_24m Number of revolving trades opened in past 24 m...   80 out_prncp Remaining outstanding principal for total amou...   81 out_prncp_inv Remaining outstanding principal for portion of...   82 pct_tl_nvr_dlq Percent of trades never delinquent   83 percent_bc_gt_75 Percentage of all bankcard accounts \u0026gt; 75% of l...   84 policy_code publicly available policy_code=1 new products ...   85 pub_rec Number of derogatory public records   86 pub_rec_bankruptcies Number of public record bankruptcies   87 purpose A category provided by the borrower for the lo...   88 pymnt_plan Indicates if a payment plan has been put in pl...   89 recoveries post charge off gross recovery   90 revol_bal Total credit revolving balance   91 revol_util Revolving line utilization rate, or the amount...   92 sub_grade LC assigned loan subgrade   93 tax_liens Number of tax liens   94 term The number of payments on the loan. Values are...   95 title The loan title provided by the borrower   96 tot_coll_amt Total collection amounts ever owed   97 tot_cur_bal Total current balance of all accounts   98 tot_hi_cred_lim Total high credit/credit limit   99 total_acc The total number of credit lines currently in ...   100 total_bal_ex_mort Total credit balance excluding mortgage   101 total_bal_il Total current balance of all installment accounts   102 total_bc_limit Total bankcard high credit/credit limit   103 total_cu_tl Number of finance trades   104 total_il_high_credit_limit Total installment high credit/credit limit   105 total_pymnt Payments received to date for total amount funded   106 total_pymnt_inv Payments received to date for portion of total...   107 total_rec_int Interest received to date   108 total_rec_late_fee Late fees received to date   109 total_rec_prncp Principal received to date   110 total_rev_hi_lim Ê Total revolving high credit/credit limit   111 url URL for the LC page with listing data.   112 verification_status Indicates if income was verified by LC, not ve...   113 verified_status_joint Indicates if the co-borrowers' joint income wa...   114 zip_code The first 3 numbers of the zip code provided b...     \n \n  Lets choose some of the most important variables from these.\nThe Response Variable:\n Interest Rate (int_rate)  And some of possible important factors are:\n Annual Income (annual_inc) State (addr_state) Purpose (purpose) Description for Loan (desc) Amount Requested (loan_amount) Amount Funded (funded_amnt) Loan Length (term) Debt Income Ratio (dti) Home Ownership status (home_ownership) FICO high (fico_range_high) FICO low (fico_range_low) Last FICO low (last_fico_range_low) Last FICO high (last_fico_range_high) Average current balance (avg_cur_bal) Charge Offs in last Year (chargeoff_within_12_mths) Number of 30+ days past-due incidences (delinq_2yrs) Employment Length (emp_length) No. of Credit Inquiries (inq_last_6mths) Maximum current balance owed on all revolving (max_bal_bc) Total credit revolving balance (revol_bal) LC Verification status (verification_status) Revolving line utilization rate (revol_util) Percentage of account never delinquent (pct_tl_nvr_dlq) Months since most recent 90-day or worse rating (mths_since_last_major_derog) Total Credit Balance (total_bal_ex_mort)  We will first look at effects of some of these variables using EDA.\nLater, if we find any need to use some additional variables, we will revisit this list.\nFirst, lets load our data as a Pandas data frame:\n   In\u0026nbsp;[5]: df = pd.read_csv(\u0026quot;/home/ssingh/LendingClubData/LoanStats3c_securev1.csv\u0026quot;, skiprows=1, skipfooter=2) df.info(verbose = False) \n  \n \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 235629 entries, 0 to 235628 Columns: 115 entries, id to total_il_high_credit_limit dtypes: float64(44), int64(47), object(24) memory usage: 206.7+ MB     \n In\u0026nbsp;[6]: df.head(3) \n  \nOut[6]:    id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade sub_grade ... num_tl_90g_dpd_24m num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75 pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort total_bc_limit total_il_high_credit_limit     0 38098114 40860827 15000 15000 15000 60 months 12.39% 336.64 C C1 ... 0 4 100.0 0.0 0 0 196500 149140 10000 12000   1 36805548 39558264 10400 10400 10400 36 months 6.99% 321.08 A A3 ... 0 4 83.3 14.3 0 0 179407 15030 13000 11325   2 37662224 40425321 7650 7650 7650 36 months 13.66% 260.20 C C3 ... 0 2 100.0 100.0 0 0 82331 64426 4900 64031    3 rows × 115 columns\n  \n \n  Lets us remove columns/data that is redundant for prediction of interest rates. For example, id, member_id for sure are of no importance to us. Let\u0026rsquo;s work through the columns in batches to keep the cognitive burden low:\n   In\u0026nbsp;[7]: # .ix[row slice, column slice]  df.ix[:4,:7] \n  \nOut[7]:    id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate     0 38098114 40860827 15000 15000 15000 60 months 12.39%   1 36805548 39558264 10400 10400 10400 36 months 6.99%   2 37662224 40425321 7650 7650 7650 36 months 13.66%   3 37612354 40375473 12800 12800 12800 60 months 17.14%   4 37822187 40585251 9600 9600 9600 36 months 13.66%      \n \n  We won\u0026rsquo;t need id or member_id as it has no real predictive power so we can drop them from this table int_rate was loaded as an object data type instead of float due to the \u0026lsquo;%\u0026rsquo; character. Let\u0026rsquo;s strip that out and convert the column type. And Also do similar transformation for term variable to get rid of months.\n   In\u0026nbsp;[8]: df.drop([\u0026#39;id\u0026#39;,\u0026#39;member_id\u0026#39;], 1, inplace=True) df.int_rate = pd.Series(df.int_rate).str.replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;).astype(float) df[\u0026#39;term\u0026#39;].replace(to_replace=\u0026#39;[^0-9]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) df[\u0026#39;term\u0026#39;] = df[\u0026#39;term\u0026#39;].convert_objects(convert_numeric=True) \n  \n  Moving on to next columns:\n   In\u0026nbsp;[9]: df.ix[:4,5:15] \n  \nOut[9]:    installment grade sub_grade emp_title emp_length home_ownership annual_inc verification_status issue_d loan_status     0 336.64 C C1 MANAGEMENT 10+ years RENT 78000.0 Source Verified Dec-2014 Current   1 321.08 A A3 Truck Driver Delivery Personel 8 years MORTGAGE 58000.0 Not Verified Dec-2014 Current   2 260.20 C C3 Technical Specialist \u0026lt; 1 year RENT 50000.0 Source Verified Dec-2014 Charged Off   3 319.08 D D4 Senior Sales Professional 10+ years MORTGAGE 125000.0 Verified Dec-2014 Current   4 326.53 C C3 Admin Specialist 10+ years RENT 69000.0 Source Verified Dec-2014 Fully Paid      \n \n  At first, it seems we employment title should be important. Let us first have a look at how many unique values we have for these. We would like to convert emp_length into an integer variable.\n   In\u0026nbsp;[10]: print(df.emp_title.value_counts().head()) print(df.emp_title.value_counts().tail()) df.emp_title.unique().shape \n  \n Teacher 4569 Manager 3772 Registered Nurse 1960 RN 1816 Supervisor 1663 Name: emp_title, dtype: int64 Care Aid 1 Deputy Probation 1 Front office staff 1 factor 1 Independent Contractor/Driver 1 Name: emp_title, dtype: int64    Out[10]: (75353,)  \n \n  This is just too many. Unless, we do some semantics based grouping of these titles, we would not be able to get any meaningful data out of this. If you think harder, this should be highly correlated with income. Just for the purpose of loan, it is highly unlikely (not impossible though!) that a high paying job title would be different than another!\n   In\u0026nbsp;[11]: df.drop([\u0026#39;emp_title\u0026#39;], 1, inplace=True) \n  \n  Let first look at unique value of emp_legth variable.\n   In\u0026nbsp;[12]: df.emp_length.value_counts() \n  \nOut[12]: 10+ years 79505 2 years 20487 3 years 18267 \u0026lt; 1 year 17982 1 year 14593 4 years 13528 7 years 13099 5 years 13051 n/a 12019 8 years 11853 6 years 11821 9 years 9424 Name: emp_length, dtype: int64  \n \n  Let us make this variable simple integers. We will replace na entries with 0, and all non numeric entries.\n   In\u0026nbsp;[13]: df.replace(\u0026#39;n/a\u0026#39;, np.nan, inplace=True) df.emp_length.fillna(value=0,inplace=True) df[\u0026#39;emp_length\u0026#39;].replace(to_replace=\u0026#39;^\u0026lt;\u0026#39;, value=0.0, inplace=True, regex=True) df[\u0026#39;emp_length\u0026#39;].replace(to_replace=\u0026#39;[^0-9]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) df[\u0026#39;emp_length\u0026#39;] = df[\u0026#39;emp_length\u0026#39;].astype(int) \n  \n  Lets us look at unique emp_length entries again:\n   In\u0026nbsp;[14]: df.emp_length.value_counts() \n  \nOut[14]: 10 79505 0 30001 2 20487 3 18267 1 14593 4 13528 7 13099 5 13051 8 11853 6 11821 9 9424 Name: emp_length, dtype: int64  \n \n  We should convert verification status into three ordinal values. Lets see what are different possible values of the verification status. We will convert these to ordinals accordingly.\n   In\u0026nbsp;[15]: df.verification_status.value_counts() \n  \nOut[15]: Source Verified 97741 Not Verified 70659 Verified 67229 Name: verification_status, dtype: int64  \n \n  We will assign the lowest rating to Not Verified and the highest rating to Verified.\n   In\u0026nbsp;[16]: df[\u0026quot;verification_status\u0026quot;] = df[\u0026quot;verification_status\u0026quot;].astype(\u0026#39;category\u0026#39;) df[\u0026quot;verification_status\u0026quot;] = df[\u0026quot;verification_status\u0026quot;].cat.set_categories([\u0026quot;Not Verified\u0026quot;, \u0026quot;Source Verified\u0026quot;, \u0026quot;Verified\u0026quot;], ordered = True) \n  \n  Loan status and issue dates are not of any interest to us, as we only plan to use this data for making models that can predict interest rate. However, let us do some digging into this data to see some statistics of loans on Lending Club! Let us look at a histogram of different states of loans.\n   In\u0026nbsp;[23]: import seaborn as sns sns.set() sns.set_context(\u0026quot;notebook\u0026quot;, font_scale=1.5, rc={\u0026quot;lines.linewidth\u0026quot;: 2.5}) total = float(len(df.index)) ax = sns.countplot(x=\u0026quot;loan_status\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); for p in ax.patches: height = p.get_height() ax.text(p.get_x(), height+18, \u0026#39;%2.2f\u0026#39;%(height*100/total)+\u0026quot;%\u0026quot;) plt.xticks(rotation=60) plt.show() \n  \n  \n \n  We can also look at a histogram of number of loans issued based on the month of the year. Just for fun, we will also look for any correlation between the issue month of loans and their states.\n   In\u0026nbsp;[18]: df[\u0026#39;issue_d\u0026#39;].replace(to_replace=\u0026#39;[^A-Z,a-z]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) ax = sns.countplot(x=\u0026quot;issue_d\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); for p in ax.patches: height = p.get_height() ax.text(p.get_x(), height+18, \u0026#39;%2.1f\u0026#39;%(height*100/total)) plt.xticks(rotation=60) plt.show() \n  \n  \n \n  First, we find that more loans are issued during the holidays seasons: Oct and July! Nothing surprising there.\n   In\u0026nbsp;[19]: g = sns.factorplot(\u0026quot;loan_status\u0026quot;, col=\u0026quot;issue_d\u0026quot;, col_wrap=4, data=df,kind=\u0026quot;count\u0026quot;, aspect=1.25) (g.set_axis_labels(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) .set_titles(\u0026quot;{col_name}\u0026quot;) .set_xticklabels(rotation=60) .despine(left=True)) \n  \nOut[19]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fe6351a72b0\u0026gt;  \n  \n \n  We do not find any obvious correlation between the issue date and their states at this time. Not interesting. Lets drop both issue date from our further analysis. Moving on to next set of columns.\nWe will also divide loan status into two categories: Low, and High risks.\n   In\u0026nbsp;[20]: defaultList = [\u0026quot;Default\u0026quot;, \u0026quot;Charged Off\u0026quot;, \u0026quot;Late (31-120 days)\u0026quot;, \u0026quot;Late (16-30 days)\u0026quot;, \u0026quot;In Grace Period\u0026quot;] df.loc[df.loan_status.isin(defaultList), \u0026quot;loan_status\u0026quot;] = \u0026quot;High\u0026quot; goodList = [\u0026quot;Current\u0026quot;, \u0026quot;Fully Paid\u0026quot;] df.loc[df.loan_status.isin(goodList), \u0026quot;loan_status\u0026quot;] = \u0026quot;Low\u0026quot; \n  \n In\u0026nbsp;[21]: df.drop([\u0026#39;issue_d\u0026#39;],1, inplace=True) \n  \n  Let us stop here for this post. We will continue our EDA analysis in the next. We will save our pandas object as pickle and then catch up from there.\n   In\u0026nbsp;[22]: df.to_pickle(\u0026quot;/home/ssingh/LendingClubData/Part1.pickle\u0026quot;) \n  \n\n","title":"EDA of Lending Club Data","url":"https://sadanand-singh.github.io/posts/lceda1/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this section, we will continue re-using the data from the previous post based on Pseudo Facebook data from udacity.\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited tsv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[1]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning)    Out[1]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  Continuing with our analysis from the last post, finding a relationship between age and friends counts, let us add gender to the equation.\nIn order to do this, we will first use groupby() and agg() to get group by data.\n   In\u0026nbsp;[2]: def groupByStats(df, groupCol, statsCol): \u0026#39;\u0026#39;\u0026#39; return a dataframe with groupByCo groupbyCol: a string or a list of strings for col names in df statsCol: a string for col in df of which we need stats for \u0026#39;\u0026#39;\u0026#39;\n # Define the aggregation calculations aggregations = { statsCol: { (statsCol+\u0026#39;_mean\u0026#39;): \u0026#39;mean\u0026#39;, (statsCol+\u0026#39;_median\u0026#39;): \u0026#39;median\u0026#39;, (statsCol+\u0026#39;_q25\u0026#39;): lambda x: np.percentile(x,25), (statsCol+\u0026#39;_q75\u0026#39;): lambda x: np.percentile(x,75), \u0026#39;n\u0026#39;: \u0026#39;count\u0026#39; } } df_group_by_groupCol = df.groupby(groupCol, as_index=False, group_keys=False).agg(aggregations) df_group_by_groupCol.columns = df_group_by_groupCol.columns.droplevel() if isinstance(groupCol, list): cols = groupCol + list(df_group_by_groupCol.columns)[len(groupCol):] else: cols = [groupCol] + list(df_group_by_groupCol.columns)[1:] df_group_by_groupCol.columns = cols return df_group_by_groupCol    \n In\u0026nbsp;[3]: pf_group_by_age_gender = groupByStats(pf[pf.gender.notnull()], [\u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;], \u0026#39;friend_count\u0026#39;) pf_group_by_age_gender.head().T \n  \nOut[3]:    gender age friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 female 13 316.00 193 259.160622 148.0 39.0   1 female 14 410.50 847 362.428571 224.0 79.0   2 female 15 592.50 1139 538.681299 276.0 104.0   3 female 16 581.25 1238 519.514540 258.5 102.0   4 female 17 586.75 1236 538.994337 245.5 81.0      \n \n  Now, we can use plots to compare friend counts across age and gender.\n   In\u0026nbsp;[4]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\npf_group_by_age_gender[\u0026#39;unit\u0026#39;] = \u0026quot;U\u0026quot; ax = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_median\u0026#39;, condition=\u0026#39;gender\u0026#39;, unit=\u0026#39;unit\u0026#39;, data=pf_group_by_age_gender) plt.ylabel(\u0026quot;Friend Count Median\u0026quot;) plt.xlim(10,113) \n  \nOut[4]: (10, 113)  \n  \n \n  It would be helpful to analyze these differences in relative terms. So lets answer a different question - how many times more friends does the average female users have than average male users.\nTo answer the above question we will need to transform our data. Right now, our data is in “long format” - a row of data different values of different variables. We will need to convert this to a “wide format” - where we will create columns called \u0026lsquo;male\u0026rsquo; and \u0026lsquo;female\u0026rsquo;, that will have median counts in them.\nThis can be done using the “pivot()” method of pandas.\n   In\u0026nbsp;[5]: pf_group_by_age_gender_wide = pf_group_by_age_gender.pivot(index=\u0026#39;age\u0026#39;, columns=\u0026#39;gender\u0026#39;, values=\u0026#39;friend_count_median\u0026#39;) pf_group_by_age_gender_wide.columns.name = \u0026#39;\u0026#39; pf_group_by_age_gender_wide.reset_index(level=0, inplace=True) pf_group_by_age_gender_wide.head() \n  \nOut[5]:    age female male     0 13 148.0 55.0   1 14 224.0 92.5   2 15 276.0 106.5   3 16 258.5 136.0   4 17 245.5 125.0      \n \n  Similarly, we can use the melt() function to convert a wide format data back to a long format data. Now lets plot ratio of female to male median friend counts.\n   In\u0026nbsp;[6]: pf_group_by_age_gender_wide[\u0026#39;female_male_ratio\u0026#39;] = pf_group_by_age_gender_wide.female/pf_group_by_age_gender_wide.male pf_group_by_age_gender_wide[\u0026#39;unit\u0026#39;] = \u0026quot;u\u0026quot; ax = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;female_male_ratio\u0026#39;, unit=\u0026#39;unit\u0026#39;, data=pf_group_by_age_gender_wide) plt.plot([13, 113], [1, 1], linewidth=2, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;\u0026ndash;\u0026#39;) plt.ylabel(\u0026quot;Female-Male Ratio\u0026quot;) \n  \nOut[6]: \u0026lt;matplotlib.text.Text at 0x7fe86804b4a8\u0026gt;  \n  \n \n  In this particular case of looking at multiple variables, it would make more sense to look at count of friends as function of tenure of Facebook as well. People with a longer tenure of Facebook account are likely to accumulate a larger number of friends.\nIn the following section we will look at friend count of females, males at different ages along with their Facebook tenure.\n   In\u0026nbsp;[7]: pf[\u0026#39;year_joined\u0026#39;] = np.floor(2014 - pf[\u0026#39;tenure\u0026#39;]/365) \n  \n In\u0026nbsp;[8]: pf.year_joined.value_counts(dropna=False) \n  \nOut[8]:  2013.0 43588 2012.0 33366 2011.0 9860 2010.0 5448 2009.0 4557 2008.0 1507 2007.0 581 2014.0 70 2006.0 15 2005.0 9 NaN 2 Name: year_joined, dtype: int64  \n \n  The tabular view of this shows the distribution. We will next use the cut() method to bin this variable.\n   In\u0026nbsp;[9]: pf[\u0026#39;year_joined\u0026#39;] = pd.cut(pf.year_joined, bins=[2004,2009,2011,2012,2014]) pf[\u0026#39;year_joined\u0026#39;] = pf[\u0026#39;year_joined\u0026#39;].astype(\u0026#39;category\u0026#39;) \n  \n In\u0026nbsp;[10]: pf[\u0026#39;year_joined\u0026#39;].value_counts(dropna=False) \n  \nOut[10]: (2012, 2014] 43658 (2011, 2012] 33366 (2009, 2011] 15308 (2004, 2009] 6669 NaN 2 Name: year_joined, dtype: int64  \n \n  Let us now plot all these variables together. In particular, we want to create a plot of friend counts vs age where a different line is shown for each bin of year joined.\n   In\u0026nbsp;[11]: pf_group_by_age = groupByStats(pf, [\u0026#39;age\u0026#39;, \u0026#39;year_joined\u0026#39;], \u0026#39;friend_count\u0026#39;) pf_group_by_age.head().T \n  \nOut[11]:    age year_joined friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 13 (2009, 2011] 691.50 11 469.818182 362.0 124.50   1 13 (2011, 2012] 406.75 48 352.333333 248.5 145.25   2 13 (2012, 2014] 167.00 425 135.668235 63.0 18.00   3 14 (2009, 2011] 1033.75 28 860.928571 517.0 279.25   4 14 (2011, 2012] 383.00 449 350.812918 224.0 109.00      \n \n In\u0026nbsp;[12]: pf_group_by_age[\u0026#39;unit\u0026#39;] = \u0026#39;U\u0026#39; fig, ax = plt.subplots(figsize=(8,6)) g = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_median\u0026#39;, unit=\u0026#39;unit\u0026#39;,\ncondition=\u0026#39;year_joined\u0026#39;, data=pf_group_by_age, ax=ax) \n  \n  \n \n  Our initial hypothesis seems to be correct here - People with larger Facebook tenure tend to have higher friend counts. Lets us plot the mean of these bins and also the grand means of data.\n   In\u0026nbsp;[13]: fig, ax = plt.subplots(figsize=(8,6)) g = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_mean\u0026#39;, unit=\u0026#39;unit\u0026#39;,\ncondition=\u0026#39;year_joined\u0026#39;, data=pf_group_by_age, ax=ax) g = pf_group_by_age.groupby(\u0026#39;age\u0026#39;, as_index=False).mean() g = g.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;, ax=ax) \n  \n  \n \n  Since the trend holds up after conditioning on the each of the buckets of year joined, we can increase our confidence that this observation isn’t just an artifact. Let us look at this from a different angle.\nWe can define a variable called “friend rate”, i.e. number of friends each person had on a per day basis.\n   In\u0026nbsp;[14]: df = pf.loc[pf[\u0026#39;tenure\u0026#39;] \u0026gt;= 1] (df[\u0026#39;friend_count\u0026#39;]/df[\u0026#39;tenure\u0026#39;]).describe() \n  \nOut[14]: count 98931.000000 mean 0.609609 std 2.557356 min 0.000000 25% 0.077486 50% 0.220486 75% 0.565802 max 417.000000 dtype: float64  \n \n  We can now look at the effect of this variable in more detail using the following plot:\n   In\u0026nbsp;[15]: df.is_copy = False s = df[\u0026#39;friendships_initiated\u0026#39;]/df[\u0026#39;tenure\u0026#39;] df[\u0026#39;friendships_initiated_per_tenure\u0026#39;] = s.astype(\u0026#39;int64\u0026#39;)\ng = sns.lmplot(x=\u0026quot;tenure\u0026quot;, y=\u0026quot;friendships_initiated_per_tenure\u0026quot;, hue=\u0026quot;year_joined\u0026quot;, legend_out=False, data=df, size=5, fit_reg=False, scatter_kws={\u0026#39;alpha\u0026#39;: 0.5}) plt.xlim(1,3400) \n  \nOut[15]: (1, 3400)  \n  \n \n  This shows that people initiate less number of friends as they have longer tenure at Facebook.\nTill now, we have looked at different aspects of the Facebook data set. We will now use a new data set for some more detailed multivariate analysis, and then finally come back to the Facebook data set for comparison.\nWe are going to work with a data set describing household purchases of five flavors of Dannon Yogurt in 8 oz sizes. Their price is recorded with each purchase occasion. This yogurt data set has a quite different structure than our pseudo-Facebook data set. The synthetic Facebook data has one row per individual with that row giving their characteristics and counts of behaviors over a single period of time. On the other hand, the yogurt data has many rows per household, one for each purchase occasion. This kind of micro-data is often useful for answering different types of questions than we’ve looked at so far.\nWe will start by loading the yogurt data set and then looking at its summary.\n   In\u0026nbsp;[16]: yo = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/yogurt.csv\u0026quot;) #summarize data yo.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[16]:    count mean std min 50% max     obs 2380.0 1.367797e+03 790.076032 1.0 1369.50 2743.00   id 2380.0 2.128592e+06 17799.723643 2100081.0 2126532.00 2170639.00   time 2380.0 1.004967e+04 227.079811 9662.0 10045.00 10459.00   strawberry 2380.0 6.491597e-01 1.058612 0.0 0.00 11.00   blueberry 2380.0 3.571429e-01 0.819690 0.0 0.00 12.00   pina.colada 2380.0 3.584034e-01 0.803858 0.0 0.00 10.00   plain 2380.0 2.176471e-01 0.606556 0.0 0.00 6.00   mixed.berry 2380.0 3.886555e-01 0.904311 0.0 0.00 8.00   price 2380.0 5.925089e+01 10.913256 20.0 65.04 68.96      \n \n  We notice that most of the variables are integers here. We should convert the id variable to factor. This will come handy later since the same household data is available in multiple rows.\n   In\u0026nbsp;[17]: yo[\u0026#39;id\u0026#39;] = yo[\u0026#39;id\u0026#39;].astype(\u0026#39;category\u0026#39;) #summarize data yo.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[17]:    count unique top freq mean std min 50% max     obs 2380.0    1367.8 790.076 1 1369.5 2743   id 2380.0 332 2.13229e+06 74        time 2380.0    10049.7 227.08 9662 10045 10459   strawberry 2380.0    0.64916 1.05861 0 0 11   blueberry 2380.0    0.357143 0.81969 0 0 12   pina.colada 2380.0    0.358403 0.803858 0 0 10   plain 2380.0    0.217647 0.606556 0 0 6   mixed.berry 2380.0    0.388655 0.904311 0 0 8   price 2380.0    59.2509 10.9133 20 65.04 68.96      \n \n  Let us look at the histogram of yogurt prices first.\n   In\u0026nbsp;[18]: ax = sns.distplot(yo[\u0026quot;price\u0026quot;], kde=False, bins=36) plt.xlabel(\u0026#39;Price\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Count\u0026#39;, fontsize=12) \n  \nOut[18]: \u0026lt;matplotlib.text.Text at 0x7fe8665c31d0\u0026gt;  \n  \n \n  Now that we have some idea about distribution of prices, let’s figure out on a given purchase occasion how many eight ounce yogurts does a household purchase. To answer this we need to combine counts of the different yogurt flavors into one variable. Then we can look at the histogram.\n   In\u0026nbsp;[19]: yo[\u0026#39;all_purchase\u0026#39;] = yo[\u0026#39;strawberry\u0026#39;]+yo[\u0026#39;blueberry\u0026#39;]+yo[\u0026#39;plain\u0026#39;]+yo[\u0026#39;pina.colada\u0026#39;]+yo[\u0026#39;mixed.berry\u0026#39;] ax = sns.distplot(yo[\u0026#39;all_purchase\u0026#39;], kde=False, bins=36) plt.xlim(1,22) plt.xlabel(\u0026#39;All Purchases\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Count\u0026#39;, fontsize=12) \n  \nOut[19]: \u0026lt;matplotlib.text.Text at 0x7fe86657a198\u0026gt;  \n  \n \n  It seems most household purchase one 8 oz yogurt at any given purchase. Now we will look at the scatter plot of prime vs time data.\n   In\u0026nbsp;[20]: fig, ax = plt.subplots(figsize=(8,6)) sd = {\u0026#39;alpha\u0026#39;: 0.25, \u0026#39;edgecolors\u0026#39;: \u0026#39;black\u0026#39;} g = sns.regplot(x=\u0026#39;time\u0026#39;, y=\u0026#39;price\u0026#39;, data=yo, x_jitter=2, y_jitter=0.5, fit_reg=False, ax=ax, color=\u0026#39;red\u0026#39;, scatter_kws=sd) \n  \n  \n \n  We see the baseline price of yogurt has been increasing over time. We also see some scattered prices around the baseline price, which could be simply due to usage of coupons, sales etc. by customers.\nWhen familiarizing yourself with a new data set that contains multiple observations of the same units, it’s often useful to work with a sample of those units so that it’s easy to display the raw data for that sample. In the case of the yogurt data set, we might want to look at a small sample of households in more detail so that we know what kind of within and between household variation we are working with. This analysis of a sub-sample might come before trying to use within household variation as part of a model. For example, this data set was originally used to model consumer preferences for variety. But, before doing that, we’d want to look at how often we observe households buying yogurt, how often they buy multiple items, and what prices they’re buying yogurt at. One way to do this is to look at some sub-sample in more detail. Let’s pick 16 households at random and take a closer look.\n   In\u0026nbsp;[21]: sample_ids = yo.id.unique() sample_ids = pd.Series(sample_ids).sample(16, random_state=200) df = yo.ix[yo[\u0026#39;id\u0026#39;].isin(list(sample_ids)),[\u0026#39;id\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;all_purchase\u0026#39;]] df = df.reset_index(drop=True) df[\u0026#39;id\u0026#39;]=pd.Series(list(df.id)).astype(\u0026#39;category\u0026#39;) df.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[21]:    count unique top freq mean std min 50% max     id 171.0 16 2.13229e+06 74        price 171.0    59.8746 9.82084 33.04 65.04 68.96   time 171.0    10002.6 233.708 9664 9981 10457   all_purchase 171.0    1.84795 1.38489 1 1 10      \n \n In\u0026nbsp;[22]: g = sns.lmplot(x=\u0026#39;time\u0026#39;,y=\u0026#39;price\u0026#39;, hue=\u0026#39;all_purchase\u0026#39;, data=df, scatter_kws={\u0026quot;s\u0026quot;: 20*df[\u0026#39;all_purchase\u0026#39;]},\ncol=\u0026#39;id\u0026#39;, col_wrap=4, size=2, aspect=1, fit_reg=False, palette=\u0026quot;Set1\u0026quot;) g.set_xticklabels(rotation=90) \n  \nOut[22]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fe866497a90\u0026gt;  \n  \n \n  From these plots, we can see the variation and how often each household buys yogurt. And it seems that some household purchases more quantities than others with the larger circles. For most of the households, the price of yogurt holds steady, or tends to increase over time.\nNow, there are, of course, some exceptions, like in household 2124545 and in 2118927 household, we might think that the household is using coupons to drive the price down. Now, we don’t have the coupon data to associate with this buying data, but we can see how that information could be paired to this data to better understand the consumer behavior.\nThe general idea is that if we have observations over time, we can facet by the primary unit, case, or individual in the data set. For our yogurt data it was the households we were faceting over.\nThis faceted time series plot is something we can’t generate with our pseudo Facebook data set. Since we don’t have data on our sample of users over time.\nThe Facebook data isn’t great for examining the process of friending over time. The data set is just a cross section, it’s just one snapshot at a fixed point that tells us the characteristics of individuals. Not the individuals over, say, a year.\nBut if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure. This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases.\nMuch of the analysis we’ve done so far focused on some pre-chosen variable, relationship or question of interest. We then used EDA to let those chosen variables speak and surprise us. Most recently, when analyzing the relationship between two variables we look to incorporate more variables in the analysis to improve it. For example, by seeing whether a particular relationship is consistent across values of those other variables. In choosing a third or fourth variable to plot we relied on our domain knowledge. But often, we might want visualizations or summaries to help us identify such auxiliary variables. In some analyses, we may plan to make use of a large number of variables. Perhaps, we are planning on predicting one variable with ten, 20, or hundreds of others. Or maybe we want to summarize a large set of variables into a smaller set of dimensions. Or perhaps, we’re looking for interesting relationships among a large set of variables. In such cases, we can help speed up our exploratory data analysis by producing many plots or comparisons at once. This could be one way to let the data set as a whole speak in part by drawing our attention to variables we didn’t have a preexisting interest in.\nWe should let the data speak to determine variables of interest. There’s a tool that we can use to create a number of scatter plots automatically.\nIt’s called a scatter plot matrix. In a scatter plot matrix. There’s a grid of scatter plots between every pair of variables. As we’ve seen, scatter plots are great, but not necessarily suited for all types of variables. For example, categorical ones. So there are other types of visualizations that can be created instead of scatter plots. Like box plots or histograms when the variables are categorical.\nLet’s produce the scatter plot matrix for our pseudo Facebook data set. We’re going to use the pairplot() method to do so.\n   In\u0026nbsp;[23]: pf_subset = pf.iloc[:,1:4] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[24]: pf_subset = pf.iloc[:,4:8] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[25]: pf_subset = pf.iloc[:,8:11] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[26]: pf_subset = pf.iloc[:,11:-1] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n  At the very least, a scatter plot matrix can be a useful starting point in many analyses.\nA matrix such as this one will be extremely helpful when we have even more variables than those in the pseudo-Facebook data set.\nExamples arise in many areas, but one that has attracted the attention of statisticians is genomic data. In these data sets, they’re often thousands of genetic measurements for each of a small number of samples. In some cases, some of these samples have a disease, and so we’d like to identify genes that are associated with the disease.\nWe will use an example data set of gene expression in tumors. The data contains the expression of 6,830 genes, compared with a larger baseline reference sample.\n   In\u0026nbsp;[27]: nci = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/nci.tsv\u0026quot;, sep = \u0026#39;\\s+\u0026#39;, header=None) nci.columns = list(range(1,65)) \n  \n In\u0026nbsp;[56]: fig, ax = plt.subplots(figsize=(8,6)) sns.heatmap(data=nci.loc[1:200,:], vmin=0, vmax=1.0, xticklabels=False, yticklabels=False, ax=ax, cmap=\u0026quot;YlGnBu\u0026quot;) plt.xlabel(\u0026quot;Case\u0026quot;, fontsize=14) _ = plt.ylabel(\u0026quot;Gene\u0026quot;, fontsize=14) \n  \n  \n \n  Heat maps can also be a good way to look at distributions of large dimensional data.\nIn summary in this post, we started with simple extensions to the scatter plot, and plots of conditional summaries that you worked with in lesson four, such as adding summaries for multiple groups. Then, we tried some techniques for examining a large number of variables at once, such as scatter-plot matrices and heat maps. We also learned how to reshape data, moving from broad data with one row per case, to aggregate data with one row per combination of variables, and we moved back and forth between long and wide formats for our data.\nIn the next and the final post in this series, We will do an in-depth analysis of the US department of Education dataset on college education, highlighting the role of EDA.\n  \n","title":"Exploring Multiple Variables","url":"https://sadanand-singh.github.io/posts/pyplotsmultivariables/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this section, we will be re-using the data from the previous post based on Pseudo Facebook data from udacity.\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[24]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning)    Out[24]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  Usually, it is best to use a scatter plot to analyze two variables:\n   In\u0026nbsp;[47]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False) plt.xlim(13, 90) plt.ylim(0,5000) \n  \nOut[47]: (0, 5000)  \n  \n \n  We can notice some really interesting behavior in the ugly scatter plot above:\n The age data is binned, as expected (only integers allowed) Young people around the age of 20 have maximum friend count. There is unusual spike in friends count of people aged more than 100. This could mostly be some flaw in the data, probably based on incorrect entry by the users. People around the age of 70 too have quite a large amount of friends. This is pretty interesting and could point to the use of the social media site by an unexpected group of people.  We can use summary command to find the bounds on age and then use that to limit age axis.\n   In\u0026nbsp;[26]: pf.age.describe() \n  \nOut[26]: count 99003.000000 mean 37.280224 std 22.589748 min 13.000000 25% 20.000000 50% 28.000000 75% 50.000000 max 113.000000 Name: age, dtype: float64  \n \n  Furthermore, we notice at some areas of the plot being too dense, where as some to be really sparse. The areas where points are too dense is called “over plotting” - It is impossible to extract any meaningful statistics from this region. In order to overcome this, we can set the transparency of the plots using the alpha parameter in the plt.scatter() method. Using a value of 1\u0026frasl;20 means, one point that will plotted will be equal to 20 original points.\n   In\u0026nbsp;[27]: ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;green\u0026#39;, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}) plt.xlim(13, 90) plt.ylim(0,5000) plt.plot([13, 90], [600, 600], linewidth=2, color=\u0026#39;r\u0026#39;) \n  \nOut[27]: [\u0026lt;matplotlib.lines.Line2D at 0x7f07d45a22e8\u0026gt;]  \n  \n \n  Based on these new plots, we can find bulk of higher friend count for younger people is still less than 600. We still find higher count for age group of 70.\nFurthermore, we can do a better representation of data using the coord_trans() method. We will be using a square root function.\nIn order to that, we will first create an \u0026ldquo;sqrt\u0026rdquo; scale.\n   In\u0026nbsp;[28]: import numpy as np from numpy import ma from matplotlib import scale as mscale from matplotlib import transforms as mtransforms from matplotlib.ticker import AutoLocator\nclass SqrtScale(mscale.ScaleBase): \u0026quot;\u0026quot;\u0026quot; Scales data using np.sqrt method.\nThe scale function: np.sqrt(x)\nThe inverse scale function: x**2 \u0026quot;\u0026quot;\u0026quot;\n # The scale class must have a member ``name`` that defines the # string used to select the scale. For example, # ``gca().set_yscale(\u0026quot;mercator\u0026quot;)`` would be used to select this # scale. name = \u0026#39;sqrt\u0026#39; def __init__(self, axis): \u0026quot;\u0026quot;\u0026quot; Any keyword arguments passed to ``set_xscale`` and ``set_yscale`` will be passed along to the scale\u0026#39;s constructor. \u0026quot;\u0026quot;\u0026quot; mscale.ScaleBase.__init__(self) def get_transform(self): \u0026quot;\u0026quot;\u0026quot; Override this method to return a new instance that does the actual transformation of the data. The SqrtTransform class is defined below as a nested class of this one. \u0026quot;\u0026quot;\u0026quot; return self.SqrtTransform() def set_default_locators_and_formatters(self, axis): \u0026quot;\u0026quot;\u0026quot; Override to set up the locators and formatters to use with the scale. \u0026quot;\u0026quot;\u0026quot; axis.set_major_locator(AutoLocator()) class SqrtTransform(mtransforms.Transform): # There are two value members that must be defined. # ``input_dims`` and ``output_dims`` specify number of input # dimensions and output dimensions to the transformation. # These are used by the transformation framework to do some # error checking and prevent incompatible transformations from # being connected together. When defining transforms for a # scale, which are, by definition, separable and have only one # dimension, these members should always be set to 1. input_dims = 1 output_dims = 1 is_separable = True def __init__(self): mtransforms.Transform.__init__(self) def transform_non_affine(self, a): \u0026quot;\u0026quot;\u0026quot; This transform takes an Nx1 ``numpy`` array and returns a transformed copy. \u0026quot;\u0026quot;\u0026quot; return np.sqrt(a) def inverted(self): \u0026quot;\u0026quot;\u0026quot; Override this method so matplotlib knows how to get the inverse transform for this transform. \u0026quot;\u0026quot;\u0026quot; return SqrtScale.InvertedSqrtTransform()  class InvertedSqrtTransform(mtransforms.Transform): input_dims = 1 output_dims = 1 is_separable = True  def __init__(self): mtransforms.Transform.__init__(self)  def transform_non_affine(self, a): return a**2  def inverted(self): return SqrtScale.SqrtTransform() # Now that the Scale class has been defined, it must be registered so # that matplotlib can find it. mscale.register_scale(SqrtScale) \n  \n In\u0026nbsp;[29]: fig, ax = plt.subplots() fig.set_size_inches(8.6, 6.4) ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;cyan\u0026#39;, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}, ax=ax) plt.xlim(13, 90) plt.ylim(0,4599) plt.yscale(\u0026#39;sqrt\u0026#39;) plt.plot([13, 90], [600, 600], linewidth=2, color=\u0026#39;r\u0026#39;) \n  \nOut[29]: [\u0026lt;matplotlib.lines.Line2D at 0x7f07d24f5588\u0026gt;]  \n  \n \n  On a similar way, we can look at relationship between friends initiated and age.\n   In\u0026nbsp;[30]: fig, ax = plt.subplots() fig.set_size_inches(8.6, 6.4) kws = {\u0026#39;alpha\u0026#39;: 0.05} ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friendships_initiated\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;purple\u0026#39;, scatter_kws=kws, ax=ax) plt.xlim(13, 90) plt.ylim(0,4599) plt.yscale(\u0026#39;sqrt\u0026#39;) \n  \n  \n \n  Interestingly, we find this distribution to be very similar to the one for friend count.\nScatter plots try to keep us very close to the data. It represents each and every data point. However, in order to judge the quality of a data, it is important to know its important statistics like mean, median etc. How does average of a variable vary wrt to the some other variable.\nWe want to say, study how does average friend count vary with age. In order to study this we will use the grouping properties of pandas module.\nFirst we want to group our data frame by age. Then, we can create a new data frame that lists friend count mean, median and frequency (n) by using first the groupby() and then using the agg() method. We can look at first few data points of this new data frame using the head() method.\n   In\u0026nbsp;[121]: def groupByStats(pf, groupCol, statsCol): \u0026#39;\u0026#39;\u0026#39; return a dataframe with groupByCol\u0026#39;\u0026#39;\u0026#39;\n # Define the aggregation calculations aggregations = {  statsCol: { (statsCol+\u0026#39;_mean\u0026#39;): \u0026#39;mean\u0026#39;, (statsCol+\u0026#39;_median\u0026#39;): \u0026#39;median\u0026#39;, (statsCol+\u0026#39;_q25\u0026#39;): lambda x: np.percentile(x,25), (statsCol+\u0026#39;_q75\u0026#39;): lambda x: np.percentile(x,75), \u0026#39;n\u0026#39;: \u0026#39;count\u0026#39;/div }  } pf_group_by_age = pf.groupby(groupCol, as_index=False).agg(aggregations).rename(columns = {\u0026#39;\u0026#39;:groupCol}) pf_group_by_age.columns = pf_group_by_age.columns.droplevel() return pf_group_by_age  pf_group_by_age = groupByStats(pf, \u0026#39;age\u0026#39;, \u0026#39;friend_count\u0026#39;) pf_group_by_age.head(20)    \nOut[121]:    age friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 13 230.00 484 164.750000 74.0 23.75   1 14 293.00 1925 251.390130 132.0 44.00   2 15 377.50 2618 347.692131 161.0 55.00   3 16 385.75 3086 351.937135 171.5 63.00   4 17 360.00 3283 350.300640 156.0 56.00   5 18 368.00 5196 331.166282 162.0 55.00   6 19 350.00 4391 333.692097 157.0 59.00   7 20 304.00 3769 283.499071 135.0 49.00   8 21 265.00 3671 235.941160 121.0 42.00   9 22 239.00 3032 211.394789 106.0 40.00   10 23 216.00 4404 202.842643 93.0 32.00   11 24 209.50 2827 185.712062 92.0 33.00   12 25 156.00 3641 131.021148 62.0 19.00   13 26 169.00 2815 144.008171 75.0 28.00   14 27 159.00 2240 134.147321 72.0 28.00   15 28 150.00 2364 125.835448 66.0 23.00   16 29 142.25 1936 120.818182 66.0 25.00   17 30 146.00 1716 115.208042 67.5 24.00   18 31 143.00 1694 118.459858 63.0 25.00   19 32 140.00 1443 114.279972 63.0 21.00      \n \n  Now, let us look at this new data frame visually. We can first look at the relationship between average friend count and age.\n   In\u0026nbsp;[97]: pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False) plt.ylabel(\u0026quot;Friend Count Mean\u0026quot;) \n  \nOut[97]: \u0026lt;matplotlib.text.Text at 0x7f07d16f9dd8\u0026gt;  \n  \n \n  We can use this plot as good summary of the original scatter plot and put the two on top of each other.\n   In\u0026nbsp;[109]: ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;cyan\u0026#39;, x_jitter=0.5, y_jitter=1.0, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_q25\u0026#39;, ax=ax, color=\u0026#39;red\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_median\u0026#39;, ax=ax, color=\u0026#39;blue\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, ax=ax, color=\u0026#39;green\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_q75\u0026#39;, ax=ax, color=\u0026#39;red\u0026#39;) plt.xlim(13, 70) plt.ylim(0,1000) plt.ylabel(\u0026quot;Friend Count\u0026quot;) \n  \nOut[109]: \u0026lt;matplotlib.text.Text at 0x7f07d091cbe0\u0026gt;  \n  \n \n  In the above plot, we can see that between the age group 30-69, 75% of population has less than 200 friends.\nIn stead of using 4 different summary measures to analyze the above data, we can use a single number!\nOften analysts will use correlation coefficients to summarize this. We will use the Pearson product moment correlation \u0026reg;. You can look at the pandas corr() method for details. This measures a linear correlation between two variables.\n   In\u0026nbsp;[112]: df = pf[(pf[\u0026#39;age\u0026#39;] \u0026lt; 70) \u0026amp; (pf[\u0026#39;age\u0026#39;] \u0026gt;= 13)] df[\u0026#39;age\u0026#39;].corr(df[\u0026#39;friend_count\u0026#39;], method=\u0026#39;pearson\u0026#39;) \n  \nOut[112]: -0.17121437273281295  \n \n  We can also have other measures of relationship. For example, a measure of monotonic relationship would be done using Spearman coefficient. Similarly, a measure of strength of dependence between two variables can be done using the “Kendall Rank Coefficient”. A more detailed description about these can be found at http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/.\nWe will now look at variables that are strictly correlated using scatter plots.\nOne such example in our dataset would be a relationship between likes_received (y) vs. www_likes_received (x).\n   In\u0026nbsp;[119]: ax = sns.regplot(x=\u0026#39;www_likes_received\u0026#39;, y=\u0026#39;likes_received\u0026#39;, data=pf, color=\u0026#39;cyan\u0026#39;, ci=None, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;}) plt.xlim(0, np.percentile(pf.www_likes_received, 95)) plt.ylim(0, np.percentile(pf.likes_received, 95)) \n  \nOut[119]: (0, 561.0)  \n  \n \n  We have used numpy percentile() method to find upper limits of x and y data. Additionally, we added a correlation line using the regplot().\nWe can find the numerical value of the correlation between these two variables.\n   In\u0026nbsp;[120]: pf[\u0026#39;www_likes_received\u0026#39;].corr(pf[\u0026#39;likes_received\u0026#39;], method=\u0026#39;pearson\u0026#39;) \n  \nOut[120]: 0.9479901803455516  \n \n  Correlation between two variables might not be a good thing always. For example in the above case, it was simply due to the artifact of the two data sets were highly coupled, one was a super set of the other.\nLet us take a look again at the modified data frame created using the groupby methods. In particular, we want to remove any noise in the average values.\n   In\u0026nbsp;[150]: pf[\u0026#39;age_with_months\u0026#39;] = pf.age + (12-pf.dob_month)/12 pf_group_by_age_with_months = groupByStats(pf, \u0026#39;age_with_months\u0026#39;, \u0026#39;friend_count\u0026#39;) pf1 = pf_group_by_age[pf_group_by_age[\u0026#39;age\u0026#39;] \u0026lt; 71] pf2 = pf_group_by_age_with_months[pf_group_by_age_with_months[\u0026#39;age_with_months\u0026#39;] \u0026lt; 71] \n  \n In\u0026nbsp;[157]: f, (ax1, ax2, ax3) = plt.subplots(3) f.set_size_inches(9, 9) sns.regplot(x=\u0026#39;age_with_months\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf2, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;}, ax=ax1) pf2.plot(x=\u0026#39;age_with_months\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax1) ax1.set_xlim([13, 71])\nsns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf1, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;green\u0026#39;}, ax=ax2) pf1.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax2) ax2.set_xlim([13, 71])\npf11 = pf1.copy() pf11[\u0026#39;ageRounded\u0026#39;] = np.round(pf1[\u0026#39;age\u0026#39;]/5.0)*5.0 sns.regplot(x=\u0026#39;ageRounded\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf11, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;cyan\u0026#39;}, ax=ax3) pf11.plot(x=\u0026#39;ageRounded\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax3) ax3.set_xlim([13, 71]) \n  \nOut[157]: (13, 71)  \n  \n \n  This is an example of bias variance trade off, and is similar to the trade off we make when choosing the bin width in histograms. One way, we can do this quite easily in seaborn is using the lowess option, however, in the current implementation it fails to provide any error estimate on the fitted regression!\nLowess option in the seaborn library uses Loess and Lowess method for smoothing. Here, the model is based on the idea that data is continuous and smooth.\nSo, through this post, we have noticed several ways to plot the same data. The obvious that arises is which plot to choose? In EDA, however, answer to this is, simply you should choose. The idea in EDA is that the same data when plotted differently, glean different incites.\n  \n","title":"Pseudo Facebook Data - Exploring Two Variables","url":"https://sadanand-singh.github.io/posts/pyplotstwovariables/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this post, we will learn about EDA of single variables using simple plots like histograms, frequency plots and box plots.\nData sets used below are part of a project from the UD651 course on udacity by Facebook. The data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a \u0026lt;TAB\u0026gt; delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[1]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median RuntimeWarning)    Out[1]:    count unique top freq mean std min 50% max     userid 99003.0    1.59705e+06 344059 1.00001e+06 1.59615e+06 2.19354e+06   age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0    14.5304 9.01561 1 14 31   dob_year 99003.0    1975.72 22.5897 1900 1985 2000   dob_month 99003.0    6.28337 3.52967 1 6 12   gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  We need convert some of the variables from numeric to category.\n   In\u0026nbsp;[2]: cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;)\n#summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \n /p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median RuntimeWarning)    Out[2]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  The goal of this analysis is to understand user behavior and their demographics. We want to understand what they are doing on the Facebook and what they use. Please note this is not a real Facebook dataset.\nOur goal is to do some basic EDA (Exploratory Data Analysis) to understand any underlying patterns in the data. We will first look at a histogram of User\u0026rsquo;s Birthdays.\n   In\u0026nbsp;[3]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nax = sns.countplot(x=\u0026quot;dob_day\u0026quot;, data=pf) \n  \n  \n \n  We see some peculiar behavior of the data on the 1st of the month. Let us plot this data in more detail, in per month basis.\n   In\u0026nbsp;[4]: g = sns.factorplot(\u0026quot;dob_day\u0026quot;, col=\u0026quot;dob_month\u0026quot;, col_wrap=4, data=pf, kind=\u0026#39;count\u0026#39;, size=2.5, aspect=.8) g.set(xticklabels=[]) \n  \nOut[4]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fffcd441208\u0026gt;  \n  \n \n  This explains the above plot. Because of the default settings, or users privacy concerns, numerous people have 1\u0026frasl;1 as their birthdays!\nNow, let us explore the distribution of friend counts in this data.\n   In\u0026nbsp;[5]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, bins=100) plt.xlim(0,1000) \n  \nOut[5]: (0, 1000)  \n  \n \n  We see the data has some outliers near 5000. This is an example of a long tail data. We want our analysis to be focused on the bunch of Facebook users, so we need to limit the axes of these plots. Additionally, we also want to look at these data as a function of gender. However, We also want to remove any data where gender is NA.\n   In\u0026nbsp;[6]: df = pf[pf.gender.notnull()] g = sns.FacetGrid(df, col=\u0026quot;gender\u0026quot;) g = g.map(plt.hist, \u0026quot;friend_count\u0026quot;, bins=100, color=\u0026quot;b\u0026quot;) plt.xlim(0,1000) \n  \nOut[6]: (0, 1000)  \n  \n \n  If we want to know, mean statistics of our data, we can use the \u0026lsquo;value_counts\u0026rsquo; command.\n   In\u0026nbsp;[164]: pf.groupby(\u0026#39;gender\u0026#39;).friend_count.describe() \n  \nOut[164]: gender female count 40254.000000 mean 241.969941 std 476.039706 min 0.000000 25% 37.000000 50% 96.000000 75% 244.000000 max 4923.000000 male count 58574.000000 mean 165.035459 std 308.466702 min 0.000000 25% 27.000000 50% 74.000000 75% 182.000000 max 4917.000000 Name: friend_count, dtype: float64  \n \n  Let us know look at the tenure of usage (measured in Years) of Facebook.\n   In\u0026nbsp;[8]: df = pf[pf.tenure.notnull()] ax = sns.distplot(df[\u0026quot;tenure\u0026quot;]/365, kde=False, bins=36) plt.xlim(0,7) plt.xlabel(\u0026#39;Number of years using Facebook\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Number of users in sample\u0026#39;, fontsize=12) \n  \nOut[8]: \u0026lt;matplotlib.text.Text at 0x7fffc9590ef0\u0026gt;  \n  \n \n  We will now look at any pattern in the ages of Facebook users in this dataset.\n   In\u0026nbsp;[9]: ax = sns.distplot(pf[\u0026quot;age\u0026quot;], kde=False, bins=100) plt.xlim(13,113) plt.xlabel(\u0026#39;Age of Users in Years\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Number of users in sample\u0026#39;, fontsize=12) \n  \nOut[9]: \u0026lt;matplotlib.text.Text at 0x7fffc94f7780\u0026gt;  \n  \n \n  One general theme of observation here is that most of the data have a long tail. In these circumstances, it is better to look at such data after certain types of transformation. Let us do such an analysis of “friend_count”.\n   In\u0026nbsp;[35]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, hist_kws={\u0026quot;alpha\u0026quot;: 0.9}) \n  \n  \n \n In\u0026nbsp;[49]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, bins=np.logspace(0,4), hist_kws={\u0026quot;alpha\u0026quot;: 0.9}) plt.xscale(\u0026#39;log\u0026#39;) \n  \n  \n \n  Let us try to compare distribution of male vs female friend counts.\n   In\u0026nbsp;[175]: def plotDensity(x, color=None, label=None, bins=np.linspace(0,1000,200), **kws):\n w = 100*np.ones_like(x)/x.size plt.hist(x, bins=bins, alpha=0.4, histtype=\u0026#39;step\u0026#39;, linewidth=2, label=label, color=color, weights=w, **kws) return  g = sns.FacetGrid(df, col=None, hue=\u0026#39;gender\u0026#39;, size=6.0, xlim=(6,600), ylim=(0,5), legend_out=True) g = (g.map(plotDensity, \u0026#39;friend_count\u0026#39;)).add_legend() g = g.set_axis_labels(\u0026#39;Friend Count\u0026#39;, \u0026#39;% of users\u0026#39;) \n  \n  \n \n  Similarly, we can compare distributions of www likes.\n   In\u0026nbsp;[179]: g = sns.FacetGrid(df, col=None, hue=\u0026#39;gender\u0026#39;, size=6.0, xlim=(1,15000)) g = (g.map(plotDensity, \u0026#39;www_likes\u0026#39;, bins=np.logspace(0,5,50))).add_legend() g = g.set_axis_labels(\u0026#39;www Likes Count\u0026#39;, \u0026#39;% of users\u0026#39;) plt.xscale(\u0026#39;log\u0026#39;) \n  \n  \n \n  We cal also look at the total number of likes numerically per gender, as follows:\n   In\u0026nbsp;[173]: pf.groupby(\u0026#39;gender\u0026#39;).www_likes.sum() \n  \nOut[173]: gender female 3507665 male 1430175 Name: www_likes, dtype: int64  \n \n  We can also compare two distributions graphically using “box plots”. We can also look at the actual value using the by command. Here, we are trying to understand which gender initiated more friendships.\n   In\u0026nbsp;[185]: ax = sns.boxplot(x=\u0026#39;gender\u0026#39;, y=\u0026#39;friendships_initiated\u0026#39;, data=df) plt.ylim(0,200) \n  \nOut[185]: (0, 200)  \n  \n \n In\u0026nbsp;[186]: pf.groupby(\u0026#39;gender\u0026#39;).friendships_initiated.describe() \n  \nOut[186]: gender female count 40254.000000 mean 113.899091 std 195.139308 min 0.000000 25% 19.000000 50% 49.000000 75% 124.750000 max 3654.000000 male count 58574.000000 mean 103.066600 std 184.292570 min 0.000000 25% 15.000000 50% 44.000000 75% 111.000000 max 4144.000000 Name: friendships_initiated, dtype: float64  \n \n  Next, we want to understand if users have used certain features of Facebook or not. If we look at the summary of mobile_likes variable, median is close to 0, indicating a lot many users with 0 values for this variable. We can look also look at the logical value if value of this quantity is non-zero. We can additionally create a new variable called mobile_check_in that takes a value 1 if mobile_likes is non-zero.\n   In\u0026nbsp;[187]: pf.mobile_likes.describe() \n  \nOut[187]: count 99003.000000 mean 106.116300 std 445.252985 min 0.000000 25% 0.000000 50% 4.000000 75% 46.000000 max 25111.000000 Name: mobile_likes, dtype: float64  \n \n In\u0026nbsp;[201]: (pf.mobile_likes \u0026gt; 0).value_counts() \n  \nOut[201]: True 63947 False 35056 Name: mobile_likes, dtype: int64  \n \n In\u0026nbsp;[200]: pf[\u0026#39;mobile_check_in\u0026#39;] = pd.Series(np.where(pf[\u0026#39;mobile_likes\u0026#39;] \u0026gt; 0, 1, 0)).astype(\u0026#39;category\u0026#39;) pf.mobile_check_in.value_counts() \n  \nOut[200]: 1 63947 0 35056 Name: mobile_check_in, dtype: int64  \n \n  We can find percentage of people who have done mobile check in.\n   In\u0026nbsp;[203]: frac = (pf.mobile_check_in == 1).sum()/pf.mobile_check_in.size print(\u0026quot;Fraction of Mobile Check-ins = \u0026quot;, frac) \n  \n Fraction of Mobile Check-ins = 0.645909719907     \n  We find that about 65% of people have used mobile devices for check in and hence it would be a good decision to continue development of such products.\nIn summary, here we have learned to make inferences about single variable data using a combination of plots - histograms, box plots and frequency plots; along with various numerical data.\n  \n","title":"Pseudo Facebook Data - Plots in Python","url":"https://sadanand-singh.github.io/posts/onevariableeda/"},{"tags":"EDA, Python, Data Science","text":"The data set used here is part of a project from UD651 course on udacity by Facebook.\nThe data from the project corresponds to a survey from reddit.com. You can load the data through the following command. We will first look at the different attributes of this data using the summary() and describe() pandas methods.\n\nIn\u0026nbsp;[45]: import pandas as pd import numpy as np #Read csv file reddit = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/reddit.csv\u0026quot;).astype(object) #summarize data reddit.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \nOut[45]:    count unique top freq     id 32754.0 32754.0 32756 1.0   gender 32553.0 2.0 0 26418.0   age.range 32666.0 7.0 18-24 15802.0   marital.status 32749.0 6.0 Single 10428.0   employment.status 32603.0 6.0 Employed full time 14814.0   military.service 32749.0 2.0 No 30526.0   children 32535.0 2.0 No 27488.0   education 32610.0 7.0 Bachelor's degree 11046.0   country 32577.0 439.0 United States 20967.0   state 20846.0 52.0 California 3401.0   income.range 31139.0 8.0 Under $20,000 7892.0   fav.reddit 28393.0 1833.0 askreddit 2123.0   dog.cat 32749.0 3.0 I like dogs. 17151.0   cheese 32749.0 11.0 Other 6563.0      \n \n  The describe() method helped us get an overview of all the data available to us. We also ensured that all the data read was a categorical data.\nLet us look at the age.range variable in more detail. We can look at the different levels of this variables using the cat.categories property of a Pandas Series.\n   In\u0026nbsp;[46]: reddit[\u0026quot;age.range\u0026quot;].astype(\u0026#39;category\u0026#39;).cat.categories \n  \nOut[46]: Index([\u0026#39;18-24\u0026#39;, \u0026#39;25-34\u0026#39;, \u0026#39;35-44\u0026#39;, \u0026#39;45-54\u0026#39;, \u0026#39;55-64\u0026#39;, \u0026#39;65 or Above\u0026#39;, \u0026#39;Under 18\u0026#39;], dtype=\u0026#39;object\u0026#39;)  \n \n  This shows there are 7 possible values of this variable and some where no data is available (NA).\nA more pictorial view of this can be seen using a histogram plot of this.\n   In\u0026nbsp;[57]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nnewOrder = [\u0026quot;Under 18\u0026quot;, \u0026quot;18-24\u0026quot;, \u0026quot;25-34\u0026quot;, \u0026quot;35-44\u0026quot;, \u0026quot;45-54\u0026quot;, \u0026quot;55-64\u0026quot;, \u0026quot;65 or Above\u0026quot;] ax = sns.countplot(x=\u0026quot;age.range\u0026quot;, data=reddit, order=newOrder) \n  \n  \n \n  Similarly, we can also plot a distribution of income range.\n   In\u0026nbsp;[51]: ax = sns.countplot(x=\u0026quot;income.range\u0026quot;, data=reddit) locs, labels = plt.xticks() ax = plt.setp(labels, rotation=90) \n  \n  \n \n  One problem with the above plots is that the different levels are not ordered. This can be fixed using ordered Factors, instead of regular factor type variables. Additionally, We need to use a more reasonable x-label for plotting income.range.\n   In\u0026nbsp;[52]: newLevels = [\u0026quot;100K\u0026quot;, \u0026quot;\u0026gt;150K\u0026quot;, \u0026quot;20K\u0026quot;,\u0026quot;30K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;50K\u0026quot;, \u0026quot;70K\u0026quot;, \u0026quot;\u0026lt;20K\u0026quot;] reddit[\u0026quot;income.range\u0026quot;] = reddit[\u0026quot;income.range\u0026quot;].astype(\u0026#39;category\u0026#39;) reddit[\u0026quot;income.range\u0026quot;] = reddit[\u0026quot;income.range\u0026quot;].cat.rename_categories(newLevels) \n  \n In\u0026nbsp;[55]: newOrder = [\u0026quot;\u0026lt;20K\u0026quot;, \u0026quot;20K\u0026quot;,\u0026quot;30K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;50K\u0026quot;, \u0026quot;70K\u0026quot;, \u0026quot;100K\u0026quot;, \u0026quot;\u0026gt;150K\u0026quot;] ax = sns.countplot(x=\u0026quot;income.range\u0026quot;, data=reddit, order=newOrder) \n  \n  \n \n\n","title":"Reddit Survey: Introduction to Pandas","url":"https://sadanand-singh.github.io/posts/pandasintroreddit/"},{"tags":"Programming, Python","text":"Python is a widely used general-purpose, high-level programming language. Due to its focus on readability, wide-spread popularity and existence of a plethora of libraries (also called modules), it is one of the most preferred programming languages for scientists and engineers.\n\nPython for Non-Programmers In this series of python tutorials, I will provide a set of lectures on various basic topics of python. The prime target audience for this series are scientist in non-programming fields like microbiology, genetics, psychology etc. who have some to none programming experience.\nHere is a brief list of topics I will cover per week. I will also post exercises at the end of each session, along with the expected outputs. You should plan to complete these exercises within 5-6 days, before the new tutorial is posted. You will be judging your exercises on your own. The goal should be to match your program\u0026rsquo;s output to the expected output.\n Week 1 : Working with Python on Windows, Concept of Variables \u0026amp; Math Operations, Displaying Outputs Week 2 : User Inputs, Modules, Comments and Basics of Strings Week 3 : More on Strings, Lists and Other Containers Week 4 : Looping/iterating, if/else Conditions Week 5 : Advanced String Operations Week 6 : Regular Expressions and Strings Week 7 : Reading and Writing Files Week 8 : Functions and Writing Scripts Week 9 : Interacting with Operating System Week 10 : Handling and Plotting Data in Python Week 11 : Basic Statistics in Python using Pandas Week 12 : Introduction to BioPython  Week 1. Introduction to Python To start working with any programming language, first thing you need is a working installation of that language. Today, we will go through installation of python on Windows machines.\nTo keep things simple, We will be running our simple programs in google chrome browser, without any need of an installation.\nFor later exercises, from Week 7 onwards, I would highly recommend getting access to a Linux/Mac machine. However, I will also provide doing the same things on Windows machines and Google Chrome as well.\nIn this week\u0026rsquo;s session, I will be assuming you will be using online python in google chrome.\nTo follow these tutorials, please run any code and observe output that you see in code blocks.\nInstalling Python Go to the following website to get access to python.\nYou should get a window like this:\n You can work with this system in two ways: A. Write your code interactively (one command at a time) on the dark screen on the left. Pressing ENTER will show you the output of that particular command.\nLets\u0026rsquo; try our first program:\nIn\u0026nbsp;[1]: print(\u0026quot;My Name is so-and-so\u0026quot;)    \n My Name is so-and-so     \n  The above program will simply output whatever was put under \u0026ldquo;quotes\u0026rdquo;. We will learn more about the print() method (what is a method/function in python?) towards the end of this session.\nVariables\u0026#182;A variable is a symbol or name that stands for a value. For example, in the expression x = 23\nx is a name/symbol whose value is numerical and equal to 23. if we write something like this: y = x+12-5, then y is a new variable whose value should be equal to 23+12-5.\nLet\u0026rsquo;s try these in a program\n   In\u0026nbsp;[2]: x=23 print(\u0026quot;x = \u0026quot;,x) \n  \n x = 23     \n In\u0026nbsp;[3]: y=x+12-5 print(\u0026quot;y =\u0026quot;,y) \n  \n y = 30     \n In\u0026nbsp;[4]: print(23+12-5) \n  \n 30     \n  We ran 3 commands. First we created a variable \u0026lsquo;x\u0026rsquo; with a value of 23. We verified it value by using a print() method! Second, we created another variable called \u0026lsquo;y\u0026rsquo; whose values is equal to mathematical operation of \u0026lsquo;x+12-5\u0026rsquo;. If you remember basic algebra, this is exactly that. Finally, we confirmed that value of \u0026lsquo;y\u0026rsquo; is exactly equal to 23+12-5.\nHopefully, you got a feel of variables. Variables are not limited to just numbers though. We can also store text. For example:\n   In\u0026nbsp;[5]: name = \u0026quot;Sadanand Singh\u0026quot; print(\u0026quot;My Name is:\u0026quot;, name) \n  \n My Name is: Sadanand Singh     \n  Here, we saved my name Sadanand Singh in a variable called \u0026ldquo;name\u0026rdquo;.\nConcept of variable is very fundamental to any programming language. You can think of them as tokens that store some value.\nLets consider this example to understand variable in more detail.\nYou are doing your taxes, all of your calculations depend on your net income. If you do all your calculation in terms of actual value of net income, every time you write the number, your chances of making a mistake increases. Furthermore, suppose, you want to update the income due to some mistake in the beginning, now you have update every place where you used your income.\nLets consider a second case where you declare a variable called \u0026ldquo;income\u0026rdquo;, and store income = 30000. Now, any time you do any calculation with income, you will be using the variable \u0026ldquo;income\u0026rdquo;. In this framework, because you have to type your income just once, chances of making mistakes are least and changing it needs just one change!\n  \nMath Operations Now that we know how to declare and use variables, we will look at first what all we can do with numbers in python. Using simple math operations are extremely easy in Python. Here are all the basic math operations that one can do in Python.\n   Syntax Math Operation Name     a+b $a + b$ Addition   a-b $a - b$ Subtraction   a*b $a \\times b$ Multiplication   a/b $a \\div b$ Division   a**b $a^b$ Power/Exponent   abs(a) $\\lvert a \\rvert$ Absolute Value   -a $-1 \\times a$ Negation   a//b quotient of $a \\div b$ Quotient   a%b Remainder of $a \\div b$ Remainder    Here are some example operation. Please repeat these and observe the use of parenthesis in using the BODMAS principle.\nIn\u0026nbsp;[6]: 3 + 5    \nOut[6]: 8  \n \n In\u0026nbsp;[7]: 2 * 3 + 3.34 + 4 - 45.67 \n  \nOut[7]: -32.33  \n \n In\u0026nbsp;[8]: 12.7 - 10 * 23.5 / 0.5 \n  \nOut[8]: -457.3  \n \n In\u0026nbsp;[9]: 12 - (11 + 34) / 2 \n  \nOut[9]: -10.5  \n \n In\u0026nbsp;[10]: a,b=5,6 \n  \n  Above is a special case of assigning multiple variables together. In the above example we stored a=5 and b=6. Lets confirm these:\n   In\u0026nbsp;[11]: print(\u0026quot;a = \u0026quot;,a) print(\u0026quot;b = \u0026quot;,b) \n  \n a = 5 b = 6     \n In\u0026nbsp;[12]: c = a**b print(\u0026quot; a raised to the power b is:\u0026quot;,c) \n  \n  a raised to the power b is: 15625     \n In\u0026nbsp;[13]: b = -b \n  \n In\u0026nbsp;[14]: print(\u0026quot;b = \u0026quot;,b) \n  \n b = -6     \n  Can you guess what we did here? First we use negation operation to get \u0026ldquo;-b\u0026rdquo; i.e. -6. Then, we we redefined b to be equal to -6! Let consider the following example: a = a-b-3. What do you expect the value to be? Now lets check if you are correct:\n   In\u0026nbsp;[15]: a = a-b-3 print(\u0026quot;new value of a is: \u0026quot;,a) \n  \n new value of a is: 8     \n  What do you expect if we do b = -b again?\n   In\u0026nbsp;[16]: b = -b print(\u0026quot;New Value of b is:\u0026quot;,b) \n  \n New Value of b is: 6     \n In\u0026nbsp;[17]: a//b \n  \nOut[17]: 1  \n \n In\u0026nbsp;[18]: a%b \n  \nOut[18]: 2  \n \n In\u0026nbsp;[19]: b**-2 \n  \nOut[19]: 0.027777777777777776  \n \n  You can perform many other advanced math operations using the \u0026ldquo;math module\u0026rdquo;. To use them, first you will need to import the math module in your code like this:\n   In\u0026nbsp;[20]: import math \n  \n  Then, you use operations like the following:\n   In\u0026nbsp;[21]: c = math.sqrt(25) print(c) c = math.log(10) print(c) c = math.log10(10) print(c) c = math.cos(math.pi) print(c) \n  \n 5.0 2.302585092994046 1.0 -1.0     \n  You can all other available mathematical operation in this module on the python website.\n  \nprint() Function in Python Primary way to see and print information on your screen in python is using a method/function called print(). We will learn more about functions in python later. For today, you can think of functions as a program that \u0026ldquo;does something\u0026rdquo;. For example, the function print(), does the job of printing things on screen.\nNotice the use of () in functions. () separates a function from a variable. For example, \u0026ldquo;foo\u0026rdquo; is a variable, whereas foo() is a function, sometimes also called as method.\nFunctions also have a concept of arguments. Arguments can be thought as inputs to functions. For example, we have function that adds 2 numbers, then this function will need 2 arguments, the two numbers that we want to add. We can denote this function as, addition(a,b).\nSimilarly, functions also have a concept of return values. Return value can be thought as the output of that function. For example, in the above example of addition(a,b) function, sum of two numbers will be the \u0026ldquo;return value\u0026rdquo; of the function. We can write this as, c = addition(a,b). Here, a and b are arguments to function addition() and c is the return value of this function.\nA function can have any number of arguments, zero to any number; where it can have either zero or 1 return values.\nNow, coming back to the print() method, that we have been using throughout this tutorial.\nprint() method can take any number of arguments separated by commas. All it does is to \u0026ldquo;print\u0026rdquo; those on your screen. Lets look at some examples:\nIn\u0026nbsp;[22]: print(3,4) print(\u0026quot;My Name is Sadanand Singh\u0026quot;) print(\u0026quot;My Name is \u0026quot;,\u0026quot;Sadanand Singh\u0026quot;,\u0026quot;and My age is: \u0026quot;, 29)    \n 3 4 My Name is Sadanand Singh My Name is Sadanand Singh and My age is: 29     \n  Now, lets try something fun.\n   In\u0026nbsp;[23]: print(\u0026quot;My Name is\u0026quot;,\u0026quot;Sadanand\u0026quot;, sep=\u0026quot;***\u0026quot;) \n  \n My Name is***Sadanand     \n In\u0026nbsp;[24]: print(\u0026quot;My Name is\u0026quot;,\u0026quot;Sadanand\u0026quot;,\u0026quot;Singh\u0026quot;,\u0026quot;My Age is\u0026quot;,\u0026quot;12\u0026quot;,\u0026quot;\u0026quot;,sep=\u0026quot;***\u0026quot;) \n  \n My Name is***Sadanand***Singh***My Age is***12***     \n  Can you explain what is happening here!\n  \nExcercise We will all things we have learned today using the exercise below.\nWe will follow the tax preparation example:\n Create a variable to store \u0026ldquo;income\u0026rdquo; Create another variable called \u0026ldquo;taxRate\u0026rdquo; which is equal to 1/100000th of \u0026ldquo;income\u0026rdquo;. Net federal tax will be equal to 1.5 times income times taxRate Net state tax will be equal to square root on federal tax Net tax will be federal tax + state tax Total final tax will be Net tax + log to the base of 2 of Net Tax Print following values clearly using print(): income, taxRate, Federal Tax, State Tax, Net Tax and Final Tax. First run with an income of 60000 Repeat with an income of 134675  Your output should look like the following in two cases.\nCase 1: income = 60000\n Total Income is: 60000 Tax Rate is: 0.059999999999999998 Total Federal Tax is: 1800.0 Total State Tax is: 42.426406871192853 Net Tax is: 1842.4264068711927 Total Tax is: 1853.2737981499038  Case 2: income = 134675\n Total Income is: 134675 Tax Rate is: 0.13467499999999999 Total Federal Tax is: 9068.6778125000001 Total State Tax is: 95.229605756298284 Net Tax is: 9163.9074182562981 Total Tax is: 9177.0691654243201  Great! Next week we dive into Python further.\n\n","title":"Python Tutorial - Week 1","url":"https://sadanand-singh.github.io/posts/pythontutweek1/"},{"tags":"Food, Recipe","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule.\nBut, its never too late. Especially, when you can start with some exotic food!\n\nCarrot halwa is a common north Indian dessert, healthy carrots cooked in milk and mango puree.\n  Ingredients  8-10 organic carrots (for a more authentic taste). Condensed milk (medium size) or 1-2 cup Ricotta cheese 1-2 cup whole Milk. Dried nuts (Chopped Almonds, Cashews, Golden raisins and Dates) Sugar/Fresh Mango puree as per taste. 3-4 Tablespoon unsalted butter 3-4 whole cardamom or 1\u0026frasl;2 teaspoon cardamom powder  Preparation  Wash and Peel the carrots and grate them. Heat a non-stick pan on medium flame and add 3-4 tablespoon of butter. When the butter melts, add the grated carrots and start frying. Stir the carrots till all the water from the carrots is lost. Add Ricotta cheese with 1-2 cup whole milk or 1 can of Condensed milk and let the carrots cook for 5-7 minutes on low flame. Continuously stir the carrots so as not to burn them. When 3/4th of the milk is dried, add chopped nuts and Sugar. Turn off the flame after 3-4 minutes, after all the milk is dried. Let the halwa cool down. Garnish it with nuts and dried milk.   Interesting Facts  Nutritional Value of carrots. More about Gajar Halwa ","title":"Carrot Halwa Recipe","url":"https://sadanand-singh.github.io/posts/carrothalwarecipe/"},{"tags":"Algorithms, Python","text":"The world of computers is moving fast. While going through some materials on algorithms, I have come across an interesting discussion -enhancements in hardware (cpu) vis-a-vis algorithms.\nOne side of the coin is the hardware - the speed of computers. The famous Moore\u0026rsquo;s law states that:\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.\nG. Moore, 1965  In simple words, Moore\u0026rsquo;s law is the observation that, over the history of computing hardware, the number of transistors in a dense integrated circuit has doubled approximately every two years. More precisely, the number of transistors in a dense integrated circuit has increased by a factor of 1.6 every two years. More recently, keeping up with this has been challenging. In the context of this discussion, the inherent assumption is that number of transistors is directly proportional to the speed of computers.\nNow, looking at the other side of the coin - speed of algorithms. According to Excerpt from Report to the President and Congress: Designing a Digital Future, December 2010 (page 97):\nEveryone knows Moore’s Law – a prediction made in 1965 by Intel co-­founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years\u0026hellip;. in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed.\nThe gain in computing speed due to algorithms have been simply phenomenal, unprecedented, to say the least! Being actively involved with realization of Moore\u0026rsquo;s law, I have been naturally attracted in the study and design of algorithms.\nTo get a more practical perspective on this, lets look at the problem of finding large Fibonacci numbers. These numbers have been used in wide areas ranging from arts to economics to biology to computer science to the game of poker! The simple definition of these numbers are:\n$$F_{n} = \\begin{cases} F_{n-2} \u0026#43; F_{n-1} \u0026amp; \\text{if } n \u0026gt; 1 \\\\ 1 \u0026amp; \\text{if } n = 1 \\\\ 0 \u0026amp; \\text{if } n = 0 \\end{cases}$$     So, first few Fibonacci numbers are: $0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 \\ldots $ These numbers grow almost as fast as powers of 2: for example, \\(F_{30}\\)    is over a million, and \\(F_{100}\\)    is 21 digits long! In general, \\(F_n \\approx 2^{0.694n}\\)    Clearly, we need a computing device to calculate say \\(F_{200}\\)   .\nHere is a simple plot of first few Fibonacci numbers:\n #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016{-webkit-user-select:none;-webkit-font-smoothing:antialiased;font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text.major{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay text.value{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay text.label{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:64px} #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016{background-color:#f0f0f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 path,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 rect,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 circle{-webkit-transition:250ms ease-in;-moz-transition:250ms ease-in;transition:250ms ease-in}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .graph \u0026gt; .background{fill:#f0f0f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .plot \u0026gt; .background{fill:#f8f8f8}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .graph{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend:hover text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guide.line{stroke:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .major.line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text.major{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .line-graph .axis.x .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .stackedline-graph .axis.x .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .xy-graph .axis.x .guides:hover .guide.line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guides:hover text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .reactive{fill-opacity:.5;stroke-opacity:.8}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .ci{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .reactive.active,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .active .reactive{fill-opacity:.9;stroke-opacity:.9;stroke-width:4}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .ci .reactive.active{stroke-width:1.5}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .series text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip rect{fill:#f8f8f8;stroke:rgba(0,0,0,0.9);-webkit-transition:opacity 250ms ease-in;-moz-transition:opacity 250ms ease-in;transition:opacity 250ms ease-in}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .legend{font-size:.8em;fill:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .x_label{font-size:.6em;fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .xlink{font-size:.5em;text-decoration:underline}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .value{font-size:1.5em}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .bound{font-size:.5em}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .max-value{font-size:.75em;fill:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .map-element{fill:#f8f8f8;stroke:rgba(0,0,0,0.6) !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .map-element .reactive{fill-opacity:inherit;stroke-opacity:inherit}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-0,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-0 a:visited{stroke:#00b2f0;fill:#00b2f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-1,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-1 a:visited{stroke:#43d9be;fill:#43d9be}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-2,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-2 a:visited{stroke:#0662ab;fill:#0662ab}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-0 text{fill:black}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-1 text{fill:black}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-2 text{fill:black} #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .guide.line{fill:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .centered{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x text{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x:not(.web) text[transform]{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x:not(.web) text[transform].backwards{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y text{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y text[transform].backwards{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 text{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 text[transform].backwards{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guide.line{stroke-dasharray:4,4}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .major.guide.line{stroke-dasharray:6,6}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.y .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.y2 .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .vertical .axis.x .guide.line{opacity:0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.always_show .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .vertical .axis.always_show .guide.line{opacity:1 !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x .guides:hover .guide.line{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guides:hover text{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .nofill{fill:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .subtle-fill{fill-opacity:.2}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot{stroke-width:1px;fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot.active{stroke-width:5px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot.negative{fill:transparent}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 tspan{stroke:none !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .series text.active{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip rect{fill-opacity:.95;stroke-width:.5}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip text{fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .showable{visibility:hidden}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .showable.shown{visibility:visible}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .gauge-background{fill:rgba(229,229,229,1);stroke:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .bg-lines{stroke:#f0f0f0;stroke-width:2px} window.pygal = window.pygal || {};window.pygal.config = window.pygal.config || {};window.pygal.config['89be0ddc-40d8-49ce-9109-b3cd2f229016'] = {\"allow_interruptions\": false, \"box_mode\": \"extremes\", \"classes\": [\"pygal-chart\"], \"css\": [\"file://style.css\", \"file://graph.css\"], \"defs\": [], \"disable_xml_declaration\": false, \"dots_size\": 2.5, \"dynamic_print_values\": false, \"explicit_size\": false, \"fill\": false, \"force_uri_protocol\": \"https\", \"formatter\": null, \"half_pie\": false, \"height\": 600, \"include_x_axis\": false, \"inner_radius\": 0, \"interpolate\": null, \"interpolation_parameters\": {}, \"interpolation_precision\": 250, \"inverse_y_axis\": false, \"js\": [\"//kozea.github.io/pygal.js/2.0.x/pygal-tooltips.min.js\"], \"legend_at_bottom\": true, \"legend_at_bottom_columns\": 3, \"legend_box_size\": 12, \"logarithmic\": true, \"margin\": 20, \"margin_bottom\": null, \"margin_left\": null, \"margin_right\": null, \"margin_top\": null, \"max_scale\": 16, \"min_scale\": 4, \"missing_value_fill_truncation\": \"x\", \"no_data_text\": \"No data\", \"no_prefix\": false, \"order_min\": null, \"pretty_print\": false, \"print_labels\": false, \"print_values\": false, \"print_values_position\": \"center\", \"print_zeroes\": true, \"range\": null, \"rounded_bars\": null, \"secondary_range\": null, \"show_dots\": true, \"show_legend\": true, \"show_minor_x_labels\": true, \"show_minor_y_labels\": true, \"show_only_major_dots\": false, \"show_x_guides\": false, \"show_x_labels\": true, \"show_y_guides\": true, \"show_y_labels\": true, \"spacing\": 10, \"stack_from_top\": false, \"strict\": false, \"stroke\": true, \"stroke_style\": null, \"style\": {\"background\": \"#f0f0f0\", \"ci_colors\": [], \"colors\": [\"#00b2f0\", \"#43d9be\", \"#0662ab\", \"#00668a\", \"#98eadb\", \"#97d959\", \"#033861\", \"#ffd541\", \"#7dcf30\", \"#3ecdff\", \"#daaa00\"], \"font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"foreground\": \"rgba(0, 0, 0, 0.9)\", \"foreground_strong\": \"rgba(0, 0, 0, 0.9)\", \"foreground_subtle\": \"rgba(0, 0, 0, 0.6)\", \"guide_stroke_dasharray\": \"4,4\", \"label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"label_font_size\": 10, \"legend_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"legend_font_size\": 14, \"major_guide_stroke_dasharray\": \"6,6\", \"major_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"major_label_font_size\": 10, \"no_data_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"no_data_font_size\": 64, \"opacity\": \".5\", \"opacity_hover\": \".9\", \"plot_background\": \"#f8f8f8\", \"stroke_opacity\": \".8\", \"stroke_opacity_hover\": \".9\", \"title_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"title_font_size\": 16, \"tooltip_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"tooltip_font_size\": 14, \"transition\": \"250ms ease-in\", \"value_background\": \"rgba(229, 229, 229, 1)\", \"value_colors\": [], \"value_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_font_size\": 16, \"value_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_label_font_size\": 10}, \"title\": \"Fibonacci Numbers\", \"tooltip_border_radius\": 0, \"tooltip_fancy_mode\": true, \"truncate_label\": null, \"truncate_legend\": null, \"width\": 800, \"x_label_rotation\": 0, \"x_labels\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"], \"x_labels_major\": null, \"x_labels_major_count\": null, \"x_labels_major_every\": null, \"x_title\": null, \"xrange\": null, \"y_label_rotation\": 0, \"y_labels\": null, \"y_labels_major\": null, \"y_labels_major_count\": null, \"y_labels_major_every\": null, \"y_title\": null, \"zero\": 1, \"legends\": [\"Fibonacci(n)\", \"2\\u207f\", \"n\\u00b2\"]}Fibonacci Numbers 112244668810102020404060608080100100200200400400600600800800100010002000200040004000600060008000800010000100002000020000400004000060000600008000080000100000100000200000200000400000400000600000600000800000800000100000010000001234567891011121314151617181920Fibonacci Numbers113.615384615384615490.01149.445344129554655490.02285.27530364372468465.533121.10526315789473451.168418732331745156.93522267206478433.112761675259658192.76518218623482416.5613228.59514170040487399.3392269055432721264.4251012145749382.3882231419204834300.2550607287449365.3571603893667955336.085020242915348.356687018645861089371.914979757085331.344530941323211144407.7449392712551314.336837464663312233443.5748987854251297.3274394559701413377479.40485829959516280.318692524917714610515.2348178137652263.309696904968915987551.0647773279353246.30079627581827161597586.8947368421052229.2918593634248172584622.7246963562753212.2829363099989184181658.5546558704452195.27400796291863196765694.3846153846154178.265081637834520213.615384615384615465.51449.445344129554655441.02885.27530364372468416.5316121.10526315789473392.0432156.93522267206478367.5564192.76518218623482343.06128228.59514170040487318.57256264.4251012145749294.08512300.2550607287449269.591024336.085020242915244.99999999999997102048371.914979757085220.5114096407.7449392712551196.00000000000006128192443.5748987854251171.500000000000061316384479.40485829959516147.01432768515.2348178137652122.51565536551.064777327935398.016131072586.894736842105273.517262144622.724696356275349.0000000000000618524288658.554655870445224.500000000000057191048576694.3846153846154-5.684341886080802e-1420113.615384615384615490.01449.445344129554655441.02985.27530364372468412.33683746466335316121.10526315789473392.0425156.93522267206478376.2255233505192536192.76518218623482363.33683746466335649228.59514170040487352.4396088191774764264.4251012145749343.0881300.2550607287449334.67367492932679100336.085020242915327.225523350519210121371.914979757085320.4878506867724511144407.7449392712551314.336837464663312169443.5748987854251308.6784538110864613196479.40485829959516303.4396088191774714225515.2348178137652298.562360815182615256551.0647773279353294.016289586.8947368421052289.714320778733417324622.7246963562753285.673674929326718361658.5546558704452281.851551841264419400694.3846153846154278.2255233505192720Fibonacci(n)2ⁿn² The most basic algorithm, that comes to mind is a recursive scheme that taps directly into the above definition of Fibonacci series.\ndef fibRecursive(n): if n == 0: return 0 if n == 1: return 1 return fibRecursive(n-2)\u0026#43;fibRecursive(n-1) \r\rIf you analyze this scheme, this is in fact an exponential algorithm, i.e. fibRecursive( n ) is proportional to \\(2^{0.694n} \\approx (1.6)^n\\)   , so it takes 1.6 times longer to compute \\(F_{n\u0026#43;1}\\)    than \\(F_n\\)   . This is interesting. Recall, under Moore\u0026rsquo;s law, computers get roughly 1.6 times faster every 2 years. So if we can reasonably compute \\(F_{100}\\)    with this year\u0026rsquo;s technology, then only after 2 years we will manage to get \\(F_{101}\\)   ! Only one more Fibonacci number every 2 years!\nLuckily, algorithms have grown at a much faster pace. Let\u0026rsquo;s consider improvements w.r.t to this current problem of finding $n^{th}$ Fibonacci number, $F_n$.\nFirst problem we should realize in the above recursive scheme is that we are recalculating lower $F_n$ at each recursion level. Lets solve this issue by storing each calculation and avoiding any re-calculation!\ndef fibN2(n): a = 0 b = 1 if n == 0: return 0 for i in range(1,n\u0026#43;1): c = a \u0026#43; b a = b b = c return b \r\rOn first glance this looks like an $\\mathcal{O}(n)$ scheme, as we consider each addition as one operation. However, we should realize that as $n$ increases, addition can not be assumed as a single operation, rather every step of addition is an $\\mathcal{O}(n)$ operation, recall first grade Math for adding numbers digit by digit!! Hence, this algorithm is an $\\mathcal{O}(n^2)$ scheme. Can we do better?\nYou bet, we can! Lets consider the following scheme:\n$$\\begin{pmatrix} 1\u0026amp;1 \\\\ 1\u0026amp;0 \\end{pmatrix}^n = \\begin{pmatrix} F_{n\u0026#43;1}\u0026amp;F_n \\\\ F_n\u0026amp;F_{n-1} \\end{pmatrix}$$     We can use a recursive scheme to calculate this matrix power using a divide and conquer scheme in $\\mathcal{O}(\\log{}n)$ time.\ndef mul(A, B): a, b, c = A d, e, f = B return a*d \u0026#43; b*e, a*e \u0026#43; b*f, b*e \u0026#43; c*f def pow(A, n): if n == 1: return A if n \u0026amp; 1 == 0: return pow(mul(A, A), n//2) else: return mul(A, pow(mul(A, A), (n-1)//2)) def fibLogN(n): if n \u0026lt; 2: return n return pow((1,1,0), n-1)[0] \r\rLets think a bit harder about this. Is it really an $\\mathcal{O}(\\log{}n)$ scheme? It involves multiplication of numbers, the method mul(A, B). What happens when $n$ is very large? Sure, this will blow up, as typical multiplication would be an $\\mathcal{O}(n^2)$ operation. So, in fact, our new scheme is $\\mathcal{O}(n^2 \\log{}n)$!\nLuckily, we can solve even large multiplications in $\\mathcal{O}(n^{log_2{3}} \\approx n^{1.585})$, using Karatsuba multiplication, which is again a divide and conquer scheme.\nHere is one simple implementation (Same as the above scheme, but with the following mul(A,B) method):\n_CUTOFF = 1536 def mul(A, B): a, b, c = A d, e, f = B return multiply(a,d) \u0026#43; multiply(b,e), multiply(a,e) \u0026#43; multiply(b,f), multiply(b,e) \u0026#43; multiply(c,f) def multiply(x, y): if x.bit_length() \u0026lt;= _CUTOFF or y.bit_length() \u0026lt;= _CUTOFF: return x * y else: n = max(x.bit_length(), y.bit_length()) half = (n \u0026#43; 32) // 64 * 32 mask = (1 \u0026lt;\u0026lt; half) - 1 xlow = x \u0026amp; mask ylow = y \u0026amp; mask xhigh = x \u0026gt;\u0026gt; half yhigh = y \u0026gt;\u0026gt; half a = multiply(xhigh, yhigh) b = multiply(xlow \u0026#43; xhigh, ylow \u0026#43; yhigh) c = multiply(xlow, ylow) d = b - a - c return (((a \u0026lt;\u0026lt; half) \u0026#43; d) \u0026lt;\u0026lt; half) \u0026#43; c \r\rSo, this final scheme is in $\\mathcal{O}(n^{1.585}\\log{}n)$ time.\nHere is one final way of solving this problem in the same $\\mathcal{O}(n^{1.585}\\log{}n)$ time, but using a somewhat simpler scheme!\nIf we know \\(F_K\\)    and \\(F_{K\u0026#43;1}\\)   , then we can find,\n$$F_{2K} = F_K \\left [ 2F_{K\u0026#43;1}-F_K \\right ]$$     $$F_{2K\u0026#43;1} = {F_{K\u0026#43;1}}^2\u0026#43;{F_K}^2$$     We can implement this using the Karatsuba multiplication as follows:\ndef fibFast(n): if n \u0026lt;= 2: return 1 k = n // 2 a = fibFast(k \u0026#43; 1) b = fibFast(k) if n % 2 == 1: return multiply(a,a) \u0026#43; multiply(b,b) else: return multiply(b,(2*a - b)) \r\rThat\u0026rsquo;s it for today. We saw how far algorithms can go in speed for such simple problems. Let me know in the comments below, if you have any faster or alternate algorithms in mind. Have fun, May zero be with you!\n","title":"Moore's Law and Algorithms - Case of Fibonacci Numbers","url":"https://sadanand-singh.github.io/posts/fibonaccinumbers/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":"In my last post on Arch Installation Guide , We installed the base system and we can now login into our new system as root using the password that we set.\n\n  \nNow, we will proceed further to install the Plasma 5 desktop.\nAdd New User Choose $USERNAME per your liking. I chose ssingh, so in future commands whenever you see ssingh please replace it with your $USERNAME.\n$ useradd -m -G wheel -s /bin/bash $USERNAME $ chfn --full-name \u0026#34;$FULL_NAME\u0026#34; $USERNAME $ passwd $USERNAME \r\rPlasma 5 Desktop Network should be setup at the start. Check the status of network using:\n$ ping google.com -c 2 $ $ PING google.com (10.38.24.84) 56(84) bytes of data. $ 64 bytes from google.com (10.38.24.84): icmp_seq=1 ttl=64 time=0.022 ms $ 64 bytes from google.com (10.38.24.84): icmp_seq=2 ttl=64 time=0.023 ms $ $ --- google.com ping statistics --- $ 2 packets transmitted, 2 received, 0% packet loss, time 999ms $ rtt min/avg/max/mdev = 0.022/0.022/0.023/0.004 ms $ \r\rIf you do not get this output, please follow the troubleshooting links at arch wiki on setting up network.\nI will be assuming you have an NVIDIA card for graphics installation.\nTo setup a graphical desktop, first we need to install some basic X related packages, and some essential packages (including fonts):\n$ pacman -S xorg-server xorg-server-utils nvidia nvidia-libgl \r\rTo avoid the possibility of forgetting to update your initramfs after an nvidia upgrade, you have to use a pacman hook like this:\n$ vim /etc/pacman.d/hooks/nvidia.hook $ ... [Trigger] Operation=Install Operation=Upgrade Operation=Remove Type=Package Target=nvidia [Action] Depends=mkinitcpio When=PostTransaction Exec=/usr/bin/mkinitcpio -p linux ... $ \r\rNvidia has a daemon that is to be run at boot. To start the persistence daemon at boot, enable the nvidia-persistenced.service.\n$ systemctl enable nvidia-persistenced.service $ systemctl start nvidia-persistenced.service \r\r\nKWIN FLICKERING ISSUE To avoid screen tearing in KDE (KWin), add following:\n$ vim /etc/profile.d/kwin.sh $ ... export __GL_YIELD=\u0026#34;USLEEP\u0026#34; ...   If this does not help please try adding the following instead -\n$ vim /etc/profile.d/kwin.sh $ ... export KWIN_TRIPLE_BUFFER=1 ...   Do not have both of the above enabled at the same time. Please look at Arch Wiki for additional details.   Now continue installing remaining important packages for the GUI.\n$ pacman -S mesa ttf-hack ttf-anonymous-pro $ pacman -S tlp tlp-rdw acpi_call bash-completion git meld $ pacman -S ttf-dejavu ttf-freefont ttf-liberation \r\rNow, we will install the packages related to Plasma 5:\n$ pacman -S plasma-meta kf5 kdebase kdeutils kde-applications $ pacman -S kdegraphics gwenview \r\rNow we have to setup a display manager. I chose recommended SDDM for plasma 5.\n$ pacman -S sddm sddm-kcm $ vim /etc/sddm.conf ... [Theme] # Current theme name Current=breeze # Cursor theme CursorTheme=breeze_cursors ... $ systemctl enable sddm \r\rAlso make sure that network manager starts at boot:\n$ systemctl disable dhcpcd.service $ systemctl enable NetworkManager \r\rAudio Setup This is pretty simple. Install following packages and you should be done:\n$ pacman -S alsa-utils pulseaudio pulseaudio-alsa libcanberra-pulse $ pacman -S libcanberra-gstreamer jack2-dbus kmix $ pacman -S mpv mplayer \r\rUseful Tips This part is optional and you can choose as per your taste. Sync time using the systemd service:\n$ vim /etc/systemd/timesyncd.conf $ ... [Time] NTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org FallbackNTP=0.pool.ntp.org 1.pool.ntp.org 0.fr.pool.ntp.org ... $ $ timedatectl set-ntp true $ timedatectl status $ ... Local time: Tue 2016-09-20 16:40:44 PDT Universal time: Tue 2016-09-20 23:40:44 UTC RTC time: Tue 2016-09-20 23:40:44 Time zone: US/Pacific (PDT, -0700) Network time on: yes NTP synchronized: yes RTC in local TZ: no ... $ \r\rOn Plasma 5, It is recommended to enable no-bitmaps to improve the font rendering:\n$ sudo ln -s /etc/fonts/conf.avail/70-no-bitmaps.conf /etc/fonts/conf.d \r\rIf you use vim as your primary editor, you may find this vimrc quite useful.\nThat\u0026rsquo;s It. You are done. Start playing your new beautiful desktop. Please leave your comments with suggestions or any word of appreciation if this has been of any help to you.\nFollow this page for any additional suggestions or improvements in this guide.\n","title":"Plasma 5 Installation on Arch Linux","url":"https://sadanand-singh.github.io/posts/plasmainstall/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":"You must be thinking - yet another installation guide! There is no dearth of Installation guides of Arch on web. So why another one?\nWith advancements like BTRFS file system, UEFI motherboards and modern in-development desktop environment like Plasma 5; traditional Arch Wiki guide and Arch Beginners\u0026rsquo; Guide can only be of a limited help. After I got my new desktop, my goal was to setup it with a modern setup. I decided to go with Arch Linux with btrfs file system and Plasma 5 desktop. Coming from OSX, I just love how far linux has come in terms of looks - quite close to OSX!\n\n For all of you who love installation videos-\n  \nI will cover this in two parts. First in this post, I will install the base system. Then, in a follow up post, I will discuss details of setting up final working Plasma 5 desktop.\nInitial Setup Download the latest iso from Arch website and create the uefi usb installation media. I used my mac to do this on terminal:\n$ diskutil list $ diskutil unmountDisk /dev/disk1 $ dd if=image.iso of=/dev/rdisk1 bs=1m 20480\u0026#43;0 records in 20480\u0026#43;0 records out 167772160 bytes transferred in 220.016918 secs (762542 bytes/sec) $ diskutil eject /dev/disk1 \r\rUse this media to boot into your machine. You should boot into UEFI mode if you have a UEFI motherboard and UEFI mode enabled.\nTo verify you have booted in UEFU mode, run:\n$ efivar -l \r\rThis should give you a list of set UEFI variables. Please look at the Begineers\u0026rsquo; Guide in case you do not get any list of UEFI variables.\nEthernet/Wifi Ethernet should have started by default on your machine. If you do not plan to use wifi during installation, you can skip to the next section. If desired later, wifi will still be configurable after you are done with all the installation.\nTo setup wifi simply run:\n$ wifi-menu \r\rThis is a pretty straight forward tool and will setup wifi for you for this installation session.\nThis will also create a file at /etc/netctl/. We will use this file later to enable wifi at the first session after installation.\nSystem Updates For editing different configurations, I tend to use vim. So we will update our package cache and install vim.\n$ pacman -Syy $ pacman -S vim \r\rHard Drives In my desktop, I have three hard drives, one 256 GB solid state drive (SDD), one 1 TB HDD and another 3TB HDD. I set up my drives as follows: -SDD for root(/), /boot, and /home partitions, 1st HDD for /data and the 2nd HDD for /media partitions.\nFor UEFI machines, we need to use a GPT partition table and /boot partition has to be a fat32 partition with a minimum size of 512 MB. We will format rest other partitions with BTRFS. See this link for benefits of using btrfs partitions.\nFirst list your hard drives with the following:\n$ lsblk $ cat /proc/partitions \r\rAssuming, my setup above, now create gpt partitions and format them.\n$ dd if=/dev/zero of=/dev/sda bs=1M count=5000 $ gdisk /dev/sda Found invalid MBR and corrupt GPT. What do you want to do? (Using the GPT MAY permit recovery of GPT data.) 1 - Use current GPT 2 - Create blank GPT \r\rThen press 2 to create a blank GPT and start fresh\nZAP: $ press x - to go to extended menu $ press z - to zap $ press Y - to confirm $ press Y - to delete MBR \r\rIt might now kick us out of gdisk, so get back into it:\n$ gdisk /dev/sda $ Command (? for help): m $ Command (? for help): n $ Partition number (1-128, default 1): $ First sector (34-500118158, default = 2048) or {\u0026#43;-}size{KMGTP}: $ Last sector (2048-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: 512M $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): ef00 $ Changed type of partition to \u0026#39;EFI System\u0026#39; $ Partition number (2-128, default 2): $ First sector (34-500118, default = 16779264) or {\u0026#43;-}size{KMGTP}: $ Last sector (16779264-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): $ Changed type of partition to \u0026#39;Linux filesystem\u0026#39; $ Command (? for help): p $ Press w to write to disk $ Press Y to confirm \r\rRepeat the above procedure for /dev/sdb and /dev/sdc, but create just one partition with all values as default. At the end we will have three partitions: /dev/sda1, /dev/sda2, /dev/sdb1 and /dev/sdc1\nNow we will format these partitions.\n$ mkfs.vfat -F32 /dev/sda1 $ mkfs.btrfs -L arch /dev/sda2 $ mkfs.btrfs -L data /dev/sdb1 $ mkfs.btrfs -L media /dev/sdc1 \r\rNow, we will create btrfs subvolumes and mount them properly for installation and final setup.\n$ mount /dev/sda2 /mnt $ btrfs subvolume create /mnt/ROOT $ btrfs subvolume create /mnt/home $ umount /mnt $ mount /dev/sdb1 /mnt $ btrfs subvolume create /mnt/data $ umount /mnt $ mount /dev/sdc1 /mnt $ btrfs subvolume create /mnt/media $ umount /mnt \r\rNow, once the sub-volumes have been created, we will mount them in appropriate locations with optimal flags.\n$SSD_MOUNTS=\u0026#34;rw,noatime,nodev,compress=lzo,ssd,discard, space_cache,autodefrag,inode_cache\u0026#34; $ HDD_MOUNTS=\u0026#34;rw,nosuid,nodev,relatime,space_cache\u0026#34; $ EFI_MOUNTS=\u0026#34;rw,noatime,discard,nodev,nosuid,noexec\u0026#34; $ mount -o $SSD_MOUNTS,subvol=ROOT /dev/sda2 /mnt $ mkdir -p /mnt/home $ mkdir -p /mnt/data $ mkdir -p /mnt/media $ mount -o $SSD_MOUNTS,nosuid,subvol=home /dev/sda2 /mnt/home $ mount -o $HDD_MOUNTS,subvol=data /dev/sdb1 /mnt/data $ mount -o $HDD_MOUNTS,subvol=media /dev/sdc1 /mnt/media $ mkdir -p /mnt/boot $ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot \r\rBase Installation Now, we will do the actually installation of base packages.\n$ pacstrap /mnt base base-devel btrfs-progs $ genfstab -U -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab \r\rEdit the /mnt/ect/fstab file to add following /tmp mounts.\ntmpfs /tmp tmpfs rw,nodev,nosuid 0 0 tmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0 \r\rWifi at First Boot Copy our current wifi setup file into the new system. This will enable wifi at first boot. Next, chroot into our newly installed system: $cp /etc/netctl/wl* /mnt/etc/netctl/    Finally bind root for installation.\n$ arch-chroot /mnt /bin/bash \r\rBasic Setup Here are some basic commands you need to run to get the installation started.\n$ pacman -Syy $ pacman -S sudo vim $ vim /etc/locale.gen ... # en_SG ISO-8859-1 en_US.UTF-8 UTF-8 # en_US ISO-8859-1 ... $ locale-gen $ echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf $ export LANG=en_US.UTF-8 $ ls -l /usr/share/zoneinfo $ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime $ hwclock --systohc --utc $ sed -i \u0026#34;s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\u0026#34; /etc/sudoers $ HOSTNAME=euler $ echo $HOSTNAME \u0026gt; /etc/hostname $ pacman -S dosfstools efibootmgr $ sed -i \u0026#39;s/^\\(HOOKS=.*fsck\\)\\(.*$\\)/\\1 btrfs\\2/g\u0026#39; /etc/mkinitcpio.conf $ mkinitcpio -p linux $ passwd \r\rWifi Packages We also need to install following packages for wifi to work at first boot:\n$ pacman -S iw wpa_supplicant     We will also add hostname to our /etc/hosts file:\n$ vim /etc/hosts ... 127.0.0.1 localhost.localdomain localhost $HOSTNAME ::1 localhost.localdomain localhost $HOSTNAME ... \r\rBootloader Setup systemd-boot, previously called gummiboot, is a simple UEFI boot manager which executes configured EFI images. The default entry is selected by a configured pattern (glob) or an on-screen menu. It is included with the systemd, which is installed on an Arch systems by default.\nAssuming /boot is your boot drive, first run the following command to get started:\n$ bootctl --path=/boot install \r\rIt will copy the systemd-boot binary to your EFI System Partition ( /boot/EFI/systemd/systemd-bootx64.efi and /boot/EFI/Boot/BOOTX64.EFI - both of which are identical - on x64 systems ) and add systemd-boot itself as the default EFI application (default boot entry) loaded by the EFI Boot Manager.\nFinally to configure out boot loader, we will need the UUID of out root drive (/dev/sda2). You can find that by:\n$ lsblk -no NAME,UUID /dev/sda2 \r\rNow, make sure that the following two files look as follows, where $UUID is the value obtained from above command:\n$ vim /boot/loader/loader.conf ... timeout 3 default arch ... $ vim /boot/loader/entries/arch.conf ... title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options root=UUID=$UUID rw rootfstype=btrfs rootflags=subvol=ROOT ... \r\rIMPORTANT Please note that you will to need manually run bootctl command every time systemd-boot gets updated.\n$ bootctl update     Network Setup First setup hostname using systemd:\n$ hostnamectl set-hostname $HOSTNAME \r\rCheck the \u0026ldquo;Ethernet controller\u0026rdquo; entry (or similar) from the lspci -v output. It should tell you which kernel module contains the driver for your network device. For example:\n$ lspci -v $ ... 04:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 11) Subsystem: ASUSTeK Computer Inc. Device 859e Flags: bus master, fast devsel, latency 0, IRQ 29 I/O ports at d000 [size=256] Memory at f7100000 (64-bit, non-prefetchable) [size=4K] Memory at f2100000 (64-bit, prefetchable) [size=16K] Capabilities: \u0026lt;access denied\u0026gt; Kernel driver in use: r8169 Kernel modules: r8169 ... $ \r\rNext, check that the driver was loaded via dmesg | grep module_name. For example:\n$ dmesg | grep r8169 $ ... [ 3.215178] r8169 Gigabit Ethernet driver 2.3LK-NAPI loaded [ 3.215185] r8169 0000:04:00.0: can\u0026#39;t disable ASPM; OS doesn\u0026#39;t have ASPM control [ 3.220477] r8169 0000:04:00.0 eth0: RTL8168g/8111g at 0xffffc90000c74000, 78:24:af:d7:1d:3d, XID 0c000800 IRQ 29 [ 3.220481] r8169 0000:04:00.0 eth0: jumbo features [frames: 9200 bytes, tx checksumming: ko] [ 3.226949] r8169 0000:04:00.0 enp4s0: renamed from eth0 [ 5.128713] r8169 0000:04:00.0 enp4s0: link down [ 5.128713] r8169 0000:04:00.0 enp4s0: link down [ 8.110869] r8169 0000:04:00.0 enp4s0: link up ... $ \r\rProceed if the driver was loaded successfully. Otherwise, you will need to know which module is needed for your particular model. Please follow the Arch Wiki Networking guide for further assistance.\nGet current device names via /sys/class/net or ip link. For example:\n$ ls /sys/class/net $ ... enp4s0 lo wlp3s0 ... $ $ ip link $ ... 2: enp4s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 78:24:af:d7:1d:3d brd ff:ff:ff:ff:ff:ff ... $ \r\rUsing this name of the device, we need to configure, enable following two systemd services: systemd-networkd.service and systemd-resolved.service.\nFor compatibility with resolv.conf, delete or rename the existing file and create the following symbolic link:\n$ ln -s /usr/lib/systemd/resolv.conf /etc/resolv.conf \r\rNetwork configurations are stored as *.network in /etc/systemd/network. We need to create ours as follows.:\n$ vim /etc/systemd/network/wired.network $ ... [Match] Name=enp4s0 [Network] DHCP=ipv4 ... $ \r\rNow enable these services:\n$ systemctl enable systemd-resolved.service $ systemctl enable systemd-networkd.service \r\rYour network should be ready for first use!\nFirst Boot Now we are ready for the first boot! Run the following command:\n$ exit $ umount -R /mnt $ reboot \r\rAwesome! We are ready to play with our new system. Alas! what you have is just a basic installation without any GUI.\nPlease see my next post for where to go next!\n","title":"Arch Installation Guide","url":"https://sadanand-singh.github.io/posts/archinstall/"},{"tags":"Algorithms, Puzzles","text":"Here are two math puzzles, solve, comment and enjoy the discussion!\n\n Puzzle 1: Prime Numbers Prove that $p^2-1$ is divisible by 24, where $p$ is a prime number with $p\u0026gt;3$.\nThis is a simple one - do not go by the technicality of the problem statement.\nSolution $p^2-1 = (p-1)\\times (p+1)$ Given, $p$ is a prime number $\u0026gt;3$, $p-1$ and $p+1$ are even. We can also write,\n$$p-1=2K, \\text{and } p+1=2K+2=2(K+1)$$\nGiven, $K \\in \\mathbb{N}$, either $K$ or $K+1$ are also even. Hence, $(p-1)\\times (p+1)$ is divisible by $2\\times 4 = 8$.\nFurthermore, as $p$ is prime, we can write it as either $p = 3s+1$ or $p = 3s-1$, where $s \\in \\mathbb{N}$. In either case, one of $p-1$ or $p+1$ are divisible by 3 as well.\nHence, $p^2-1$ is divisible by $8\\times 3 = 24$.\nPuzzle 2: Hopping on a Chess Board On a chess board, basically a $8\\times 8$ grid, how many ways one can go from the left most bottom corner to the right most upper corner? In chess naming conventions, from $a1$ to $h8$.\nNOTE In the original post this problem was ill defined.\nPlease solve this problem with the constraints that only up and right moves are allowed.    Can you find the generic answer for the case of $N\\times M$ grid?\nSolution Correction For an $N\\times M$ grid, we need only $N-1$ right and $M-1$ up moves.\nThank you Devin for pointing this out.   Given only forward moves are allowed, for any arbitrary grid of $N\\times M$, a total of $(N-1) + (M-1)$ moves are needed.\nAny $N-1$ of these moves can be of type right and $M-1$ of type up. In short, this is a combinatorial problem of distributing $N+M-2$ objects into groups of $N-1$ and $M-1$. This is simply,\n$$\\dbinom{N+M-2}{N-1} = \\frac{(N+M-2)!}{(N-1)! (M-1)!}$$\nIn the particular case of the chess board, $N = M = 8$. Hence, total number of possible paths are:\n$$\\text{No. of Paths} = \\frac{14!}{7! 7!} =3432$$\nThank you Rohit and Amber for posting quick solutions!\nLet me know if you have any other interesting alternative solutions to these problems.\n","title":"Two Simple Math Puzzles","url":"https://sadanand-singh.github.io/posts/primenumberandpath/"},{"tags":"Food, Recipe","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule.\nYes, the silence is finally over, and what\u0026rsquo;s better than giving your taste buds some rejuvenation after some long busy weeks at work. Continuing with my healthy but tasty choices, today I tried to cook Palak (Spinach) with Paneer.\n\nPalak Paneer is a common north Indian cuisine, Indian cottage cheese cooked in spinach puree. Its a bit involved than my last few dishes. Nevertheless, I promise you - the \u0026ldquo;yummy-ness\u0026rdquo; of this one is worth all the trouble!\n  Ingredients  1 lb Paneer (Indian Cottage Cheese) 1 Bunch Fresh Spinach Leaves 1\u0026frasl;2 Medium Onion Finely Cut 1\u0026frasl;2 inch Ginger Root Grated 1 Medium Tomato Finely Chopped 1 Clove of Garlic Minced 1 Medium Bay Leaf 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;4 tbsp Coriander Powder 1 tbsp Cumin Seeds 1 tbsp Garam Masala 2 pinch Asafoetida Heeng 2 tbsp Olive Oil Salt to taste 1 cup Milk or Cream   Preparation Spinach Puree  Wash the leaves thoroughly in running water. In a deep pot, bring about 6 cup of water to boiling temperature. Add a pinch of salt and 2 pinch asafoetida (Heeng) into the boiling water. Add Spinach leaves in boiling water and let it cook for 3 minutes. Strain boiled spinach leaves under cold water. In a blender, make a puree of leaves along with ginger, garlic and milk or cream.   Curry for Paneer  Cut Paneer into small cubes. Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add cumin seeds, bay leaf and minced garlic. Once cumin seeds start to pop, add finely cut onions. Fry the onions until oil starts to separate (about 1-2 minutes). Add finely chopped tomatoes and all the spices except Garam Masala. Once oil start to separate out, add the spinach puree. Stir well while cooking. Add salt and Garam Masala after about 2 minutes. After about another 2 minutes add Paneer cubes. Cook for another 3-4 minutes on low heat.  Let it cool before serving. It can be served with Indian Breads or rice.\n Interesting Facts  Nutritional Value of Paneer. More about Paneer Nutritional Value of Spinach. ","title":"Palak Paneer Recipe","url":"https://sadanand-singh.github.io/posts/palakpaneerrecipe/"},{"tags":"Food, Recipe","text":"Typically, hunger and laziness come to me as inseparable couples. To make things worse, I have been trying to eat healthy.\nNevertheless, here is another recipe that solves all these at once. Its my five minute Tilapia Fish Recipe.\nTwo major ingredients - fish and veggies, of this recipe can be grabbed from the frozen section of any supermarket.\n\n  Ingredients  2 frozen Boneless Tilapia Fish Pieces 1 Packet Frozen Steamed Veggies 2 tbsp Cooking Oil 1\u0026frasl;2 tbsp Parsley 1 tbsp Gamram Masala 1 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1 tbsp Red Chili Powder 1\u0026frasl;2 Lime Salt to Taste  Preparation  Thaw and wash fish pieces carefully. In a skillet, put a tbsp of oil and add frozen veggies to it. Add 1\u0026frasl;2 of all the spices and salt to it. Fry for about 3-5 minutes and keep it aside on a plate. In the same skillet, add rest of the oil. After about a minute, add fish pieces on medium heat. Sprinkle salt and half of the remaining spices on the top of the fish. After about 3 minutes, turn the fish and sprinkle rest of the spices. Let the fish cook slowly. Sprinkle some lime juice on the fish. Once the fish is of mustard color, put fish pieces on veggies.  Serve and garnish it with lime juice and some cilantro for an exotic lunch/dinner.\n Interesting Facts  More about Tilapia Fish Nutritional Value of Tilapia Fish ","title":"Tilapia Fish Recipe","url":"https://sadanand-singh.github.io/posts/tilapiafish/"},{"tags":"Food, Recipe","text":"Today, I share one of my favorite dishes - Aloo Paratha. It is a dish of mashed potato stuffed bread from the Northern India. It is my favorite breakfast dish when I am in India.\nHere is my try in making some edible parathas. It might take few trials in making a perfectly round-shaped Paratha. Please share your views and inputs in comments below.\n\n  Ingredients For Stuffing  2 medium Potatoes 2 tbsp Cooking Oil 1\u0026frasl;2 tbsp Coriander Powder 1\u0026frasl;2 tbsp Gamram Masala 1\u0026frasl;2 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1 tbsp Red Chili Flakes 1\u0026frasl;2 Medium Onion Salt to Taste   For Bread  4 cups of Whole Wheat Flour 4-5 tbsp Butter or Ghee 1 tbsp Black Indian Onion Seeds (Kalaunji) 1 tbsp Ajwain (Carom Seeds) Salt to taste  Preparation Stuffing  Boil and peel Potatoes. Mash boiled Potatoes and mix salt and chili flakes. In a skillet heat oil and add Garam Masala. Once it start to sputter, add cut onions. Add rest of spices and fry until golden brown. Add mashed Potatoes mix and fry for another 2-3 minutes.  Bread (Paratha)  Knead flour with Ajwain, Kalaunji and salt. Make small balls Make craters in the balls and fill them with the stuffing. Roll these into flat circular breads, typical Indian Bread shape. Heat these slowly on a skillet on each side. Once almost cooked, apply some butter or Ghee.  Let it cool before serving. It can be served with Indian spicy pickle and yogurt (Dahi).\n Interesting Facts  More about Aloo Paratha More about Ajwain ","title":"Aloo Paratha Recipe","url":"https://sadanand-singh.github.io/posts/alooparatha/"},{"tags":"Food, Recipe","text":"I am quite found of Paneer. However, cooking it can be a hassle.\nYou can get paneer generally at any Indian grocery store. For enthusiasts, Here is a recipe for making Paneer from milk.\nHere is a version of recipe that I use quite often. It involves two parts - Baking Paneer and Cooking the veggies.\n\n  Ingredients For Baking Paneer  400 g Paneer 1 tbsp Coriander Powder 1 tbsp Gamram Masala 1\u0026frasl;2 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1\u0026frasl;4 cup Milk or Yogurt Salt to Taste  For Veggies  500 g Cut Cauliflowers 1 cup Peas 1\u0026frasl;2 Medium Onion 1 Medium Tomato 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste  Preparation Bake Paneer  Thaw Paneer and cut in cubes. Preheat the oven at 400 F Mix Paneer with all the spices and milk/yogurt. Place cubes in a baking sheet and bake it at 400F for about 30 minutes until Paneer is well cooked.  Prepare Veggies  Cut onion, tomatoes and cauliflowers. Heat oil in the pan and add cumin seeds. Once Seeds start to pop, add cut onions. Fry the onions until oil starts to separate. (about 1-2 minutes) Add spices, peas and tomatoes. (about 1-2 minutes) After about a minute, add cauliflowers. After another minute or so, add salt, and keep stirring occasionally. Once the cauliflowers are almost cooked, add baked Paneer pieces (about 3-5 minutes). Add a little amount of water and cook for about a minute. (optional) Garnish with cilantro leaves.  Let it cool before serving. It can be served either Indian breads or rice.\n Interesting Facts  Nutritional Value of Paneer. Baked Paneer in this recipe is also referred as Paneer Tikka More about Paneer ","title":"Mix-Veg Paneer Recipe","url":"https://sadanand-singh.github.io/posts/mixedvegpaneer/"},{"tags":"Linux","text":"It has been long due. Just built a new desktop. Here are different parts I used to build this beauty. (Updated Cost)\n\n i7 4790 3.6 GHz (Haswell) - Can\u0026rsquo;t Reveal - (Market Price $300) Asus Micro MATX B85-G R2.0 Intel LGA1150 - $50 ADATA XPG V1.0 DDR3 1866 2x4 GB RAM - $50 OCZ Vertex 460A Series 2.5\u0026rdquo; 240 GB - $60 WD Blue 1TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache - $45 Ultra LSP V2 650 Watt PSU - $30 Ultra XBlaster Pro U12-42350 Case - $15 Asus BW-12B1ST/BLK/G/AS Blue Ray Burner - $20 Das Keyboard V4 Evoluent Ergo Right Mouse  Total Value - $580 (Excluding Keyboard/Mouse)\nIt is up and running Arch Linux with BTFRS file system, Plasma 5 Desktop and so on. Over the next few posts, I will be posting my installation ordeals.\nDo not forget to add your comments below. Please add your alternative solutions to anything I did.\n","title":"My New Desktop","url":"https://sadanand-singh.github.io/posts/mynewcompspecs/"},{"tags":"Food, Recipe","text":"This is for all the lazy souls like me - in a mood to eat something tasty, but in no mood to cook for long.\nVermicelli, or also known as seviyan in Hindi, is commonly cooked as a sweet dish in Indian subcontinent. As I try to be away from all things sweet, I came across this recipe which uses this in a quite spicy flavor.\n\nSo, here is my super quick and tasty recipe for these noodles. It takes less than 12 minutes ( 4 Minutes Preparation + 8 Minutes Cooking).\n  Ingredients  300 g Vermicelli Seviyan 1 cup Peas 1\u0026frasl;2 Medium Onion 1 Medium Tomato 1 Clove of Garlic 4 Curry Leaves 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste 1\u0026frasl;2 cup Peanuts optional  Preparation  Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add garlic and cut onions. Fry the onions until oil starts to separate (about 1-2 minutes) Add spices, curry leaves, peas and tomatoes. After about a minute, add noodles. After another minute or so, add salt, and keep stirring occasionally. Once the noodles have become light golden brown in color, add 1\u0026frasl;2 cup of water (about 2-3 minutes). Keep stirring, so that noodles do not stick with each other, for about another 2 minutes  Let it cool before serving. You can use some ketchup or sauce with it. I like adding peanuts to it while frying onions.\n Interesting Facts  The sweet dish seviyan made out of this is also known as shemai in Bengali, sev in Gujarati, shavige in Kannada, sevalu or semiya in Telugu, and semiya in Tamil and Malayalam. The recipe above is also commonly known as upma in various parts of India. More about Vermicelli ","title":"Indian Vermicelli Recipe","url":"https://sadanand-singh.github.io/posts/desinoodlesrecipe/"},{"tags":"Algorithms, Python","text":"Given an alphanumeric string, find the shortest substring that occurs exactly once as a (contiguous) substring in it. Overlapping occurrences are counted as distinct. If there are several candidates of the same length, you must output all of them in the order of occurrence. The space is NOT considered as a valid non-repeating substring.\n\nExample Consider the following cases:\nCase 1: If the given string is asdfsasa, the answer should be ['d', 'f']\nCase 2: If the given string is sadanands,\nthe answer should be ['sa', 'ad', 'da', 'na', 'nd', 'ds']\nCase 3: If the given string is wwwwwwww, the answer should be ['wwwwwwww']  My Solution Here is my solution in Python.\nIt is quite brute force. I am not sure about the order of find() and rfind() built-in methods in Python. Assuming these are $O(n)$, my algorithm is in $O(n^3)$. Please put your answers in comments below, if your answer has a better scaling.\nThe function definition that I use for finding non-empty non-repeating strings is recursive.\ndef findNsubString(s,n): subS = [] for index in range(len(s)\u0026#43;1) : x = s[index:index\u0026#43;n] if s.find(x)==s.rfind(x) : subS.append(x) if subS : return subS else : return findNsubString(s,n\u0026#43;1) \r\rI call this method as follows to get the desired results:\n#! /usr/bin/python import argparse # Parse Command Line Arguments parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--string\u0026#34;, default = \u0026#34;asda\u0026#34;, help=\u0026#34;Input\u0026#34;) args = parser.parse_args() s = args.string # Call Method to find smallest non-repeating sub-string ans = findNsubString(s,1) print(ans) \r\rA similar solution can also be written in JAVA or C++. The corresponding find() and rfind() methods in JAVA are called indexOf() and lastIndexOf(), respectively. In C++, these methods are called same as in Python.\nPlease feel free to put your method definition in any programming language.\n","title":"Shortest Non-repeating Substring","url":"https://sadanand-singh.github.io/posts/shortestsubstring/"},{"tags":"Food, Recipe","text":"I have started eating a lot of greens these days. As a kid, I always loved a vegetable made by my mom, which was made of red leaves, called \u0026ldquo;Laal Saag\u0026rdquo; in Hindi. I could never find what exactly was the English/American name for those leaves.\nRecently, at one of the Korean grocery stores, I came across red Amaranth leaves. Those looked strikingly similar to the red leaves that I used to have back home in India.\n\nSo, here is my super fast and healthy recipe for these red leaves. It took me just 10 minutes ( 5 Minutes Preparation + 5 Minutes Cooking).\n  Ingredients  1 Bunch Red Amaranth Leaves 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Garam Masala 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste  Preparation  Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add cut leaves into the pan. Add salt, close the lid and keep stirring occasionally. Once the leaves have lost water, add all the spices and keep frying until things are dry (about 2-3 minutes).  You can serve this with any Indian bread, or with rice and lentils (Daal).\n Interesting Facts  Amaranth is also known as \u0026ldquo;Chauli\u0026rdquo; or \u0026ldquo;Chavli\u0026rdquo; in Hindi. Nutritional Value of Amaranth History of Amaranth ","title":"Lal Saag Recipe","url":"https://sadanand-singh.github.io/posts/laalsaagrecipe/"},{"tags":"Puzzles, Algebra","text":"Here is another puzzle starring a monkey, transportation and money! Short summary - avoid dealing with fools!\n\nProblem Statement\nThe owner of an apple plantation has a monkey. He wants to transport his 10000 apples to the market, which is located after the forest. The distance between his apple plantation and the market is about 1000 kilometer. So he decided to take his monkey to carry the apples. The monkey can carry at the maximum of 2000 apples at a time, and it eats one apple and throws another one for every kilometer it travels.\nWhat is the largest number of apples that can be delivered to the market?\nPlease give your solutions in the comments below.\nSolution\nIf the owner lets the monkey carry apples all the way to the end of the forest, in the first trip itself, all of 2000 apples will be lost and the monkey will never return back, as the monkey will be out of any food and play.\nLets approach this in a different way. Lets break the monkey\u0026rsquo;s journey by per unit distance. To carry 10000 apples for 1 km, monkey has to make 2 x 5 - 1 = 9 trips! On each trip, the owner will loose 2 apples. Hence, for each km distance traveled until number of apples is greater than 8000, monkey requires 9 x 2 = 18 apples. So, distance traveled by monkey until number of apples is less than 8000 is int(2000 / 18) + 1 = 112 km, and by this time, 10,000 - 112 x 18 = 7984 apples are left. Similarly, until 6000 apples are left, the monkey will require 2 x 4 -1 = 7 trips for every km. Hence, the total distance traveled until no. of apples left is less than 6000 is 112 + int(2000 / 14) + 1 = 255 km, and by this time no. of apples left is 7984 - 143 x 14 = 5982. Now proceeding in a similar manner, until 4000 apples are left, the monkey will require 5 trips for each km. Total distance traveled until no. of apples left is less than 4000 is 255 + int(2000\u0026frasl;10) = 455 km. By this time, number of apples left is 5982 - 200 x 10 = 3982. Until 2000 apples are left, the monkey will require 3 trips per km traveled. Total distance traveled till this time is 455 + int(2000\u0026frasl;6) + 1 = 789 km. By this time, no. of apples left is 3982 - 334 x 6 = 1978. Below 2000 apples, the monkey will require only one trip to reach to market. Distance left is 1000 - 789 = 211 km. Number of apples required by the monkey to travel this distance is 211 x 2 = 422. Hence, total number of apples that can reach the market is just 1978 - 422 = 1556! That\u0026rsquo;s just 15.56% of all apples.\nThat\u0026rsquo;s quite bad monkey 🙈. Moral of the story is, avoid dealing with fools 😛\nThank you all who tried this.\nDisclaimer: This puzzle has been inspired by a problem at the Blog on Technical Interviews.\n","title":"Puzzle 2","url":"https://sadanand-singh.github.io/posts/consumetransportproblem/"},{"tags":"Puzzles, Algebra","text":"As promised in the intro post, here is the first puzzle!\n\nProblem Statement\nA man needs to go through a train tunnel to reach the other side. He starts running through the tunnel in an effort to reach his destination as soon as possible. When he is 1/4th of the way through the tunnel, he hears the train whistle behind him. Assuming the tunnel is not big enough for him and the train, he has to get out of the tunnel in order to survive. We know that the following conditions are true.\n If he runs back, he will make it out of the tunnel by a whisker. If he continues running forward, he will still make it out through the other end by a whisker.  What is the speed of the train compared to that of the man?\nPlease give your solutions in the comments below.\nSolution\nIf the man decided to go back, he would have met the train at the entrance of tunnel (i.e The man would have traveled 0.25 of the tunnel). Instead, if man went forward- by the time train would have reached at the entrance of tunnel, the man would have been at the 0.25+0.25 = 0.5 of tunnel. Hence, according to the second condition, the time taken by man to travel half of tunnel is same as the time taken by the train to travel all of the tunnel. Therefore, the train is traveling at twice the speed of the man.\nDisclaimer: This puzzle has been copied from a Blog on Technical Interviews.\n","title":"Puzzle 1","url":"https://sadanand-singh.github.io/posts/trainspeedproblem/"},{"tags":"Introduction","text":"This is Sadanand Singh. I am a process engineer, a physicist, a programmer, an Indian and a human being; with interests in world politics, economics, and society.\n\nTweets by sadanandsingh          This space is for my personal notes on different subjects. I plan to share my thoughts on following topics from time to time.\n Technology Statistics Machine Learning News Economics Education Politics, Society \u0026amp; Education Indian Food Puzzles  If you have interests in any of these topics, you are welcome to have a peek into my world. Please drop me your views through comments or any of the social media links.\nHAPPY WEB SURFING!!!\n","title":"Welcome","url":"https://sadanand-singh.github.io/posts/firstpost/"}]}
