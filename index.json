{"pages":[{"tags":"Deep Learning, Machine Learning, Python, Data Science","text":"Usually in a conventional neural network, one tries to predict a target vector $y$ from input vectors $x$. In an auto-encoder network, one tries to predict $x$ from $x$. It is trivial to learn a mapping from $x$ to $x$ if the network has no constraints, but if the network is constrained the learning process becomes more interesting. In this article, we are going to take a detailed look at the mathematics of different types of autoencoders (with different constraints) along with a sample implementation of it using Keras, with a tensorflow back-end.\n\n Basic Autoencoders  The simplest AutoEncoder (AE) has an MLP-like (Multi Layer Perceptron) structure:\n Input Layer Hidden Layer, and Output Layer  However, unlike MLP, autoencoders do not require any target data. As the network is trying to learn $x$ itself, the learning algorithm is a special case of unsupervised learning.\nMathematically, lets define:\n Input vector: $x \\in \\Big[ 0, 1 \\Big]^d$ Activation function: $a(h)$ applied to very nuron of layer $h$ \\(W_i \\in \\mathbb{R}^{I_{di} \\times O_{di}}\\)   , the parameter matrix of $i$-th layer, projecting a\\(I_{di}\\)    dimensional input in a \\(O_{di}\\)   dimensional space \\(b_i \\in \\mathbb{R}^{O_{di}}\\)    bias vector  The simplest AE can then be summarized as: $$\\begin{aligned} z \u0026amp;= a(x W_1 \u0026#43; b_1) \\\\ x\u0026#39; \u0026amp;= a(z W_2 \u0026#43; b_2) \\end{aligned}$$    \nThe AE model tries to minimize the reconstruction error between the input value $x$ and the reconstructed value \\(x\u0026#39;\\)   . A typical definition of the reconstruction error is the $L_p$ distance (like $L_2$ norm) between the $x$ and \\(x\u0026#39;\\)    vectors: $$\\min \\mathcal{L} = \\min E(x, x\u0026#39;) \\stackrel{e.g.}{=} \\min || x - x\u0026#39; ||_p$$    \nAnother common variant of loss function (especially images) for AE is the cross entropy function. $$\\mathcal{L}(x, x\u0026#39;) = -\\sum_{c=1}^{M} x\u0026#39;_c \\log (x_c)$$    \nwhere $M$ is the dimensionality of the input data $x$ (for eg. no. of pixels in an image).\nAutoencoders in Practice The above example of auto-encoder is too simplistic for any real use case. It can be easily noticed that if the number of units in the hidden layer is greater than or equal to the number of input units, the network will learn the identity function easily. Hence, the simplest constraint used in real-life autoencoders is the number of hidden units ($z$) should be less than the dimensions ($d$) of the input ($z \u0026lt; d$).\nBy limiting the amount of information that can flow through the network, AE model can learn the most important attributes of the input data and how to best reconstruct the original input from an \u0026ldquo;encoded\u0026rdquo; state. Ideally, this encoding will learn and describe latent attributes of the input data. Dimensionality reduction using AEs leads to better results than classical dimensionality reduction techniques such as PCA due to the non-linearities and the type of constraints applied.\nPCA and Autoencoders If we were to construct a linear network (i.e. without the use of nonlinear activation functions at each layer) we would observe a similar dimensionality reduction as observed in PCA. See Geoffrey Hinton\u0026rsquo;s discussion.  A practical auto-encoder network consists of an encoding function (encoder), and a decoding function (decoder). Following is an example architecture for the reconstruction of images.\n In this article we will build different types of autoencoders for the fashion MNIST dataset. In stead of using more common MNIST dataset, I prefer to use fashion MNIST dataset for the reasons described here.\nFor example using MNIST data, please have a look at the article by Francois Chollet, the creator of Keras. The code below is heavily adapted from his article.\nWe\u0026rsquo;ll start simple, with a single fully-connected neural layer as encoder and as decoder. from keras.layers import Input, Dense from keras.models import Model import numpy as np # size of bottleneck latent space encoding_dim = 32 # input placeholder input_img = Input(shape=(784,)) # encoded representation encoded = Dense(encoding_dim, activation=\u0026#39;relu\u0026#39;)(input_img) # lossy reconstruction decoded = Dense(784, activation=\u0026#39;sigmoid\u0026#39;)(encoded) # full AE model: map an input to its reconstruction autoencoder = Model(input_img, decoded)  \nWe will also create separate encoding and decoding functions, that can be used to extract latent features at test time.\n# encoder: map an input to its encoded representation encoder = Model(input_img, encoded) # placeholder for an encoded input encoded_input = Input(shape=(encoding_dim,)) # last layer of the autoencoder model decoder_layer = autoencoder.layers[-1] # decoder decoder = Model(encoded_input, decoder_layer(encoded_input))   We can now set the optimizer and the loss function before training the auto-encoder model. autoencoder.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;)  \nNext, we need to get the fashion MNIST data and normalize it for training. Furthermore, we will flatten the $28\\times28$ images to a vector of size 784. Please note that running the code below for the first time will download the full dataset and hence might take few minutes.\nfrom keras.datasets import fashion_mnist (x_train, _), (x_test, _) = fashion_mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) print(x_train.shape, x_test.shape)   Output: (60000, 784) (10000, 784)\nWe can now train our model for 100 epochs: history = autoencoder.fit(x_train, x_train, epochs=100, batch_size=256, shuffle=True, validation_data=(x_test, x_test))  \nThis will print per epoch training and validation loss. But we can plot the loss history during training using the history object. import matplotlib.pyplot as plt %matplotlib inline def plot_train_history_loss(history): # summarize history for loss plt.plot(history.history[\u0026#39;loss\u0026#39;]) plt.plot(history.history[\u0026#39;val_loss\u0026#39;]) plt.title(\u0026#39;model loss\u0026#39;) plt.ylabel(\u0026#39;loss\u0026#39;) plt.xlabel(\u0026#39;epoch\u0026#39;) plt.legend([\u0026#39;train\u0026#39;, \u0026#39;test\u0026#39;], loc=\u0026#39;upper right\u0026#39;) plt.show() plot_train_history_loss(history)   Output:  After 100 epochs, the auto-encoder reaches a stable train/text loss value of about 0.282. Let us look visually how good of reconstruction this simple model does! # encode and decode some images from test set encoded_imgs = encoder.predict(x_test) decoded_imgs = decoder.predict(encoded_imgs) def display_reconstructed(x_test, decoded_imgs, n=10): plt.figure(figsize=(20, 4)) for i in range(n): # display original ax = plt.subplot(2, n, i \u0026#43; 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if decoded_imgs is not None: # display reconstruction ax = plt.subplot(2, n, i \u0026#43; 1 \u0026#43; n) plt.imshow(decoded_imgs[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() display_reconstructed(x_test, decoded_imgs, 10)  \nThe top row is the original image, while bottom row is the reconstructed image. We can see that we are loosing a lot of fine details.\n Sparsity Constraint We can add an additional constraint to the above AE model, a sparsity constraints on the latent variables. Mathematically, this is achieved by adding a sparsity penalty $\\Omega(\\mathbf{h})$ on the bottleneck layer $\\mathbf{h}$. $$\\min \\mathcal{L} = \\min E(x, x\u0026#39;) \u0026#43; \\Omega(h)$$     where, $\\mathbf{h}$ is the encoder output.\nSparsity is a desired characteristic for an auto-encoder, because it allows to use a greater number of hidden units (even more than the input ones) and therefore gives the network the ability of learning different connections and extract different features (w.r.t. the features extracted with the only constraint on the number of hidden units). Moreover, sparsity can be used together with the constraint on the number of hidden units: an optimization process of the combination of these hyper-parameters is required to achieve better performance.\nIn Keras, sparsity constraint can be achieved by adding an activity_regularizer to our Dense layer: from keras import regularizers encoding_dim = 32 input_img = Input(shape=(784,)) # add a Dense layer with a L1 activity regularizer encoded = Dense(encoding_dim, activation=\u0026#39;relu\u0026#39;, activity_regularizer=regularizers.l1(1e-8))(input_img) decoded = Dense(784, activation=\u0026#39;sigmoid\u0026#39;)(encoded) autoencoder = Model(input_img, decoded)  \nSimilar to the previous model, we can train this as well for 150 epochs. Using a regularizer is less likely to overfit and hence can be trained for longer. autoencoder.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;) history = autoencoder.fit(x_train, x_train, epochs=150, batch_size=256, shuffle=True, validation_data=(x_test, x_test)) plot_train_history_loss(history)  \nWe get a very similar loss as the previous example. Here is a plot of loss values during training.  As expected, the reconstructed images too look quite similar as before. decoded_imgs = autoencoder.predict(x_test) display_reconstructed(x_test, decoded_imgs, 10)  \n Deep Autoencoders We have been using only single layers for encoders and decoders. Given we have large enough data, there is nothing that stops us from building deeper networks for encoders and decoders.\ninput_img = Input(shape=(784,)) encoded = Dense(128, activation=\u0026#39;relu\u0026#39;)(input_img) encoded = Dense(64, activation=\u0026#39;relu\u0026#39;)(encoded) encoded = Dense(32, activation=\u0026#39;relu\u0026#39;)(encoded) decoded = Dense(64, activation=\u0026#39;relu\u0026#39;)(encoded) decoded = Dense(128, activation=\u0026#39;relu\u0026#39;)(decoded) decoded = Dense(784, activation=\u0026#39;sigmoid\u0026#39;)(decoded) autoencoder = Model(input_img, decoded)   We can train this model, same as before. autoencoder.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;) history = autoencoder.fit(x_train, x_train, epochs=150, batch_size=256, shuffle=True, validation_data=(x_test, x_test)) plot_train_history_loss(history)  \n The average loss is now 0.277, as compared to ~0.285 before! We can also see that visually all reconstructed images too look slightly better.\ndecoded_imgs = autoencoder.predict(x_test) display_reconstructed(x_test, decoded_imgs, 10)    Convolutional Autoencoders Since our inputs are images, it makes sense to use convolution neural networks (conv-nets) as encoders and decoders. In practical settings, autoencoders applied to images are always convolution autoencoders \u0026ndash;they simply perform much better.\nThe encoder will consist of a stack of Conv2D and MaxPooling2D layers (max pooling being used for spatial down-sampling), while the decoder will consist of a stack of Conv2D and UpSampling2D layers. We will also be using BatchNormalization. One major difference between this network and prior ones is that now we have 256 (4x4x16) elements in the bottleneck layer as opposed to just 32 before!\nYou can read more about convolution-based auto-encoders in further details here.\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, BatchNormalization from keras.models import Model from keras import backend as K input_img = Input(shape=(28, 28, 1)) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(input_img) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(16, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(16, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) encoded = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(16, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(encoded) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) x = Conv2D(16, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;valid\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) decoded = Conv2D(1, (3, 3), activation=\u0026#39;sigmoid\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) autoencoder = Model(input_img, decoded) autoencoder.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;)   To train it, we will use the original fashion MNIST digits with shape (samples, 1, 28, 28), and we will just normalize pixel values between 0 and 1. (x_train, _), (x_test, _) = fashion_mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  \nSimilar to before, we can train this model for 150 epochs. However, unlike before, we will checkpoint the model during training to save the best model, based on the validation loss minima.\nfrom keras.callbacks import ModelCheckpoint fpath = \u0026#34;weights-ae-{epoch:02d}-{val_loss:.3f}.hdf5\u0026#34; callbacks = [ModelCheckpoint(fpath, monitor=\u0026#39;val_loss\u0026#39;, verbose=1, save_best_only=True, mode=\u0026#39;min\u0026#39;)] history = autoencoder.fit(x_train, x_train, epochs=150, batch_size=256, shuffle=True, validation_data=(x_test, x_test), callbacks=callbacks) plot_train_history_loss(history)    We find the lowest validation loss now is 0.265, significantly lower than the previous best value of 0.277. We will first load the saved best model weights, and then plot the original and the reconstructed images from the test dataset.\nautoencoder.load_weights(\u0026#39;weights-ae-146-0.266.hdf5\u0026#39;) decoded_imgs = autoencoder.predict(x_test) display_reconstructed(x_test, decoded_imgs, 10)    At first glance, it seems not much of improvement over the deep autoencoders result. However, if you notice closely, we start to see small feature details to appear on the reconstructed images. In order to improve these models further, we will likely have to go for deeper and more complex convolution network.\nDenoising Autoencoders Another common variant of AE networks is the one that learns to remove noise from the input. Mathematically, this is achieved by modifying the reconstruction error of the loss function.\nTraditionally, autoencoders minimize some loss function: $$L\\Big(x, g\\big(f(x)\\big)\\Big)$$    \nwhere, $L$ is a loss function penalizing reconstructed input $g\\big(f(x)\\big)$ for being dissimilar to the input $x$. Also, $g(.)$ is the decoder and $f(.)$ is the encoder. A Denoising autoencoders (DAE) instead minimizes, $$L\\Big(x, g\\big(f(\\hat{x})\\big)\\Big)$$    \nwhere, $\\hat{x}$ is a copy of $x$ that has been corrupted by some form of noise. DAEs must therefore undo this corruption rather than simply copying their input. Training of DAEs forces $f(.)$, the encoder and $g(.)$, the decoder to implicitly learn the structure of \\(p_{data}(x),\\)    the distribution of the input data $x$. Please refer to the works of Alain and Bengio (2013) and Bengio et al. (2013).\nFor a example, we will first introduce noise to our train and test data by applying a guassian noise matrix and clipping the images between 0 and 1.\n(x_train, _), (x_test, _) = fashion_mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) noise_factor = 0.5 x_train_noisy = x_train \u0026#43; noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) x_test_noisy = x_test \u0026#43; noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) x_train_noisy = np.clip(x_train_noisy, 0., 1.) x_test_noisy = np.clip(x_test_noisy, 0., 1.)   Here is how the corrupted images look now. They are barely recognizable now!\ndisplay_reconstructed(x_test_noisy, None)    We will use a slightly modified version of the previous convolution autoencoder, the one with larger number of filters in the intermediate layers. This increases the capacity of our model.\ninput_img = Input(shape=(28, 28, 1)) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(input_img) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) encoded = MaxPooling2D((2, 2), padding=\u0026#39;same\u0026#39;)(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(encoded) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) x = Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, padding=\u0026#39;valid\u0026#39;, use_bias=False)(x) x = BatchNormalization(axis=-1)(x) x = UpSampling2D((2, 2))(x) decoded = Conv2D(1, (3, 3), activation=\u0026#39;sigmoid\u0026#39;, padding=\u0026#39;same\u0026#39;, use_bias=False)(x) autoencoder = Model(input_img, decoded) autoencoder.compile(optimizer=\u0026#39;rmsprop\u0026#39;, loss=\u0026#39;binary_crossentropy\u0026#39;)   We can now train this for 150 epochs. Notice the change in the training data!\nfpath = \u0026#34;weights-dae-{epoch:02d}-{val_loss:.3f}.hdf5\u0026#34; callbacks = [ModelCheckpoint(fpath, monitor=\u0026#39;val_loss\u0026#39;, verbose=1, save_best_only=True, mode=\u0026#39;min\u0026#39;)] history = autoencoder.fit(x_train_noisy, x_train, epochs=150, batch_size=256, shuffle=True, validation_data=(x_test_noisy, x_test), callbacks=callbacks) plot_train_history_loss(history)   The loss has converged to a value of 0.287. Let\u0026rsquo;s take a look at the results, top row are noisy images and the bottom row are the reconstructed images from the DAE.\n autoencoder.load_weights(\u0026#39;weights-dae-146-0.287.hdf5\u0026#39;) decoded_imgs = autoencoder.predict(x_test_noisy) display_reconstructed(x_test_noisy, decoded_imgs, 10)    Sequence-to-Sequence Autoencoders If your inputs are sequences, rather than 2D images, then you may want to use as encoder and decoder a type of model that can capture temporal structure, such as a LSTM. To build a LSTM-based auto-encoder, first use a LSTM encoder to turn your input sequences into a single vector that contains information about the entire sequence, then repeat this vector $n$ times ( where $n$ is the number of time steps in the output sequence), and run a LSTM decoder to turn this constant sequence into the target sequence.  Variational Autoencoders (VAE)  Variational autoencoders (VAE) are stochastic version of the regular autoencoders. It\u0026rsquo;s a type of autoencoder with added constraints on the encoded representations being learned. More precisely, it is an autoencoder that learns a latent variable model for its input data. So instead of letting your neural network learn an arbitrary function, you are learning the parameters of a probability distribution modeling your data. If you sample points from this distribution, you can generate new input data samples: a VAE is a \u0026ldquo;generative model\u0026rdquo;. The cartoon on the side shows a typical architecture of a VAE model. Please refer to the research papers by Kingma et al. and Rezende et al. for a thorough mathematical analysis.\nIn the probability model framework, a variational autoencoder contains a specific probability model of data $x$ and latent variables $z$ (most commonly assumed as Guassian). We can write the joint probability of the model as $p(x, z) = p(x \\vert z)p(z)$. The generative process can be written as, for each data point $i$:\n Draw latent variables \\(z_i \\sim p(z)\\)    Draw data point \\(x_i \\sim p(x\\vert z)\\)     In terms of an implementation of VAE, the latent variables are generated by the encoder and the data points are drawn by the decoder. The latent variable hence is a random variable drawn from a posterior distribution, $p(z)$. To implement the encoder and the decoder as a neural network, you need to backpropogate through random sampling and that is a problem because backpropogation cannot flow through a random node. To overcome this, the reparameterization trick is used. Most commonly, the true posterior distribution for the latent space is assumed to be Guassian. Since our posterior is normally distributed, we can approximate it with another normal distribution, $\\mathcal{N}(0, 1)$.\n$$p(z) \\sim \\mu + L \\mathcal{N}(0, 1)$$\nHere $\\mu$ and $L$ are the output of the encoder. Therefore while backpropogation, all we need is partial derivatives w.r.t. $\\mu$, $L$. In the cartoon above, $\\mu$ represents the mean vector latent variable and $L$ represents the standard deviation latent variable.\nYou can read more about VAE models at Reference 1, Reference 2, Reference 3 and Reference 4.\nIn more practical terms, VAEs represent latent space (bottleneck layer) as a Guassian random variable (enabled by a constraint on the loss function). Hence, the loss function for the VAEs consist of two terms: a reconstruction loss forcing the decoded samples to match the initial inputs (just like in our previous autoencoders), and the KL divergence between the learned latent distribution and the prior distribution, acting as a regularization term.\n$$\\min \\mathcal{L}(x, x\u0026#39;) = \\min E(x, x\u0026#39;) \u0026#43; KL\\big(q(z\\vert x)\\vert \\vert p(z)\\big)$$     Here, the first term is the reconstruction loss as before (in a typical auto-encoder). The second term is the Kullback-Leibler divergence between the encoder’s distribution, $q(z\\vert x)$ and the true posterior $p(z)$, typically a Guassian.\nAs typically (especially for images) the binary cross-entropy is used as the reconstruction loss term, the above loss term for the VAEs can be written as,\n$$\\min \\mathcal{L}(x, x\u0026#39;) = \\min -\\mathbf{E}_{z\\sim q(z\\vert x)}\\big[ \\log p(x\u0026#39; \\vert z)\\big] \u0026#43; KL\\big(q(z\\vert x)\\vert \\vert p(z)\\big)$$     To summarize a typical implementation of a VAE, first, an encoder network turns the input samples $x$ into two parameters in a latent space, z_mean and z_log_sigma. Then, we randomly sample similar points $z$ from the latent normal distribution that is assumed to generate the data, via $z$ = z_mean + exp(z_log_sigma) * $\\mathbf{\\epsilon}$, where $\\mathbf{\\epsilon}$ is a random normal tensor. Finally, a decoder network maps these latent space points back to the original input data.\nWe can now implement VAE for the fashion MNIST data. To demonstrate its generalization, we will generate two versions: one with MLP and the other with the use of convolution and deconvolution layers.\nIn the first implementation below, we will be using a simple 2-layer deep encoder and a 2-layer deep decoder. Note the use of the reparameterization trick via the sampling() method and a Lambda layer.\nfrom scipy.stats import norm from keras.layers import Input, Dense, Lambda, Flatten, Reshape from keras.layers import Conv2D, Conv2DTranspose from keras.models import Model from keras import backend as K from keras import metrics batch_size = 128 original_dim = 784 latent_dim = 2 intermediate_dim = 256 epochs = 100 epsilon_std = 1.0 x = Input(shape=(original_dim,)) h = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;)(x) z_mean = Dense(latent_dim)(h) z_log_var = Dense(latent_dim)(h) def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std) return z_mean \u0026#43; K.exp(z_log_var / 2) * epsilon # note that \u0026#34;output_shape\u0026#34; isn\u0026#39;t necessary with the TensorFlow backend z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # to reuse these later decoder_h = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;) decoder_mean = Dense(original_dim, activation=\u0026#39;sigmoid\u0026#39;) h_decoded = decoder_h(z) x_decoded_mean = decoder_mean(h_decoded) # instantiate VAE model vae = Model(x, x_decoded_mean)   As described above, we need to include two loss terms, binary cross entropy as before and the KL divergence between the encoder latent variable distribution (calculated using the reparameterization trick) and the true posterior distribution, a normal distribution!\n# Compute VAE loss xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean) kl_loss = - 0.5 * K.sum(1 \u0026#43; z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.mean(xent_loss \u0026#43; kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer=\u0026#39;rmsprop\u0026#39;)   we can now load the fashion MNIST dataset, normalize it and reshape it to correct dimensions so that it can be used with our VAE model.\n# train the VAE on fashion MNIST images (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))   We will now train our model for 100 epochs.\nhistory = vae.fit(x_train, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) plot_train_history_loss(history)   Below is the loss for the training and the validation datasets during training epochs. We find that loss has converged in 100 epochs without any sign of over fitting.\n Because our latent space is two-dimensional, there are a few cool visualizations that can be done at this point. One is to look at the neighborhoods of different classes on the latent 2D plane:\n# build a model to project inputs on the latent space encoder = Model(x, z_mean) # display a 2D plot of the digit classes in the latent space def plot_latentSpace(encoder, x_test, y_test, batch_size): x_test_encoded = encoder.predict(x_test, batch_size=batch_size) plt.figure(figsize=(6, 6)) plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test, cmap=\u0026#39;tab10\u0026#39;) plt.colorbar() plt.show() plot_latentSpace(encoder, x_test, y_test, batch_size)    Each of these colored clusters is a type of the fashion item. Close clusters are items that are structurally similar (i.e. items that share information in the latent space). We cal also look at this plot from a different perspective: the better our VAE model, the separation between very dissimilar fashion items would be larger among their clusters!\nBecause the VAE is a generative model (as described above), we can also use it to generate new images! Here, we will scan the latent plane, sampling latent points at regular intervals, and generating the corresponding image for each of these points. This gives us a visualization of the latent manifold that \u0026ldquo;generates\u0026rdquo; the fashion MNIST images.\n# generator that can sample from the learned distribution decoder_input = Input(shape=(latent_dim,)) _h_decoded = decoder_h(decoder_input) _x_decoded_mean = decoder_mean(_h_decoded) generator = Model(decoder_input, _x_decoded_mean) def plot_generatedImages(generator): # D manifold of the fashion images n = 15 # figure with 15x15 images image_size = 28 figure = np.zeros((image_size * n, image_size * n)) # linearly spaced coordinates on the unit square were transformed through the # inverse CDF (ppf) of the Gaussian # to produce values of the latent variables z, since the prior of the latent # space is Gaussian grid_x = norm.ppf(np.linspace(0.005, 0.995, n)) grid_y = norm.ppf(np.linspace(0.005, 0.995, n)) for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.array([[xi, yi]]) x_decoded = generator.predict(z_sample) digit = x_decoded[0].reshape(image_size, image_size) figure[i * image_size: (i \u0026#43; 1) * image_size, j * image_size: (j \u0026#43; 1) * image_size] = digit plt.figure(figsize=(10, 10)) plt.imshow(figure, cmap=\u0026#39;Greys_r\u0026#39;) plt.show() plot_generatedImages(generator)    We find our model has done only a so-so job in generating new images. Still, given the simplicity and very small amount of simple code we had to write, this is still quite incredible.\nWe can next build a more realistic VAE using conv and deconv layers. Below is the full code to build and train the model.\n# input image dimensions img_rows, img_cols, img_chns = 28, 28, 1 # number of convolutional filters to use filters = 64 # convolution kernel size num_conv = 3 batch_size = 128 if K.image_data_format() == \u0026#39;channels_first\u0026#39;: original_img_size = (img_chns, img_rows, img_cols) else: original_img_size = (img_rows, img_cols, img_chns) latent_dim = 2 intermediate_dim = 128 epsilon_std = 1.0 epochs = 150 x = Input(shape=original_img_size) conv_1 = Conv2D(img_chns, kernel_size=(2, 2), padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;)(x) conv_2 = Conv2D(filters, kernel_size=(2, 2), padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;, strides=(2, 2))(conv_1) conv_3 = Conv2D(filters, kernel_size=num_conv, padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;, strides=1)(conv_2) conv_4 = Conv2D(filters, kernel_size=num_conv, padding=\u0026#39;same\u0026#39;, activation=\u0026#39;relu\u0026#39;, strides=1)(conv_3) flat = Flatten()(conv_4) hidden = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;)(flat) z_mean = Dense(latent_dim)(hidden) z_log_var = Dense(latent_dim)(hidden) def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0., stddev=epsilon_std) return z_mean \u0026#43; K.exp(z_log_var) * epsilon # note that \u0026#34;output_shape\u0026#34; isn\u0026#39;t necessary with the TensorFlow backend # so you could write `Lambda(sampling)([z_mean, z_log_var])` z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) # we instantiate these layers separately so as to reuse them later decoder_hid = Dense(intermediate_dim, activation=\u0026#39;relu\u0026#39;) decoder_upsample = Dense(filters * 14 * 14, activation=\u0026#39;relu\u0026#39;) if K.image_data_format() == \u0026#39;channels_first\u0026#39;: output_shape = (batch_size, filters, 14, 14) else: output_shape = (batch_size, 14, 14, filters) decoder_reshape = Reshape(output_shape[1:]) decoder_deconv_1 = Conv2DTranspose(filters, kernel_size=num_conv, padding=\u0026#39;same\u0026#39;, strides=1, activation=\u0026#39;relu\u0026#39;) decoder_deconv_2 = Conv2DTranspose(filters, kernel_size=num_conv, padding=\u0026#39;same\u0026#39;, strides=1, activation=\u0026#39;relu\u0026#39;) if K.image_data_format() == \u0026#39;channels_first\u0026#39;: output_shape = (batch_size, filters, 29, 29) else: output_shape = (batch_size, 29, 29, filters) decoder_deconv_3_upsamp = Conv2DTranspose(filters, kernel_size=(3, 3), strides=(2, 2), padding=\u0026#39;valid\u0026#39;, activation=\u0026#39;relu\u0026#39;) decoder_mean_squash = Conv2D(img_chns, kernel_size=2, padding=\u0026#39;valid\u0026#39;, activation=\u0026#39;sigmoid\u0026#39;) hid_decoded = decoder_hid(z) up_decoded = decoder_upsample(hid_decoded) reshape_decoded = decoder_reshape(up_decoded) deconv_1_decoded = decoder_deconv_1(reshape_decoded) deconv_2_decoded = decoder_deconv_2(deconv_1_decoded) x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded) x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu) # instantiate VAE model vae = Model(x, x_decoded_mean_squash) # define the loss function xent_loss = img_rows * img_cols * metrics.binary_crossentropy( K.flatten(x), K.flatten(x_decoded_mean_squash)) kl_loss = - 0.5 * K.sum(1 \u0026#43; z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) vae_loss = K.mean(xent_loss \u0026#43; kl_loss) vae.add_loss(vae_loss) vae.compile(optimizer=\u0026#39;rmsprop\u0026#39;) # load the data (x_train, _), (x_test, y_test) = fashion_mnist.load_data() x_train = x_train.astype(\u0026#39;float32\u0026#39;) / 255. x_train = x_train.reshape((x_train.shape[0],) \u0026#43; original_img_size) x_test = x_test.astype(\u0026#39;float32\u0026#39;) / 255. x_test = x_test.reshape((x_test.shape[0],) \u0026#43; original_img_size) # train the VAE model history = vae.fit(x_train, shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None)) plot_train_history_loss(history)   Similar to the case of simple VAE model, we can look at the neighborhoods of different classes on the latent 2D plane:\n# project inputs on the latent space encoder = Model(x, z_mean) plot_latentSpace(encoder, x_test, y_test, batch_size)    We can now see that the separation between different class of images are larger than the simple MLP based VAE model.\nFinally, we can now generate new images from our, hopefully, better VAE model.\n# generator that can sample from the learned distribution decoder_input = Input(shape=(latent_dim,)) _hid_decoded = decoder_hid(decoder_input) _up_decoded = decoder_upsample(_hid_decoded) _reshape_decoded = decoder_reshape(_up_decoded) _deconv_1_decoded = decoder_deconv_1(_reshape_decoded) _deconv_2_decoded = decoder_deconv_2(_deconv_1_decoded) _x_decoded_relu = decoder_deconv_3_upsamp(_deconv_2_decoded) _x_decoded_mean_squash = decoder_mean_squash(_x_decoded_relu) generator = Model(decoder_input, _x_decoded_mean_squash) plot_generatedImages(generator)    Usage of Autoencoders Most common uses of Autoenoders are:\n Dimensionality Reduction: Dimensionality reduction was one of the first applications of representation learning and deep learning. Lower-dimensional representations can improve performance on many tasks, such as classification. Models of smaller spaces consume less memory and runtime. The hints provided by the mapping to the lower-dimensional space aid generalization. Due to non-linear nature, autoencoders tend to perform better than traditional techniques like PCA, kernel PCA etc. Denoising and Transformation: You can distort the data and add some noise in it before feeding it to DAEs. This can help in generalizing over the test set. AEs are also useful in image transformation tasks, eg. document cleaning, applying color to images, medical image segmentation using U-net, a variant of autoencoders etc.\n Information Retrieval: the task of finding entries in a database that resemble a query entry. This task derives the usual benefits from dimensionality reduction that other tasks do, but also derives the additional benefit that search can become extremely efficient in certain kinds of low-dimensional spaces.\n In Natural Language Processing\n Word Embeddings Machine Translation Document Clustering Sentiment Analysis Paraphrase Detection  Image/data Generation: We saw theoretical details of generative nature of VAEs above. See this blog post by openAI for a detailed review of image generation.\n Anamoly detection: For highly imbalanced data (like credit card fraud detection, defects in manufacturing etc.) you may have sufficient data for the positive class and very few or no data for the negative class. In such situations, you can train an AE on your positive data and learn features and then compute reconstruction error on the training set to find a threshold. During testing, you can use this threshold to reject those test instances whose values are greater than this threshold. However, optimizing the threshold that can generalize well on the unseen test cases is challenging. VAEs have been used as alternative for this task, where reconstruction error is probabilistic and hence easier to generalize. See this article by FICO where they use autoencoders for detecting anomalies in credit scores.\n  This is it! its been quite a long article. Hope this is helpful to some of you. Please let me know via comments below if any particular issues/concepts you would like me to go over in more details. I would also love to know if any particular topic in machine/deep learning you would like me to cover in future posts.\n","title":"A Practical guide to Autoencoders","url":"https://sadanand-singh.github.io/posts/autoencoders/"},{"tags":"Machine Learning, Algorithms, Python","text":"In the previous post, we learned about tree based learning methods - basics of tree based models and the use of bagging to reduce variance. We also looked at one of the most famous learning algorithms based on the idea of bagging- random forests. In this post, we will look into the details of yet another type of tree-based learning algorithms: boosted trees.  Boosting Boosting, similar to Bagging, is a general class of learning algorithm where a set of weak learners are combined to get strong learners. For classification problems, a weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. Recall that bagging involves creating multiple copies of the original training data set via bootstrapping, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Notably, each tree is built on a bootstrap data set, independent of the other trees.\nIn contrast, in Boosting, the trees are grown sequentially: each tree is grown using information from previously grown trees. Boosting does not involve bootstrap sampling; instead each tree is fit on a modified version of the original data set.\nUnlike fitting a single large decision tree to the data, which amounts to fitting the data hard and potentially overfitting, the boosting approach instead learns slowly. Given the current model, a decision tree is fit to the residuals from the model. That is, a tree is fit using the current residuals, rather than the outcome as the response.\nThe above concept of additively fitting weak learners to get a strong learner is not limited to tree based methods. In fact, we can choose these weak learners to be any model of our choice. In practice, however, decision trees or sometimes linear models are the most common choices. In this post, we will be focusing mainly on boosted trees - i.e. our choice of weak learners would be decision trees. Boosted trees are one of the most used models in on-line machine learning competitions like Kaggle.\nLet us consider the case of boosted trees more rigorously. In particular, we will look at the case of boosted decision trees for regression problems. To begin, set the resultant model $\\hat{F}(x) = 0$ and residual of the training response as the training target $y$. Now Sequentially, for steps $b=1,2\\ldots,B$ update $\\hat{F}(x)$ as follows: $\\hat{F}(x)\\leftarrow \\hat{F}(x) + \\lambda_b \\hat{f}^{b}(x)$, where $\\hat{f}^{b}(x)$ is a decision tree learner fit on $(X, r)$ with $d$ splits (or $d+1$ terminal nodes) and $\\lambda_b$ is a weighting parameter. Also update residuals, $r$ as, $r \\leftarrow r - \\lambda_b \\hat{f}^{b}(x)$. At the end of $B$ steps, we will get the final model as $\\hat{F}(x) = \\sum_{b=1}^{B}\\lambda_b\\hat{f}^{b}(x)$.\nThe above algorithm of boosted is quite generic. One can come with several strategies for how the decision trees are grown and fit on residuals, and for the choice of the weighting parameter $\\lambda$. Therefore, there are various boosting methods. Some common examples are:\n Adaptive Boosting (AdaBoost) BrownBoost Gentle Boost LP Boost Gradient Boosting  In this post, we will focus specifically on two of the most common boosting algorithms, AdaBoost and Gradient Boosting.\nAdaptive Boosting (AdaBoost) Adaptive Boosting or AdaBoost refers to a particular formulation of boosting where the idea of residual learning is implemented through the concept of sample weights. In particular, at any iteration, $b$, all training samples have a weight $w_i^b$ $\\forall i = 1, \\ldots N$, which is equal to the current error $E(\\hat{f}(x_i))$. The decision tree $\\hat{f}^{b}(x)$ is fit to minimize the error $E_b = \\sum_{i=1}^{N}{E\\big(\\hat{F}(x) + \\lambda_b \\hat{f}^{b}(x)\\big)}$. AdaBoost uses an exponential error/loss function: $E(\\hat{F}(x_i)) = e^{-y_i \\hat{F}(x_i)}$. The choice of the error function, $E()$, also determines the way weights $w_i^b$ and the weighting parameter $\\lambda_b$ are updated at different steps. A neat mathematical derivation of these quantities for the case of binary classification can be found at the Wiki article.\nIn particular case of the classification problems, we can also think of AdaBoost to be an algorithm where at any step, the misclassified samples get a higher weight than the correctly classified samples. The weights attached to samples are used to inform the training of the weak learner, in this case, decision trees to be grown in a way that favor splitting the sets of samples with high weights.\nIn practice, the above definition of the AdaBoost model is modified to include a learning rate term, $\\gamma$, as a regularization term that shrinks the contribution of all of the models. In particular, the model is updated as, $\\hat{F}(x) \\leftarrow \\hat{F}(x) + \\gamma \\lambda_b \\hat{f}^{b}(x)$. The main tuning parameters of AdaBosst are, the learning rate $\\gamma$, number of estimators $B$, and decision tree related parameters like depth of trees $d$, and number of samples per split etc.\n\nAdaBoost in Python The python scikit-learn library implementations of AdaBoost (AdaBoostClassifier and AdaBoostRegressor) are based on a modified version of the AdaBoost algorithms, AdaBoost SAMME and SAMME.R. Stage-wise Additive Modeling using a Multi-class Exponential loss function (SAMME) algorithm provides an extension of AdaBoost for the case of multi-class classification. The SAMME.R (R for real) variation of the algorithm is for prediction of weighted probabilities rather than the class itself.  AdaBoost Classifier in Python Recall the US income data that we used in the previous based post on tree based methods. In summary, in this dataset, we are required to predict the income range of adults (\u0026lt;=50K or \u0026gt;50K) based on following features: Race, Sex, Education, Work Class, Capital Loss, Capital Gain, Relationship, Marital Status, Age Group, Occupation and Hours of Work per Week. We have already seen that, with the best decision tree model, we were able to achieve a prediction accuracy of 85.9%. With the use of random forest models, we were able to increase the accuracy to 86.6%.\nLet us try to solve the same problem using AdaBoost classifier from scikit-learn module. Please have a look at the previous post on tree-based methods to understand the EDA and preparation of the data.\nfrom sklearn.ensemble import AdaBoostClassifier aclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=5), n_estimators=100) aclf.fit(x_train, y_train) aclf.score(x_test, y_test)   Without any parameter tuning, we see an accuracy of 85.54%!\n\nGenetic Algorithm for Parameter Search Once the number of hyper-parameters and the range of those parameter in models becomes large, the time required to find the best parameters becomes exponentially large using simple grid search. Instead of trying out every possible combination of parameters, we can evolve only the combinations that give the best results using Genetic Algorithms. A scikit-learn friendly implementation of this can be found in the sklearn-deap library. In many of the examples below we will be using this library to search for best model parameters.\nThe genetic algorithm itself has three main parameters: population size, tournament size and no. of generations. Typically, for the problem of parameter search, we can use a population of 10-15 per hyper-parameter. The tournament size parameter affects the diversity of population being considered in future generation. This roughly translates into global vs local optimization for parameter search. In other words, if the tournament size is too large, we will have higher chance of getting a solution that is from local minima. A typical value of 0.1 times the population works well for the exercise of finding optimal model parameters. The number of generation decides the convergence of genetic algorithms. A larger value leads to better convergence but requires larger computation time.\n  Let us try using genetic algorithm to find optimal model parameters for AdaBoost classifier.\nfrom evolutionary_search import EvolutionaryAlgorithmSearchCV parameters = { \u0026#39;base_estimator__max_features\u0026#39;:(11, 9, 6), \u0026#39;base_estimator__max_depth\u0026#39;:(1, 2, 4, 8), \u0026#39;base_estimator__min_samples_split\u0026#39;: (2, 4, 8), \u0026#39;base_estimator__min_samples_leaf\u0026#39;: (16, 12, 8, 4), \u0026#39;n_estimators\u0026#39;: (50, 100, 200, 500), \u0026#39;learning_rate\u0026#39;: (1, 0.1, 0.01, 10) } clf2 = EvolutionaryAlgorithmSearchCV(estimator=aclf, params=parameters, scoring=\u0026#34;accuracy\u0026#34;, cv=5, verbose=1, population_size=200, gene_mutation_prob=0.10, gene_crossover_prob=0.5, tournament_size=10, generations_number=100, n_jobs=8) clf2.fit(x_train, y_train)   This should take about 50 minutes on a reasonable desktop machine! We can now use the best parameters from this and create a new AdaBoost classifier.\naclf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=4, max_features=11, min_samples_leaf=4, min_samples_split=2), n_estimators=100, learning_rate=0.1) aclf.fit(x_train, y_train) aclf.score(x_test, y_test)   We see a significant improvement in our results with an accuracy of 87.06% on the testing data!\nGiven our data is highly imbalanced, let us look at the confusion matrix of our model on the test data. Note that we are using the confusion_matrix() method from the previous post on tree based methods.\ny_pred = aclf.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    We find that our model has a much better predictive power (94.1%) for the dominant class (\u0026lt;=50K), while it has prediction rate of only 64.2% for the other class.\nGradient Boosting The approach used in the case of AdaBoost can be also viewed as an exponential loss minimization approach. Let us look at this mathematically for the case of a generic loss function, $L\\big(y,\\hat{F}(x)\\big)$. The goal of the method is to find an $\\hat{F}(x)$ that minimizes the average value of the loss function $L\\big(y,\\hat{F}(x)\\big)$ on the training set. This can be achieved by starting with a constant function $\\hat{F_0}(x)$, and incrementally expanding it in a greedy fashion as follows: $$\\begin{aligned} \u0026amp; \\hat{F_0}(x) = \\mathop{\\arg\\min}\\limits_{\\gamma} \\sum_{i=1}^{N} L\\big(y,\\gamma \\big) \\\\ \u0026amp; \\hat{F_b}(x) = \\hat{F}_{b-1}(x) \u0026#43; \\mathop{\\arg\\min}\\limits_{f \\in \\mathcal{H}} \\sum_{i=1}^{N} L\\big(y,\\hat{F}_{b-1}(x) \u0026#43; f(x) \\big) \\text{, for } b=1,2,\\ldots, B \\end{aligned}$$    \nwhere $f(x) \\in \\mathcal{H}$ refers to base learners, in this case tree models. The problem with this set up is that, it is computationally infeasible to choose optimal $f(x)$ at every step for an arbitrary loss function $L\\big(y,\\hat{F}(x)\\big)$.\nHowever, this can be simplified using a steepest descent approach. In this approach, at any step the decision tree model is trained on the pseudo-residuals, rather than residuals. The approach can be described in the following algorithm:\n Initialize model as, $$\\hat{F_0}(x) = \\mathop{\\arg\\min}\\limits_{\\gamma} \\sum_{i=1}^{N} L\\big(y,\\gamma \\big)$$     for steps $b=1,2,\\ldots, B$:  compute pseudo-residuals as: $$r_{ib} = -\\Bigg[ \\frac{\\partial{L\\big(y_i, \\hat{F}_{b-1}(x_i)\\big)}}{\\partial{\\hat{F}_{b-1}(x_i)}} \\Bigg]$$     Fit a decision tree $\\hat{f^b}(x)$ to pseudo-residuals, i.e. train it using the training set $(x_i, r_{ib})_{i=1}^N$ Compute multiplier $\\gamma_b$ using line search, where $0\u0026lt;\\nu\u0026lt;1$ is the learning rate parameter $$\\gamma_b = \\mathop{\\arg\\min}\\limits_{\\gamma} \\sum_{i=1}^N L\\big(y_i, \\hat{F}_{b-1}(x) \u0026#43; \\nu \\gamma \\hat{f^b}(x)\\big)$$     update the model $$\\hat{F_b}(x) = \\hat{F}_{b-1}(x) \u0026#43; \\nu \\gamma_b \\hat{f^b}(x)$$       In most real implementations of gradient boosted trees, rather than an individual tree weighing parameter $\\gamma_b$, different parameters are used at different splits, $\\gamma_{jb}$. If you recall from the last post, a decision tree model corresponds to diving the feature space in multiple rectangular regions and hence it can be represented as,\n$$\\hat{f^b}(x) = \\sum_{j=1}^{J} k_{jb} I\\big(x\\in R_{jb}\\big)$$     where, $J$ is the number of terminal nodes (leaves), $I\\big(x\\in R_{jb}\\big)$ is an indicator function which is 1 if $x\\in R_{jb}$ and $k_{jb}$ is the prediction in $j^{th}$ leaf. Now, we can replace $\\hat{f^b}(x)$ in above algorithm and replace $\\gamma_b$ for the whole tree by $\\gamma_{jb}$ per terminal node (leaf).\n$$\\hat{F_b}(x) = \\hat{F}_{b-1}(x) \u0026#43; \\nu \\sum_{j=1}^{J_b} \\gamma_{jb} I\\big(x\\in R_{jb}\\big)$$     where $\\gamma_{jb}$ is given by the following line search, $$\\gamma_{jb} = \\mathop{\\arg\\min}\\limits_{\\gamma} \\sum_{x_i \\in R_{jb}} L\\big(y_i, \\hat{F}_{b-1}(x) \u0026#43; \\gamma \\big)$$    \nHere $J$ refers to the number of terminal nodes (leaves) in any of constituent decision trees. A value of $J_b =2$, i.e. decision stumps means no interactions among feature variables are considered. With a value of $J_b=3$ the model may include effects of the interaction between up to two variables, and so on. Typically a value of $4 \\le J_b \\le 8$ work well for boosting.\n\nRegularization of Gradient Boosted Trees Gradient Boosted Trees can be regularized by multiple approaches. Some common approaches are:\n Shrinkage / Learning Rate: For each gradient step, the step magnitude is multiplied by a factor between 0 and 1 called a learning rate ($0 \u0026lt;\\nu \u0026lt; 1$). In other words, each gradient step is shrunken by some factor $\\nu$. The rational for this to work as a regularization parameter has never been clear to me. My personal take is the shrinkage enables us to use a different prior. Telgarsky et al. provide a mathematical proof that shrinkage makes gradient boosting to produce an approximate maximum margin classifier, i.e. a classifier which is able to maximize the associated distance from the decision boundary for each example. Sub-Sampling: Motivated by the bagging method, at each iteration of the algorithm, a decision tree is fit on a subsample of the training set drawn at random without replacement. Also, like in bagging, sub-sampling allows one to define an out-of-bag error of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimate actual performance improvement and the optimal number of iterations. Minimum sample size for splitting trees, and Minimum sample size for tree leaves: It is used in the tree building process by ignoring any splits that lead to nodes containing fewer than this number of training set instances. Imposing this limit helps to reduce variance in predictions at leaves. The number of trees or Boosting Iterations, $B$ Increasing $B$ reduces the error on training set, but setting it too high may lead to over-fitting. An optimal value of $B$ is often selected by monitoring prediction error on a separate validation data set. Sampling Features: We can apply the idea of randomly choosing small subsets of features for different trees, as in the case of Random Forest models.    scikit-learn Implementation scikit-learn provides a simple and generic implementation of the above described algorithm that is valid of different types of loss functions. Below is a simple implementation for the case of income data.\nfrom sklearn.ensemble import GradientBoostingClassifier params = {\u0026#39;n_estimators\u0026#39;: 200, \u0026#39;max_depth\u0026#39;: 4, \u0026#39;subsample\u0026#39;: 0.75, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;min_samples_leaf\u0026#39;: 4, \u0026#39;random_state\u0026#39;: 3} gclf = GradientBoostingClassifier(**params) gclf.fit(x_train, y_train) gclf.score(x_test, y_test)   Turns out we have got a quite good model just by chance! This has an accuracy of 87.11% on our test data!\nWe could try finding optimal parameters for this as before. However, in my experience this is a very academic implementation of boosted trees. Other implementation like XGBoost, LightGBM and CatBoost are more optimized implementations and hence we will focus our tuning for some of these libraries only.\nXGBoost The biggest drawback of the gradient boosting trees is that the algorithm is quite sequential in nature and hence are very slow and they can not take advantage of advanced computing features for parallelization like multiple threads and cores, GPU etc.\nXGBoost is a python (C++ and R as well) library that provides an optimized implementation of gradient boosted trees. It uses various tricks like regularization of number of leaves and leaf weights, sparsity aware split finding, column block for parallel learning and cache-aware access. Using these tricks the implementation provides a parallelized efficient version of the algorithm. The details of these can be found here. XGBoost is one of the most famous machine learning libraries used in on-line machine learning competitions like Kaggle.\n\nXGBoost: Additional Remarks Regularization: Apart from regular gradient boosted trees, XGBoost provides two additional types of regularization, by adding $L_1$ constraints on number of leaves ($J_b$) and $L_2$ constraints on the leaf weights ($\\gamma_{jb}$) to the loss function. Mathematically, The loss function is modified as follows: $$\\text{Loss Term at step b}= \\sum_{i=1}^{N} L\\big( y_i, \\hat{F_b}(x_i) \\big) \u0026#43; \\sum_{k=1}^b \\Big (\\eta J_k \u0026#43; \\frac{1}{2} \\lambda \\left\\lVert \\gamma_{jk} \\right\\rVert^2 \\Big )$$\n \n  Here, the second term in the loss function, penalizes the complexity of the model, i.e. decision tree functions.\nAdditional Weak Learners: Apart from decision trees, XGBoost also supports linear models and DART (decision trees with dropout) as weak learners. In the DART algorithm, only a subset of available trees are considered in calculating the pseudo-residuals on which the new trees are fit.\n  XGBoost has many parameters that control the fitting of the model. Below are some of the relevant parameters and tuning them would be helpful in the most common cases. Please note that original XGBoost library parameters might have a different name than before, since I am using the scikit-learn API parameter names below.\n max_depth (default=3) : Maximum depth of a tree, increase this value will make the model more complex / likely to be over-fitting. A value of 0 indicates no limit. A Typical Value in the range of 2-10 can be used for model optimization. n_estimators (default=100) : The number of boosting steps to perform. This can also be seen as number of trees to be used in the boosted model. This number is inversely proportional to the learning_rate (eta) parameter, i.e. if we use a smaller value of learning_rate, n_estimators has be made larger. A Typical Value in the range of $\\ge 100$ can be used for model optimization. However, it is best to used XGBoost built-in cv() method to find this parameter (See the example ahead). min_child_weight (default=1) : The minimum sum of instance weight needed in a child node. If the tree partition step results in a leaf node with the sum of instance weight less than the min_child_weight, then any further partitioning will be stopped. The larger the value of this parameter, the more conservative the algorithm will be. A Typical Value in the range of 1-10 can be used for model optimization. learning_rate (default=0.1) : The step size shrinkage used to prevent over-fitting. After each boosting step, we can directly get the weights of new features. and the learning_rate parameter (also referred as eta in regular XGBoost API) shrinks the feature weights to make the boosting process more conservative (See above formulation of Gradient Boosted Trees for mathematical details). A Typical Value in the range of 0.001-0.3 can be used for model optimization. gamma (default=0) : (Also referred as min_split_loss in regular XGBoost API) The minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be. The value of this parameter depends on the type of loss function being used. A Typical Value in the range of 0.0-0.7 can be used for model optimization. reg_alpha (default=0) : (Also referred as alpha in regular XGBoost API) L1 regularization term on weights, increasing this value will make model more conservative (see XGBoost paper for mathematical details). A Typical Value in the range of 0-1 can be used for model optimization. reg_lambda (default=1) : (Also referred as lambda in regular XGBoost API) L2 regularization term on weights, increasing this value will make model more conservative (see XGBoost paper for mathematical details). A Typical Value in the range of 0-1 can be used for model optimization. subsample (default=1) : Subsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees. This parameter is used to prevent over-fitting. A Typical Value in the range of 0.5-1.0 can be used for model optimization. colsample_bytree (default=1) : Subsample ratio of columns when constructing each tree. Similar to Random Forest models, models tend to be more generalizable, if a number between 0 and 1 is used. A Typical Value in the range of 0.5-1.0 can be used for model optimization. scale_pos_weight (default=1) : Controls the balance of positive and negative class weights, useful for unbalanced class problems. A typical value to consider: sum(negative cases) / sum(positive cases).  Apart from above set of parameters, there are several parameters that should also be considered while tuning. Some examples of such parameters are: objective (objective function/ loss function to use, depends on problem, for eg. binary vs. multi-class classification), tree_method (The tree construction algorithm used in XGBoost(see description in the paper), random_state (seed for random number generator), n_jobs (number of threads to use to train the model) and many other parameters related to different types of tree methods used. This article can be used as a good general reference for tuning XGBoost models.\nLet us tune XGBoost model for our problem of income prediction. A simple sklearn API implementation can be used as below.\nimport xgboost as xgb params = {\u0026#39;n_estimators\u0026#39;: 100, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 0.75, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;min_samples_split\u0026#39;: 2, \u0026#39;min_samples_leaf\u0026#39;: 8, \u0026#39;random_state\u0026#39;: 32, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;n_jobs\u0026#39;: 8 } xclf = xgb.XGBClassifier(**params) xclf.fit(x_train, y_train) xclf.score(x_test, y_test)   With this reasonable guess of parameters based on previous models, we already see an accuracy of 86.75%.\nLet us try to find the optimal parameters for the XGBoost model. If we simply try to do a brute force grid search, it can be computationally very expensive and unreasonable on a desktop. Here is a sample parameters list that can give us an idea of what such a grid search could look like.\nindependent_params = { \u0026#39;random_state\u0026#39;: 32, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;n_jobs\u0026#39;: 8, } params = {\u0026#39;n_estimators\u0026#39;: (100, 200, 400, 800, 1000), \u0026#39;max_depth\u0026#39;: (4, 6, 8), \u0026#39;subsample\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;learning_rate\u0026#39;: (0.1, 0.01, 0.05), \u0026#39;colsample_bytree\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;min_child_weight\u0026#39;: range(1,6,2), \u0026#39;reg_alpha\u0026#39;: [i/10.0 for i in range(0,5)], \u0026#39;gamma\u0026#39;:[i/10.0 for i in range(0,5)], \u0026#39;reg_lambda\u0026#39;: (1, 0.1, 10) }   If we try to do a grid search of this with 5-fold cross validation, it will involve a whopping 0.81 million model training calls! And, even this will not be enough, as we will need additional model training steps to fine-tune our parameter search for finer and/or different range of parameters. Clearly, we need a different approach to solve this.\nI will take an approach of optimizing different set of parameters in batches. To begin, we will choose a fixed learning rate of 0.1, and n_estimators=200. We will try to find only tree related parameters (i.e. max_depth, gamma, subsample and colsample_bytree) using grid search or genetic algorithm.\nind_params = { \u0026#39;random_state\u0026#39;: 32, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;n_estimators\u0026#39;: 200, \u0026#39;learning_rate\u0026#39;: 0.1, } params = {\u0026#39;max_depth\u0026#39;: (4, 6, 8), \u0026#39;subsample\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;colsample_bytree\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;gamma\u0026#39;: [i/10 for i in range(0,5)] } clf2 = EvolutionaryAlgorithmSearchCV(estimator=xgb.XGBClassifier(**ind_params), params=params, scoring=\u0026#34;accuracy\u0026#34;, cv=5, verbose=1, population_size=60, gene_mutation_prob=0.10, gene_crossover_prob=0.5, tournament_size=5, generations_number=100, n_jobs=8) clf2.fit(x_train, y_train)   This gives us the following optimal values for different parameters:\nBest individual is: {'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.8, 'gamma': 0.2} with fitness: 0.8710727557507447  We can do a finer grid search to get more precise values. For this exercise, let us move on to the next stage of parameter tuning of XGBoost.\nXGBoost provides an optimized version of cross validation using cv() method which supports early stopping to give us optimal value of n_estimators.\nxgb1 = xgb.XGBClassifier( learning_rate =0.1, n_estimators=10000, max_depth=6, min_child_weight=1, gamma=0.2, subsample=1.0, colsample_bytree=0.8, objective= \u0026#39;binary:logistic\u0026#39;, n_jobs=8, scale_pos_weight=1, random_state=32) xgb_param = xgb1.get_xgb_params() xgtrain = xgb.DMatrix(x_train, label=y_train) cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=xgb1.get_params()[\u0026#39;n_estimators\u0026#39;], nfold=5, metrics=\u0026#39;auc\u0026#39;, early_stopping_rounds=50) print(\u0026#34;Number of Predicted n_estimators = \u0026#34;, cvresult.shape[0])   This gives us a value of n_estimators = 206. We will now use these parameters to search for the next set of tree building parameters: max_depth and min_child_weight.\nparams = { \u0026#39;max_depth\u0026#39;:range(3,10,2), \u0026#39;min_child_weight\u0026#39;:range(1,6,2) } ind_params = {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 206, \u0026#39;gamma\u0026#39;: 0.2, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.8, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;random_state\u0026#39;: 32} clf2 = EvolutionaryAlgorithmSearchCV(estimator=xgb.XGBClassifier(**ind_params), params=params, scoring=\u0026#34;accuracy\u0026#34;, cv=5, verbose=1, population_size=30, gene_mutation_prob=0.10, gene_crossover_prob=0.5, tournament_size=5, generations_number=100, n_jobs=8) clf2.fit(x_train, y_train)   The optimal set of parameters found by this are:\nBest individual is: {'max_depth': 7, 'min_child_weight': 1} with fitness: 0.8712877368631184  We can now use these parameters as fixed values and optimize regularization parameters: reg_alpha and reg_lambda.\nind_params = {\u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 206, \u0026#39;gamma\u0026#39;: 0.2, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.8, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;random_state\u0026#39;: 32, \u0026#39;max_depth\u0026#39;: 7, \u0026#39;min_child_weight\u0026#39;: 1} params = {\u0026#39;reg_alpha\u0026#39;:[0, 0.001, 0.005, 0.01, 0.05], \u0026#39;reg_lambda\u0026#39;:[0.01, 0.1, 1, 10, 100]} clf2 = EvolutionaryAlgorithmSearchCV(estimator=xgb.XGBClassifier(**ind_params), params=params, scoring=\u0026#34;accuracy\u0026#34;, cv=5, verbose=1, population_size=30, gene_mutation_prob=0.10, gene_crossover_prob=0.5, tournament_size=3, generations_number=100, n_jobs=8) clf2.fit(x_train, y_train)   The optimal set of parameters found by this search are:\nBest individual is: {'reg_alpha': 0.001, 'reg_lambda': 1} with fitness: 0.8714720063880101  We can now decrease the learning rate by an order to magnitude to get a more stable model. However, we will also need to find the optimal value of number of estimators again using the cv() method.\nind_params = {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 5000, \u0026#39;gamma\u0026#39;: 0.2, \u0026#39;reg_alpha\u0026#39;: 0.001, \u0026#39;reg_lambda\u0026#39;: 1, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.8, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;random_state\u0026#39;: 32, \u0026#39;max_depth\u0026#39;: 7, \u0026#39;n_jobs\u0026#39;: 8, \u0026#39;min_child_weight\u0026#39;: 1} xgb2 = xgb.XGBClassifier(**ind_params) xgb_param = xgb2.get_xgb_params() xgtrain = xgb.DMatrix(x_train, label=y_train) cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=xgb1.get_params()[\u0026#39;n_estimators\u0026#39;], nfold=5, metrics=\u0026#39;auc\u0026#39;, early_stopping_rounds=50) print(\u0026#34;Number of Predicted n_estimators = \u0026#34;, cvresult.shape[0])   We get an optimal value of n_estimators = 1559. Let us use now all of these optimized values to make a final XGBoost model.\nind_params = {\u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 1559, \u0026#39;gamma\u0026#39;: 0.2, \u0026#39;reg_alpha\u0026#39;: 0.001, \u0026#39;reg_lambda\u0026#39;: 1.0, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.8, \u0026#39;objective\u0026#39;: \u0026#39;binary:logistic\u0026#39;, \u0026#39;random_state\u0026#39;: 32, \u0026#39;max_depth\u0026#39;: 7, \u0026#39;n_jobs\u0026#39;: 8, \u0026#39;min_child_weight\u0026#39;: 1} xclf = xgb.XGBClassifier(**ind_params) xclf.fit(x_train, y_train) xclf.score(x_test, y_test)   We get test accuracy of 86.99%. Now, this seems odd. After we did all this computation and we still have got a test accuracy that is smaller than an optimized version of scikit-learn\u0026rsquo;s Gradient Boosting Tree implementation. However, if you have paid attention to the metric, you can notice - for cross validation, I started using \u0026lsquo;auc\u0026rsquo; as metric, instead of \u0026lsquo;accuracy\u0026rsquo;. This will give us better accuracy for the less abundant class (\u0026gt; 50K salary) but at the cost of slight decrease in the accuracy of the more abundant class (\u0026lt;= 50K salary). We can verify this by the following confusion matrix plot.\ny_pred = xclf.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    We see the largest accuracy of the less abundant class with an accuracy of 64.9%, compared to the previous best of 64.2%. We can also look the importance of different features for this XGBoost model.\nimportances = xclf.feature_importances_ indices = np.argsort(importances) cols = [cols[x] for x in indices] plt.figure(figsize=(10,6)) plt.title(\u0026#39;Feature Importances\u0026#39;) plt.barh(range(len(indices)), importances[indices], color=\u0026#39;b\u0026#39;, align=\u0026#39;center\u0026#39;) plt.yticks(range(len(indices)), cols) plt.xlabel(\u0026#39;Relative Importance\u0026#39;)    LightGBM Similar to XGBoost, LightGBM is another optimized implementation of Gradient Boosting developed by Microsoft (similar to XGBoost available in Python, C++ and R). The main difference between LightGBM and other Gradient boosted trees (like XGBoost) implementations is in the way trees are grown. The details of this can be found on their features page. Briefly, LightGBM splits the tree leaf-wise with the best fit whereas other boosting algorithms split the tree depth-wise or level-wise. The two approaches can be best visualized in the following illustrations:\nLevel-wise Splits:\n Leaf-wise Splits:\n  Courtsey: LightGBM User Guide   Leaf-wise splits lead to increase in complexity and may lead to over-fitting, and hence extra caution needs to be taken in tuning. Some of the biggest advantages of LightGBM over XGBoost is in terms of extremely fast training speed, lower memory usage, compatibility with large datasets and highly parallel computational support using threads, MPI and GPUs. LightGBM also has inbuilt support for categorical variables, unlike XGBoost, where one has to pre-process the data to convert all of the categorical features to integer ones using one-hot encoding or label encoding.\nSince the trees are grown differently in LightGBM, its tuning procedure is quite different than XGBoost. Note that the latest version of XGBoost also provides a tree building strategy (depth-wise) which is quite similar to LightGBM.\nAll the parameters described above for XGBoost are also valid for LightGBM library. However, some parameters can have different names. Given the strategy of growing trees is different in LightGBM, an additional parameter needs to be tuned as well.\nnum_leaves: Maximum tree leaves for base learners. Note that since trees are grown depth-first, this parameters is independent of the max_depth parameter and has to be tuned independently. A typical value for starting should be much less than 2max _depth.\nWhen using the scikit-learn API of LightGBM, one should keep in mind that some of the parameter names are not standard ones (even though described in the API reference). In particular, I found that seed and nthreads as parameters, instead of random_state and n_jpbs, respectively.\nLet us tune a LightGBM model for the problem of Income prediction.\nimport lightgbm as lgb params = {\u0026#39;n_estimators\u0026#39;: 100, \u0026#39;num_leaves\u0026#39;: 48, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 0.75, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;min_child_samples\u0026#39;: 8, \u0026#39;seed\u0026#39;: 32, \u0026#39;nthread\u0026#39;: 8 } lclf = lgb.LGBMClassifier(**params) lclf.fit(x_train, y_train) lclf.score(x_test, y_test)   This results in test accuracy of 86.8%.\nGiven this library also has many parameters, similar to XGBoost, we need to use a similar strategy of tuning in stages.\nFirst we will fix learning rate to a reasonable value of 0.1 and number of estimators = 200, and tune only the major tree building parameters: max_depth, subsample, colsample_bytree and num_leaves. We will use the genetic algorithm to search for optimal values of these parameters.\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;n_estimators\u0026#39;: 200, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;nthread\u0026#39;: 1 } params = {\u0026#39;max_depth\u0026#39;: (4, 6, 8), \u0026#39;subsample\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;colsample_bytree\u0026#39;: (0.75, 0.8, 0.9, 1.0), \u0026#39;num_leaves\u0026#39;: (12, 16, 36, 48, 54, 60, 80, 100) } clf2 = EvolutionaryAlgorithmSearchCV(estimator=lgb.LGBMClassifier(**ind_params), params=params, scoring=\u0026#34;accuracy\u0026#34;, cv=5, verbose=1, population_size=50, gene_mutation_prob=0.10, gene_crossover_prob=0.5, tournament_size=5, generations_number=100, n_jobs=8) clf2.fit(x_train, y_train)   This gives the following set of optimal parameters:\nBest individual is: {'max_depth': 6, 'subsample': 1.0, 'colsample_bytree': 0.75, 'num_leaves': 54} with fitness: 0.870888486225853  Now, we can use grid search to fine tune the search of number of leaves parameter.\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;n_estimators\u0026#39;: 200, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;nthread\u0026#39;: 1, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.75, } params = { \u0026#39;num_leaves\u0026#39;: (48, 50, 52, 54, 56, 58, 60) } clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_)   Similar to XGBoost, LightGBM also provides a cv() method that can be used to find optimal value of number of estimators using early stopping strategy. Another strategy would be to search for this parameter as well. In the following, I want to use this grid search strategy to find best value of number of estimators, just to show how tedious this can be!\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;nthread\u0026#39;: 1, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.75, \u0026#39;num_leaves\u0026#39;: 54 } params = {\u0026#39;n_estimators\u0026#39;: (200,300,400,800,1000)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;n_estimators\u0026#39;: (250,275,300,320,340,360,380)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;n_estimators\u0026#39;: (322,324,325,326,327,328,330,332,334,336,338)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_)   We find that the optimal value of n_estimators to be 327.\nNow, we can use the similar strategy to find and fine-tune the best regularization parameters.\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;learning_rate\u0026#39;: 0.1, \u0026#39;n_estimators\u0026#39;: 327, \u0026#39;nthread\u0026#39;: 1, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.75, \u0026#39;num_leaves\u0026#39;: 54 } params = {\u0026#39;reg_alpha\u0026#39; : [0,0.1,0.5,1],\u0026#39;reg_lambda\u0026#39; : [1,2,3,4,6],} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;reg_alpha\u0026#39; : [0.2,0.3,0.4,0.5,0.6,0.7,0.9],\u0026#39;reg_lambda\u0026#39; : [1.5,2,2.5],} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;reg_alpha\u0026#39; : [0.55,0.58,0.6,0.62,0.65,0.68],\u0026#39;reg_lambda\u0026#39; : [1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9],} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;reg_alpha\u0026#39; : [0.61,0.62,0.63,0.64],\u0026#39;reg_lambda\u0026#39; : [1.85,1.88,1.9,1.95,1.98],} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_)   Finally, we can decrease the learning rate to 0.01 and find the optimal value of n_estimators.\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;nthread\u0026#39;: 1, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.75, \u0026#39;num_leaves\u0026#39;: 54, \u0026#39;reg_alpha\u0026#39;: 0.62, \u0026#39;reg_lambda\u0026#39;: 1.9 } params = {\u0026#39;n_estimators\u0026#39;: (3270,3280,3300,3320,3340,3360,3380,3400)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;n_estimators\u0026#39;: (3325,3330,3335,3340,3345,3350,3355)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_) params = {\u0026#39;n_estimators\u0026#39;: (3326,3327,3328,3329,3330,3331,3332,3333,3334)} clf2 = GridSearchCV(lgb.LGBMClassifier(**ind_params), params, cv=5, n_jobs=8, verbose=1) clf2.fit(x_train, y_train) print(clf2.best_params_)   We find the optimal n_estimators to be equal to 3327 for a learning rate of 0.01. We can now built a final LightGBM model using these parameters and evaluate the test data.\nind_params = { \u0026#39;seed\u0026#39;: 32, \u0026#39;learning_rate\u0026#39;: 0.01, \u0026#39;n_estimators\u0026#39;: 3327, \u0026#39;nthread\u0026#39;: 8, \u0026#39;max_depth\u0026#39;: 6, \u0026#39;subsample\u0026#39;: 1.0, \u0026#39;colsample_bytree\u0026#39;: 0.75, \u0026#39;num_leaves\u0026#39;: 54, \u0026#39;reg_alpha\u0026#39;: 0.62, \u0026#39;reg_lambda\u0026#39;: 1.9 } lclf = lgb.LGBMClassifier(**ind_params) lclf.fit(x_train, y_train) lclf.score(x_test, y_test)   We get a test accuracy of 87.01%. Similar to previous cases, we can again look at the accuracy of individual classes using the following confusion matrix plot:\ny_pred = lclf.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    We find the results to be slightly better than XGBoost with 65% accuracy of the less abundant \u0026gt;50K salary class. We can also look the importance of different features in this model:\nimportances = lclf.feature_importances_ indices = np.argsort(importances) cols = [cols[x] for x in indices] plt.figure(figsize=(10,6)) plt.title(\u0026#39;Feature Importances\u0026#39;) plt.barh(range(len(indices)), importances[indices], color=\u0026#39;b\u0026#39;, align=\u0026#39;center\u0026#39;) plt.yticks(range(len(indices)), cols) plt.xlabel(\u0026#39;Relative Importance\u0026#39;)    Concluding Remarks XGBoost has been one of the most famous libraries used to win several machine learning competitions on Kaggle and similar sites. Slowly, LightGBM is also gaining traction due to its speed and parallelization capabilities. CatBoost is another boosting library from Yandex that has been shown to be quite efficient. I have used it personally yet though. If I find it to be worth making a move to, I will write about it in a future post.\n","title":"Understanding Boosted Trees Models","url":"https://sadanand-singh.github.io/posts/boostedtrees/"},{"tags":"Machine Learning, Algorithms, Python","text":"Tree based learning algorithms are quite common in data science competitions. These algorithms empower predictive models with high accuracy, stability and ease of interpretation. Unlike linear models, they map non-linear relationships quite well. Common examples of tree based models are: decision trees, random forest, and boosted trees.\n\n In this post, we will look at the mathematical details (along with various python examples) of decision trees, its advantages and drawbacks. We will find that they are simple and very useful for interpretation. However, they typically are not competitive with the best supervised learning approaches. In order to overcome various drawbacks of decision trees, we will look at various concepts (along with real-world examples in Python) like Bootstrap Aggregating or Bagging, and Random Forests. Another very widely used topic - Boosting will be discussed separately in a future post. Each of these approaches involves producing multiple trees that are combined to yield a single consensus prediction and often resulting in dramatic improvements in prediction accuracy.\nDecision Trees Decision tree is a supervised learning algorithm. It works for both categorical and continuous input (features) and output (predicted) variables. Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.\nLet us first understand decision trees by an example. We will then analyze the process of building decision trees in a formal way. Consider a simple dataset of a loan lending company\u0026rsquo;s customers. We are given Checking Account Balance, Credit History, Length of Employment and Status of Previous Loan for all customers. The task is to predict the risk level of customers - creditable or not creditable. One sample solution for this problem can be depicted using the following decision tree:\n Classification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can used for classification or regression predictive modeling problems. CART is one of the most common algorithms used for generating decision trees. It is used in the scikit-learn implementation of decision trees - sklearn.tree.DecisionTreeClassifier and sklearn.tree.DecisionTreeRegressor for classification and regression, respectively.\nCART Model CART model involves selecting input variables and split points on those variables until a suitable tree is constructed. The selection of which input variable to use and the specific split or cut-point is chosen using a greedy algorithm to minimize a cost function. Tree construction ends using a predefined stopping criterion, such as a minimum number of training instances assigned to each leaf node of the tree.\n Other Decision Tree Algorithms  ID3 Iterative Dichotomiser 3 C4.5 successor of ID3 CHAID Chi-squared Automatic Interaction Detector MARS: extends decision trees to handle numerical data better. Conditional Inference Trees    Regression Trees Let us look at the CART algorithm for regression trees in more detail. Briefly, building a decision tree involves two steps:\n Divide the predictor space - that is, the set of possible values for $X_1, X_2, \\ldots, X_p$ - into $J$ distinct and non-overlapping regions, $R_1, R_2, \\ldots , R_J$ . For every observation that falls into the region $R_j$, make the same prediction, which is simply the mean of the response values for the training observations in $R_j$  In order to construct $J$ regions, $R_1, R_2, \\ldots , R_J$, the predictor space is divided into high-dimensional rectangles or boxes. The goal is to find boxes $R_1, R_2, \\ldots , R_J$ that minimize the RSS, given by\n$$\\sum_{j=1}^{J} \\sum_{i \\in R_j} \\big(y_i - \\hat{y}_{R_j}\\big)^2$$     where, $\\hat{y}_{R_j}$ is the mean response for the training observations within the $j^{th}$ box.\nSince considering every possible such partition of space is computationally infeasible, a greedy approach is used to divide the space, called recursive binary splitting. It is greedy because at each step of the tree building process, the best split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step. Note that all divided regions $R_j \\forall j \\in [1, J]$ would be rectangular.\nIn order to perform recursive binary splitting, first select the predictor $X_j$ and the cut point $s$ such that splitting the predictor space into the regions (half planes) \\(R_1(j,s)=\\big\\{ X|Xj \u0026lt; s \\big\\}\\)    and \\(R_2(j,s)=\\big\\{ X|Xj \\ge s \\big\\}\\)    leads to the greatest possible reduction in RSS. Mathematically, we seek $j$ and $s$ that minimizes,\n$$\\sum_{i: x_i \\in R_1(j,s)} \\big(y_i-\\hat{y}_{R_1}\\big)^2 \u0026#43; \\sum_{i: x_i \\in R_2(j,s)} \\big(y_i-\\hat{y}_{R_2}\\big)^2$$     where $\\hat{y}_{R_1}$ is the mean response for the training observations in $R_1(j,s)$, and $\\hat{y}_{R_2}$ is the mean response for the training observations in $R_2(j,s)$. This process is repeated, looking for the best predictor and best cut point in order to split the data further so as to minimize the RSS within each of the resulting regions. However, this time, instead of splitting the entire predictor space, only one of the two previously identified regions is split. The process continues until a stopping criterion is reached; for instance, we may continue until no region contains more than $m$ observations. Once the regions $R_1, R_2, \\ldots , R_J$ have been created, the response for a given test observation is predicted using the mean of the training observations in the region to which that test observation belongs.\nClassification Trees A classification tree is very similar to a regression tree, except that it is used to predict a qualitative response rather than a quantitative one. Recall that for a regression tree, the predicted response for an observation is given by the mean response of the training observations that belong to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs (i.e. the mode response of the training observations). For the purpose of classification, many a times one is not only interested in predicting the class, rather also in probabilities of being in a given class.\nThe task of growing a classification tree is quite similar to the task of growing a regression tree. Just as in the regression setting, recursive binary splitting is used to grow a classification tree. However, in the classification setting, RSS cannot be used as a criterion for making the binary splits. We can replace RSS by a generic definition of node impurity measure $Q_m$, a measure of the homogeneity of the target variable within the subset regions $R_1, R_2, \\ldots , R_J$. In a node $m$, representing a region $R_m$ with $N_m$ observations, the proportion of training observations in the $m^{th}$ region that are from the $k^{th}$ class can be given by, $$\\hat{p}_{mk} = \\frac{1}{N_m}\\sum_{x_i \\in R_m} I\\big(y_i = k\\big)$$     where, $I\\big(y_i = k\\big)$ is the indicator function that is 1 if $y_i = k$, and 0 otherwise.\nA natural definition of the impurity measure $Q_m$ is the classification error rate. The classification error rate is the fraction of the training observations in that region that do not belong to the most common class: $$E = 1 - \\max_{k}\\hat{p}_{mk}$$     Given this is not differentiable, and hence less amenable to numerical optimization. Furthermore, this is quite insensitive to changes in the node probabilities, making classification error rate quite ineffective for growing trees. Two alternative definitions of node impurity measure that are more commonly used are gini index and cross entropy.\nGini index is a measure of total variance across the $K$ classes, defined as, $$G = \\sum_{k=1}^{K} \\hat{p}_{mk} \\big(1-\\hat{p}_{mk}\\big)$$     A small value of $G$ indicates that a node contains predominantly observations from a single class.\nIn information theory, Cross Entropy is a measure of degree of disorganization in a system. For a binary system, it is 0 if system contains all from the same class , and 1 if system contains equal numbers from the two classes. Hence, similar to Gini Index, Cross Entropy too can be used as a measure of node impurity, given by, $$S = -\\sum_{k=1}^{K} \\hat{p}_{mk} \\log\\big(\\hat{p}_{mk}\\big)$$    \nSimilar to $G$, a small value of $S$ indicates that a node contains predominantly observations from a single class.\nCommon Parameters/Concepts Now, that we understand decision tree mathematically, let us summarize some of the most common terms used in decision trees and tree-based learning algorithms. Understanding these terms should also be helpful in tuning models based on these methods.\n Root Node Represents entire population and further gets divided into two or more sets. Splitting Process of dividing a node into two or more sub-nodes. Decision Node When a sub-node splits into further sub-nodes, then it is called decision node. Leaf/ Terminal Node: Nodes that do not get split. Branch / Sub-Tree A subsection of a tree. Parent and Child Node A node, which is divided into sub-nodes is called parent node of sub-nodes where as sub-nodes are the child of parent node. Minimum samples for a node split Minimum number of samples (or observations) which are required in a node to be considered for splitting. It is used to control over-fitting, higher values prevent a model from learning relations which might be highly specific to the particular sample. It should be tuned using cross validation. Minimum samples for a terminal node (leaf) The minimum number of samples (or observations) required in a terminal node or leaf. Similar to the minimum samples for a node split, this is also used to control over-fitting. For imbalanced class problems, a lower value should be used since regions dominant with samples belonging to minority class will be much smaller in number. Maximum depth of tree (vertical depth) The maximum depth of trees. It is used to control over-fitting, lower values prevent a model from learning relations which might be highly specific to the particular sample. It should be tuned using cross validation. Maximum number of terminal nodes Also referred as number of leaves. Can be defined in place of max_depth. Since binary trees are created, a depth of $n$ would produce a maximum of $2^n$ leaves. Maximum features to consider for split The number of features to consider (selected randomly) while searching for a best split. A typical value is the square root of total number of available features. A higher typically leads to over-fitting but is dependent on the problem as well.  Example of Classification Tree For demonstrating different tree based models, I will be using the US Income dataset available at Kaggle. You should be able to download the data from Kaggle.com. Let us first look at all the different features available in this data set.\nimport pandas as pd import numpy as np from plotnine import * import matplotlib.pyplot as plt from sklearn.preprocessing import LabelEncoder from sklearn_pandas import DataFrameMapper from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier training_data = \u0026#39;./adult-training.csv\u0026#39; test_data = \u0026#39;./adult-test.csv\u0026#39; columns = [\u0026#39;Age\u0026#39;,\u0026#39;Workclass\u0026#39;,\u0026#39;fnlgwt\u0026#39;,\u0026#39;Education\u0026#39;,\u0026#39;EdNum\u0026#39;,\u0026#39;MaritalStatus\u0026#39;, \u0026#39;Occupation\u0026#39;,\u0026#39;Relationship\u0026#39;,\u0026#39;Race\u0026#39;,\u0026#39;Sex\u0026#39;,\u0026#39;CapitalGain\u0026#39;,\u0026#39;CapitalLoss\u0026#39;, \u0026#39;HoursPerWeek\u0026#39;,\u0026#39;Country\u0026#39;,\u0026#39;Income\u0026#39;] df_train_set = pd.read_csv(training_data, names=columns) df_test_set = pd.read_csv(test_data, names=columns, skiprows=1) df_train_set.drop(\u0026#39;fnlgwt\u0026#39;, axis=1, inplace=True) df_test_set.drop(\u0026#39;fnlgwt\u0026#39;, axis=1, inplace=True)   In the above code, we imported all needed modules, loaded both test and training data as data-frames. We also got rid of the fnlgwt column that is of no importance in our modeling exercise.\nLet us look at the first 5 rows of the training data:\ndf_train_set.head()     .dataframe thead tr:only-child th { text-align: right; }\n.dataframe thead th { text-align: left; } .dataframe tbody tr th { vertical-align: top; }     Age Workclass Education EdNum MaritalStatus Occupation Relationship Race Sex CapitalGain CapitalLoss HoursPerWeek Country Income     0 39 State-gov Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u0026lt;=50K   1 50 Self-emp-not-inc Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u0026lt;=50K   2 38 Private HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u0026lt;=50K   3 53 Private 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States \u0026lt;=50K   4 28 Private Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba \u0026lt;=50K     \nWe also need to do some data cleanup. First, I will be removing any special characters from all columns. Furthermore, any space or \u0026ldquo;.\u0026rdquo; characters too will be removed from any str data.\n#replace the special character to \u0026#34;Unknown\u0026#34; for i in df_train_set.columns: df_train_set[i].replace(\u0026#39; ?\u0026#39;, \u0026#39;Unknown\u0026#39;, inplace=True) df_test_set[i].replace(\u0026#39; ?\u0026#39;, \u0026#39;Unknown\u0026#39;, inplace=True) for col in df_train_set.columns: if df_train_set[col].dtype != \u0026#39;int64\u0026#39;: df_train_set[col] = df_train_set[col].apply(lambda val: val.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;)) df_train_set[col] = df_train_set[col].apply(lambda val: val.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;)) df_test_set[col] = df_test_set[col].apply(lambda val: val.replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;)) df_test_set[col] = df_test_set[col].apply(lambda val: val.replace(\u0026#34;.\u0026#34;, \u0026#34;\u0026#34;))   As you can see, there are two columns that describe education of individuals - Education and EdNum. I would assume both of these to be highly correlated and hence remove the Education column. The Country column too should not play a role in prediction of Income and hence we would remove that as well.\ndf_train_set.drop([\u0026#34;Country\u0026#34;, \u0026#34;Education\u0026#34;], axis=1, inplace=True) df_test_set.drop([\u0026#34;Country\u0026#34;, \u0026#34;Education\u0026#34;], axis=1, inplace=True)   Although the Age and EdNum columns are numeric, they can be easily binned and be more effective. We will bin age in bins of 10 and no. of years of education into bins of 5.\ncolnames = list(df_train_set.columns) colnames.remove(\u0026#39;Age\u0026#39;) colnames.remove(\u0026#39;EdNum\u0026#39;) colnames = [\u0026#39;AgeGroup\u0026#39;, \u0026#39;Education\u0026#39;] \u0026#43; colnames labels = [\u0026#34;{0}-{1}\u0026#34;.format(i, i \u0026#43; 9) for i in range(0, 100, 10)] df_train_set[\u0026#39;AgeGroup\u0026#39;] = pd.cut(df_train_set.Age, range(0, 101, 10), right=False, labels=labels) df_test_set[\u0026#39;AgeGroup\u0026#39;] = pd.cut(df_test_set.Age, range(0, 101, 10), right=False, labels=labels) labels = [\u0026#34;{0}-{1}\u0026#34;.format(i, i \u0026#43; 4) for i in range(0, 20, 5)] df_train_set[\u0026#39;Education\u0026#39;] = pd.cut(df_train_set.EdNum, range(0, 21, 5), right=False, labels=labels) df_test_set[\u0026#39;Education\u0026#39;] = pd.cut(df_test_set.EdNum, range(0, 21, 5), right=False, labels=labels) df_train_set = df_train_set[colnames] df_test_set = df_test_set[colnames]   Now that we have cleaned the data, let us look how balanced out data set is:\ndf_train_set.Income.value_counts()   \u0026lt;=50K 24720 \u0026gt;50K 7841 Name: Income, dtype: int64  df_test_set.Income.value_counts()   \u0026lt;=50K 12435 \u0026gt;50K 3846 Name: Income, dtype: int64  In both training and the test data sets, we find \u0026lt;=50K class to be about 3 times larger than the \u0026gt;50K class. This is begging us to treat this problem differently as this is a problem of quite imbalanced data. However, for simplicity we will be treating this exercise as a regular problem.\nEDA Now, let us look at distribution and inter-dependence of different features in the training data graphically.\nLet us first see how Relationships and MaritalStatus features are interrelated.\n(ggplot(df_train_set, aes(x = \u0026#34;Relationship\u0026#34;, fill = \u0026#34;MaritalStatus\u0026#34;)) \u0026#43; geom_bar(position=\u0026#34;fill\u0026#34;) \u0026#43; theme(axis_text_x = element_text(angle = 60, hjust = 1)) )    Let us look at effect of Education (measured in terms of bins of no. of years of education) on Income for different Age groups.\n(ggplot(df_train_set, aes(x = \u0026#34;Education\u0026#34;, fill = \u0026#34;Income\u0026#34;)) \u0026#43; geom_bar(position=\u0026#34;fill\u0026#34;) \u0026#43; theme(axis_text_x = element_text(angle = 60, hjust = 1)) \u0026#43; facet_wrap(\u0026#39;~AgeGroup\u0026#39;) )  \n Recently, there has been a lot of talk about effect of gender based bias/gap in the income. We can look at the effect of Education and Race for males and females separately.\n(ggplot(df_train_set, aes(x = \u0026#34;Education\u0026#34;, fill = \u0026#34;Income\u0026#34;)) \u0026#43; geom_bar(position=\u0026#34;fill\u0026#34;) \u0026#43; theme(axis_text_x = element_text(angle = -90, hjust = 1)) \u0026#43; facet_wrap(\u0026#39;~Sex\u0026#39;) )  \n (ggplot(df_train_set, aes(x = \u0026#34;Race\u0026#34;, fill = \u0026#34;Income\u0026#34;)) \u0026#43; geom_bar(position=\u0026#34;fill\u0026#34;) \u0026#43; theme(axis_text_x = element_text(angle = -90, hjust = 1)) \u0026#43; facet_wrap(\u0026#39;~Sex\u0026#39;) )    Until now, we have only looked at the inter-dependence of non-numeric features. Let us now look at the effect of CapitalGain and CapitalLoss on income.\n(ggplot(df_train_set, aes(x=\u0026#34;Income\u0026#34;, y=\u0026#34;CapitalGain\u0026#34;)) \u0026#43; geom_jitter(position=position_jitter(0.1)) )    (ggplot(df_train_set, aes(x=\u0026#34;Income\u0026#34;, y=\u0026#34;CapitalLoss\u0026#34;)) \u0026#43; geom_jitter(position=position_jitter(0.1)) )    Tree Classifier Now that we understand some relationship in our data, let us build a simple tree classifier model using sklearn.tree.DecisionTreeClassifier. However, in order to use this module, we need to convert all of our non-numeric data to numeric ones. This can be quite easily achieved using the sklearn.preprocessing.LabelEncoder module along with the sklearn_pandas module to apply this on pandas data-frames directly.\nmapper = DataFrameMapper([ (\u0026#39;AgeGroup\u0026#39;, LabelEncoder()), (\u0026#39;Education\u0026#39;, LabelEncoder()), (\u0026#39;Workclass\u0026#39;, LabelEncoder()), (\u0026#39;MaritalStatus\u0026#39;, LabelEncoder()), (\u0026#39;Occupation\u0026#39;, LabelEncoder()), (\u0026#39;Relationship\u0026#39;, LabelEncoder()), (\u0026#39;Race\u0026#39;, LabelEncoder()), (\u0026#39;Sex\u0026#39;, LabelEncoder()), (\u0026#39;Income\u0026#39;, LabelEncoder()) ], df_out=True, default=None) cols = list(df_train_set.columns) cols.remove(\u0026#34;Income\u0026#34;) cols = cols[:-3] \u0026#43; [\u0026#34;Income\u0026#34;] \u0026#43; cols[-3:] df_train = mapper.fit_transform(df_train_set.copy()) df_train.columns = cols df_test = mapper.transform(df_test_set.copy()) df_test.columns = cols cols.remove(\u0026#34;Income\u0026#34;) x_train, y_train = df_train[cols].values, df_train[\u0026#34;Income\u0026#34;].values x_test, y_test = df_test[cols].values, df_test[\u0026#34;Income\u0026#34;].values  \nNow we have training as well testing data in correct format to build our first model!\ntreeClassifier = DecisionTreeClassifier() treeClassifier.fit(x_train, y_train) treeClassifier.score(x_test, y_test)   The simplest possible tree classifier model with no optimization gave us an accuracy of 83.5%. In the case of classification problems, confusion matrix is a good way to judge the accuracy of models. Using the following code we can plot the confusion matrix for any of the tree-based models.\nimport itertools from sklearn.metrics import confusion_matrix def plot_confusion_matrix(cm, classes, normalize=False): \u0026#34;\u0026#34;\u0026#34; This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`. \u0026#34;\u0026#34;\u0026#34; cmap = plt.cm.Blues title = \u0026#34;Confusion Matrix\u0026#34; if normalize: cm = cm.astype(\u0026#39;float\u0026#39;) / cm.sum(axis=1)[:, np.newaxis] cm = np.around(cm, decimals=3) plt.imshow(cm, interpolation=\u0026#39;nearest\u0026#39;, cmap=cmap) plt.title(title) plt.colorbar() tick_marks = np.arange(len(classes)) plt.xticks(tick_marks, classes, rotation=45) plt.yticks(tick_marks, classes) thresh = cm.max() / 2. for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])): plt.text(j, i, cm[i, j], horizontalalignment=\u0026#34;center\u0026#34;, color=\u0026#34;white\u0026#34; if cm[i, j] \u0026gt; thresh else \u0026#34;black\u0026#34;) plt.tight_layout() plt.ylabel(\u0026#39;True label\u0026#39;) plt.xlabel(\u0026#39;Predicted label\u0026#39;)   Now, we can take a look at the confusion matrix of out first model:\ny_pred = treeClassifier.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    We find that the majority class (\u0026lt;=50K Income) has an accuracy of 90.5%, while the minority class (\u0026gt;50K Income) has an accuracy of only 60.8%.\nLet us look at ways of tuning this simple classifier. We can use GridSearchCV() with 5-fold cross-validation to tune various important parameters of tree classifiers.\nfrom sklearn.model_selection import GridSearchCV parameters = { \u0026#39;max_features\u0026#39;:(None, 9, 6), \u0026#39;max_depth\u0026#39;:(None, 24, 16), \u0026#39;min_samples_split\u0026#39;: (2, 4, 8), \u0026#39;min_samples_leaf\u0026#39;: (16, 4, 12) } clf = GridSearchCV(treeClassifier, parameters, cv=5, n_jobs=4) clf.fit(x_train, y_train) clf.best_score_, clf.score(x_test, y_test), clf.best_params_   (0.85934092933263717, 0.85897672133161351, {'max_depth': 16, 'max_features': 9, 'min_samples_leaf': 16, 'min_samples_split': 8})  With the optimization, we find the accuracy to increase to 85.9%. In the above, we can also look at the parameters of the best model. Now, let us have a look at the confusion matrix of the optimized model.\ny_pred = clf.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    With optimization, we find an increase in the prediction accuracy of both classes.\nLimitations of Decision Trees Even though decision tree models have numerous advantages,\n Very simple to understand and easy to interpret Can be visualized Requires little data preparation. Note however that sklearn.tree module does not support missing values. The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree.  These models are NOT common in use directly. Some common drawbacks of decision tree are:\n Can create over-complex trees that do not generalize the data well. Can be unstable because small variations in the data might result in a completely different tree being generated. Practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree. Certain class of functions are difficult to model using tree models, such as XOR, parity or multiplexer.  Most of these limitations can be easily overcome by using several improvements over decision trees. In the following sections, we will be looking some of these concepts, mainly bagging, and random forests.\nTree Pruning Since decision trees have a very high tendency to over-fit the data, a smaller tree with fewer splits (that is, fewer regions $R_1, \\ldots, R_J$) might lead to lower variance and better interpretation at the cost of a little bias. One possible alternative to the process described above is to build the tree only so long as the decrease in the node impurity measure, $Q_m$ due to each split exceeds some (high) threshold. However, due to greedy nature of the splitting algorithm, it is too short-sighted since a seemingly worthless split early on in the tree might be followed by a very good split i.e., a split that leads to a large reduction in $Q_m$ later on.\nTherefore, a better strategy is to grow a very large tree $T_0$, and then prune it back in order to obtain a subtree. There can be several strategies to pruning, Cost complexity pruning, also known as weakest link pruning in one way to do this effectively. Rather than considering every possible subtree, a sequence of trees indexed by a nonnegative tuning parameter $\\alpha$ is considered. For each value of $\\alpha$ there corresponds a subtree $T \\subset T_0$ such that \\[\\sum_{m=1}^{|T|}\\sum_{i:x_i \\in R_m}\\big(y_i-\\hat{y}_{R_m}\\big)^2 + \\alpha |T|\\] is as small as possible. Here $|T|$ indicates the number of terminal nodes of the tree $T$, $R_m$ is the rectangle (i.e. the subset of predictor space) corresponding to the $m^{th}$ terminal node, and $\\hat{y}_{R_m}$ is the predicted response associated with $R_m$, i.e., the mean (or mode in the case of classification trees) of the training observations in $R_m$. The tuning parameter $\\alpha$ controls a trade-off between the subtree’s complexity and its fit to the training data. When $\\alpha = 0$, then the subtree $T$ will simply equal $T_0$. As $\\alpha$ increases, there is a price to pay for having a tree with many terminal nodes, and so the above equation will tend to be minimized for a smaller subtree. The pruning parameter $\\alpha$ can be selected using some kind of cross validation.\nNote that sklearn.tree decision tree classifier (and regressor) does not currently support pruning.\n  Bootstrap Aggregating (Bagging) In statistics, bootstrapping is any test or metric that relies on random sampling with replacement. We saw above that decision trees suffer from high variance. This means that if we split the training data into two parts at random, and fit a decision tree to both halves, the results that we get could be quite different. Bootstrap aggregation, or bagging, is a general-purpose procedure for reducing the variance of a statistical learning method.\nGiven a set of $n$ independent observations $Z_1, Z_2, \\ldots, Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar{Z}$ of the observations is given by $\\sigma^2/n$. In other words, averaging a set of observations reduces variance. Hence a natural way to reduce the variance and hence increase the prediction accuracy of a statistical learning method is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Of there is only one problem here - we do not have access to multiple training data sets. Instead, we can bootstrap, by taking repeated samples from the (single) training data set. In this approach we generate $B$ different bootstrapped training data sets. We then train our method on the $b^{th}$ bootstrapped training set to get a prediction $\\hat{f}^{*b}(x)$ to obtain one aggregate prediction, $$\\hat{f}_{bag} = \\begin{cases}\\frac{1}{B}\\sum_{b=1}^{B} \\hat{f}^{*b}(x) \u0026amp; \\text{ for Regression Problems} \\\\ \\\\ \\mathop{\\arg\\max}\\limits_{b=1 \\ldots B} \\hat{f}^{*b}(x) \u0026amp; \\text{ for Classification Problems} \\end{cases}$$     This is called bagging. Note that aggregating can have different meaning in regression and classification problems. While mean prediction works well in the case of regression problems, we will need to use majority vote: the overall prediction is the most commonly occurring majority class among the B predictions, as aggregation mechanism for classification problems.\nOut-of-Bag (OOB) Error One big advantage of bagging is that we can get testing error without any cross validation!! Recall that the key to bagging is that trees are repeatedly fit to bootstrapped subsets of the observations. One can show that on average, each bagged tree makes use of around 2\u0026frasl;3rd of the observations. The remaining 1\u0026frasl;3rd of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. We can predict the response for the $i^{th}$ observation using each of the trees in which that observation was OOB. This will yield around $B/3$ predictions for the $i^{th}$ observation. Now using the same aggregating techniques as bagging (average for regression and majority vote for classification), we can obtain a single prediction for the $i^{th}$ observation. An OOB prediction can be obtained in this way for each of the n observations, from which the overall OOB MSE (for a regression problem) or classification error (for a classification problem) can be computed. The resulting OOB error is a valid estimate of the test error for the bagged model, since the response for each observation is predicted using only the trees that were not fit using that observation.\nFeature Importance Measures Bagging typically results in improved accuracy over prediction using a single tree. However, it can be difficult to interpret the resulting model. When we bag a large number of trees, it is no longer possible to represent the resulting statistical learning procedure using a single tree, and it is no longer clear which variables are most important to the procedure. Thus, bagging improves prediction accuracy at the expense of interpret-ability.\nInterestingly, one can obtain an overall summary of the importance of each predictor using the RSS (for bagging regression trees) or the Gini index (for bagging classification trees). In the case of bagging regression trees, we can record the total amount that the RSS is decreased due to splits over a given predictor, averaged over all $B$ trees. A large value indicates an important predictor. Similarly, in the context of bagging classification trees, we can add up the total amount that the Gini index is decreased by splits over a given predictor, averaged over all $B$ trees.\nsklearn module\u0026rsquo;s different bagged tree-based learning methods provide direct access to feature importance data as properties once the training has finished.\nRandom Forest Models Even though bagging provides improvement over regular decision tress in terms of reduction in variance and hence improved prediction, it suffers from subtle drawbacks: Bagging requires us to make fully grown trees on bootstrapped samples, thus increasing the computational complexity by $B$ times. Furthermore, since trees in the base of bagging are correlated, the prediction accuracy will get saturated as a function of $B$.\nRandom forests provide an improvement over bagged trees by way of a random small tweak that decorrelates the trees. Unlike bagging, in the case of random forests, as each tree is constructed, only a random sample of predictors is taken before each node is split. Since at the core, random forests too are bagged trees, they lead to reduction in variance. Additionally, random forests also leads to bias reduction since a very large number of predictors can be considered, and local feature predictors can play a role in the tree construction.\nRandom forests are able to work with a very large number of predictors, even more predictors than there are observations. An obvious gain with random forests is that more information may be brought to reduce bias of fitted values and estimated splits.\nThere are often a few predictors that dominate the decision tree fitting process because on the average they consistently perform just a bit better than their competitors. Consequently, many other predictors, which could be useful for very local features of the data, are rarely selected as splitting variables. With random forests computed for a large enough number of trees, each predictor will have at least several opportunities to be the predictor defining a split. In those opportunities, it will have very few competitors. Much of the time a dominant predictor will not be included. Therefore, local feature predictors will have the opportunity to define a split.\nThere are three main tuning parameters of random forests:\n Node Size: Unlike in decision trees, the number of observations in the terminal nodes of each tree of the forest can be very small. The goal is to grow trees with as little bias as possible. Number of Trees: In practice, few hundreds trees is often a good choice. Number of Predictors Sampled: Typically, if there are a total of $D$ predictors, $D/3$ predictors in the case of regression and $\\sqrt{D}$ predictors in the case of classification make a good choice.  Example of Random Forest Model Using the same income data as above, let us make a simple RandomForest classifier model with 500 trees.\nrclf = RandomForestClassifier(n_estimators=500) rclf.fit(x_train, y_train) rclf.score(x_test, y_test)   Even without any optimization, we find the model to be quite close to the optimized tree classifier with a test score of 85.1%. In terms of the confusion matrix, we again find this to be quite comparable to the optimized tree classifier with a prediction accuracy of 92.1% for the majority class (\u0026lt;=50K Income) and a prediction accuracy of 62.6% for the minority class (\u0026gt;50K Income).\ny_pred = rclf.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    As discussed before, random forest models also provide us with a metric of feature importances. We can see importance of different features in our current model as below:\nimportances = rclf.feature_importances_ indices = np.argsort(importances) cols = [cols[x] for x in indices] plt.figure(figsize=(10,6)) plt.title(\u0026#39;Feature Importances\u0026#39;) plt.barh(range(len(indices)), importances[indices], color=\u0026#39;b\u0026#39;, align=\u0026#39;center\u0026#39;) plt.yticks(range(len(indices)), cols) plt.xlabel(\u0026#39;Relative Importance\u0026#39;)    Now, let us try to optimize our random forest model. Again, this can be done using the GridSearchCV() apt with 5-fold cross-validation as below:\nparameters = { \u0026#39;n_estimators\u0026#39;:(100, 500, 1000), \u0026#39;max_depth\u0026#39;:(None, 24, 16), \u0026#39;min_samples_split\u0026#39;: (2, 4, 8), \u0026#39;min_samples_leaf\u0026#39;: (16, 4, 12) } clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, n_jobs=8) clf.fit(x_train, y_train) clf.best_score_, clf.best_params_   0.86606676699118579 {'max_depth': 24, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 1000}  We can see this model to be significantly better than our all previous models, with a prediction rate of 86.6%. In terms of confusion matrix though, we see a significant increase in the prediction accuracy of the majority class (\u0026lt;= 50K Income) with slight decrease in the accuracy for the minority class (\u0026gt;50K Income). This is a common problem with classification problems with imbalanced data.\nrclf2 = RandomForestClassifier(n_estimators=1000,max_depth=24,min_samples_leaf=4,min_samples_split=8) rclf2.fit(x_train, y_train) y_pred = rclf2.predict(x_test) cfm = confusion_matrix(y_test, y_pred, labels=[0, 1]) plt.figure(figsize=(10,6)) plot_confusion_matrix(cfm, classes=[\u0026#34;\u0026lt;=50K\u0026#34;, \u0026#34;\u0026gt;50K\u0026#34;], normalize=True)    Finally, let us also look at the feature importance from the best model.\nimportances = rclf2.feature_importances_ indices = np.argsort(importances) cols = [cols[x] for x in indices] plt.figure(figsize=(10,6)) plt.title(\u0026#39;Feature Importances\u0026#39;) plt.barh(range(len(indices)), importances[indices], color=\u0026#39;b\u0026#39;, align=\u0026#39;center\u0026#39;) plt.yticks(range(len(indices)), cols) plt.xlabel(\u0026#39;Relative Importance\u0026#39;)    We can see the answer to be significantly different than the previous random forest model. This is a common issue with this class of models! In the next post, I will be talking about boosted tree that provide a significant improvement in terms of model consistency.\nLimitations of Random Forests Apart from generic limitations of bagged trees, some of limitations of random forests are:\n Random forests don’t do well at all when you require extrapolation outside of the range of the dependent (or independent) variables - better to use other algorithms like e.g., MARS They are quite slow at both training and prediction. They don’t deal well with a large number of categories in categorical variables.  Overall, Random Forest is usually less accurate than Boosting on a wide range of tasks, and usually slower in the runtime. In the next post, we will look at the details of boosting. I hope this post has helped you understand tree based methods in more detail now. Please let me know what topics I missed or should have been more clear about. You can also let me know in the comments below if there is any particular algorithm/topic that you want me to write about!\n","title":"A Practical Guide to Tree Based Learning Algorithms","url":"https://sadanand-singh.github.io/posts/treebasedmodels/"},{"tags":"Machine Learning, Python","text":"In the previous post on Support Vector Machines (SVM), we looked at the mathematical details of the algorithm. In this post, I will be discussing the practical implementations of SVM for classification as well as regression. I will be using the iris dataset as an example for the classification problem, and a randomly generated data as an example for the regression problem.\n\n In Python, scikit-learn is a widely used library for implementing machine learning algorithms, SVM is also available in scikit-learn library and follow the usual structure (Import library, object creation, fitting model and prediction). The sklearn.svm module provides mainly two classes: sklearn.svm.svc for classification and sklearn.svm.svr for regression.\nPreparing Data for SVM Models As pointed out by Admiral deblue in the comments below, all practical implementations of SVMs have strict requirements for training and testing (prediction). The first requirement is that all data should be numerical. Therefore, if you have categorical features, they need to be converted to numerical values using variable transformation techniques like one-hot-encoding, label-encoding etc. SVM model implementations in python also do not support missing values, hence you need to either remove data with missing values or use some form of data imputing. The sklearn.preprocessing.Imputer module can be quite helpful for this exercise. Furthermore, since SVMs assume that the data it works with is in a standard range, usually either 0 to 1, or -1 to 1 etc. (so that all feature variables are treated equally), it would be best served to use the feature \u0026ldquo;normalization\u0026rdquo; before training the model. The sklearn.preprocessing.StandardScaler module can use used for such normalization.\nIn general, sklearn models require training data (X) to be numpy nd-array and dependent variable (y) as numpy 1-d array. With newer versions of Pandas, Pandas data-frame and series can also be used for providing X and y to sklearn models.\nsklearn.pipeline provides an impressive set of tools to deal with various aspects of data preparation for training different models in a coherent manner. This will be a topic of discussion for a post in near future.\nSVM for Classification Problems The iris dataset is a simple dataset of contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other two; the latter are NOT linearly separable from each other. Each instance has 4 features:\n sepal length sepal width petal length petal width  A typical problem to solve is to predict the class of the iris plant based on these 4 features. For brevity and visualization, in this example we will be using only the first two features.\nSetup Below is the simplest implementation of a SVM for this problem. In this example, we see the simplest implementation of SVM classifier with the linear and the radial basis function (rbf) kernels.\nimport pandas as pd import numpy as np from sklearn import svm, datasets import matplotlib.pyplot as plt %matplotlib inline iris = datasets.load_iris() X = iris.data[:, :2] # we only take the first two features. y = iris.target # Plot resulting Support Vector boundaries with original data # Create fake input data for prediction that we will use for plotting # create a mesh to plot in x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() \u0026#43; 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() \u0026#43; 1 h = (x_max / x_min)/100 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) X_plot = np.c_[xx.ravel(), yy.ravel()] # Create the SVC model object C = 1.0 # SVM regularization parameter svc = svm.SVC(kernel=\u0026#39;linear\u0026#39;, C=C, decision_function_shape=\u0026#39;ovr\u0026#39;).fit(X, y) Z = svc.predict(X_plot) Z = Z.reshape(xx.shape) plt.figure(figsize=(15, 5)) plt.subplot(121) plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1) plt.xlabel(\u0026#39;Sepal length\u0026#39;) plt.ylabel(\u0026#39;Sepal width\u0026#39;) plt.xlim(xx.min(), xx.max()) plt.title(\u0026#39;SVC with linear kernel\u0026#39;) # Create the SVC model object C = 1.0 # SVM regularization parameter svc = svm.SVC(kernel=\u0026#39;rbf\u0026#39;, C=C, decision_function_shape=\u0026#39;ovr\u0026#39;).fit(X, y) Z = svc.predict(X_plot) Z = Z.reshape(xx.shape) plt.subplot(122) plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3) plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1) plt.xlabel(\u0026#39;Sepal length\u0026#39;) plt.ylabel(\u0026#39;Sepal width\u0026#39;) plt.xlim(xx.min(), xx.max()) plt.title(\u0026#39;SVC with RBF kernel\u0026#39;) plt.show()    Parameter Tuning Similar to any machine learning algorithm, we need to choose/tune hyper-parameters for these models. The important parameters to tune are: C (the penalty parameter or the error term. Remember from our last post, this acts as a regularization parameter for SVM) and $\\gamma$ (Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’ kernels). In above example, we used a default value of $\\gamma = \\frac{1}{n_{features}} = 0.5$.\n Multi-class Classification SVM by definition is well suited for binary classification. In order to perform multi-class classification, the problem needs to be transformed into a set of binary classification problems.\nThere are two approaches to do this:\nOne vs. Rest Approach (OvR): This strategy involves training a single classifier per class, with the samples of that class as positive samples and all other samples as negatives. This strategy requires the base classifiers to produce a real-valued confidence score for its decision, rather than just a class label; discrete class labels alone can lead to ambiguities, where multiple classes are predicted for a single sample.\nOne vs. One Approach (OvO): In the one-vs.-one (OvO) strategy, one trains K(K − 1)/2 binary classifiers for a K-way multi-class problem; each receives the samples of a pair of classes from the original training set, and must learn to distinguish these two classes. At prediction time, a voting scheme is applied: all K(K − 1)/2 classifiers are applied to an unseen sample and the class that got the highest number of \u0026ldquo;+1\u0026rdquo; predictions gets predicted by the combined classifier. Like OvR, OvO suffers from ambiguities in that some regions of its input space may receive the same number of votes.\nIn svm.svc implementation, decision_function_shape parameter provides the option to choose one of two strategy. Although, by default OvO strategy is chosen for historical reasons, it is always recommended to switch to the OvR approach.\n  Let us first understand what effects $C$ and $\\gamma$ parameters have on SVM models. As seen below, we find that higher the value of $\\gamma$, it will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem. $C$ controls the trade off between smooth decision boundary and classifying the training points correctly.\n  We will be using 5-fold cross validation to perform grid search to calculate optimal hyper-parameters. This is easily achieved in scikit-learn using the sklearn.model_selection.GridSearchCV class.\nfrom sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV from sklearn.metrics import classification_report from sklearn.utils import shuffle # shuffle the dataset X, y = shuffle(X, y, random_state=0) # Split the dataset in two equal parts X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=0) # Set the parameters by cross-validation parameters = [{\u0026#39;kernel\u0026#39;: [\u0026#39;rbf\u0026#39;], \u0026#39;gamma\u0026#39;: [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5], \u0026#39;C\u0026#39;: [1, 10, 100, 1000]}, {\u0026#39;kernel\u0026#39;: [\u0026#39;linear\u0026#39;], \u0026#39;C\u0026#39;: [1, 10, 100, 1000]}] print(\u0026#34;# Tuning hyper-parameters\u0026#34;) print() clf = GridSearchCV(svm.SVC(decision_function_shape=\u0026#39;ovr\u0026#39;), parameters, cv=5) clf.fit(X_train, y_train) print(\u0026#34;Best parameters set found on development set:\u0026#34;) print() print(clf.best_params_) print() print(\u0026#34;Grid scores on training set:\u0026#34;) print() means = clf.cv_results_[\u0026#39;mean_test_score\u0026#39;] stds = clf.cv_results_[\u0026#39;std_test_score\u0026#39;] for mean, std, params in zip(means, stds, clf.cv_results_[\u0026#39;params\u0026#39;]): print(\u0026#34;%0.3f (\u0026#43;/-%0.03f) for %r\u0026#34; % (mean, std * 2, params)) print()  \nOutput:\n# Tuning hyper-parameters Best parameters set found on development set: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'} Grid scores on training set: 0.634 (+/-0.066) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'} 0.634 (+/-0.066) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'} 0.634 (+/-0.066) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'} 0.768 (+/-0.168) for {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'} 0.768 (+/-0.161) for {'C': 1, 'gamma': 0.2, 'kernel': 'rbf'} 0.768 (+/-0.173) for {'C': 1, 'gamma': 0.5, 'kernel': 'rbf'} 0.634 (+/-0.066) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'} 0.634 (+/-0.066) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'} 0.768 (+/-0.168) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'} 0.750 (+/-0.193) for {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'} 0.750 (+/-0.193) for {'C': 10, 'gamma': 0.2, 'kernel': 'rbf'} 0.732 (+/-0.183) for {'C': 10, 'gamma': 0.5, 'kernel': 'rbf'} 0.634 (+/-0.066) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'} 0.768 (+/-0.168) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'} 0.759 (+/-0.178) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'} 0.741 (+/-0.164) for {'C': 100, 'gamma': 0.1, 'kernel': 'rbf'} 0.723 (+/-0.175) for {'C': 100, 'gamma': 0.2, 'kernel': 'rbf'} 0.732 (+/-0.183) for {'C': 100, 'gamma': 0.5, 'kernel': 'rbf'} 0.768 (+/-0.168) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'} 0.759 (+/-0.178) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'} 0.750 (+/-0.193) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'} 0.732 (+/-0.183) for {'C': 1000, 'gamma': 0.1, 'kernel': 'rbf'} 0.732 (+/-0.183) for {'C': 1000, 'gamma': 0.2, 'kernel': 'rbf'} 0.696 (+/-0.164) for {'C': 1000, 'gamma': 0.5, 'kernel': 'rbf'} 0.768 (+/-0.173) for {'C': 1, 'kernel': 'linear'} 0.759 (+/-0.178) for {'C': 10, 'kernel': 'linear'} 0.759 (+/-0.178) for {'C': 100, 'kernel': 'linear'} 0.759 (+/-0.178) for {'C': 1000, 'kernel': 'linear'}  We have done a few things in above code. Let us break down these in steps.\nFirst, if you pay attention to the input dataset, it lists three different class of iris plants in order. In order for models to be forgetful about such an order, its safer to first shuffle the dataset. This is achieved using the shuffle() method. We also want to take aside a fraction of dataset for final testing of our algorithms success. This is done using the train_test_split() method. In this particular case, we have kept aside about 1\u0026frasl;4 th of the dataset for testing.\nMoving to the main part of the code: tuning of hyper-parameters for SVM. It is done using the GridSearchCV() class (The highlighted lines in the above code blocks). At the end, we are also printing out the accuracy score for different set of parameters. We can find the best set of parameters by the clf.best_params_ property.\nClassification Scoring By default scikit-learn uses accuracy as score for classification tasks. GridSearchCV() provides option to use alternative scoring metrics via the scoring parameter. Some common alternatives are, precision, recall, auc with different averaging strategies like micro, macro, weighted etc.  Finally, we can test our model on the test dataset and evaluate various classification metrics using the classification_report() method.\nprint(\u0026#34;Detailed classification report:\u0026#34;) print() print(\u0026#34;The model is trained on the full development set.\u0026#34;) print(\u0026#34;The scores are computed on the full evaluation set.\u0026#34;) print() y_true, y_pred = y_test, clf.predict(X_test) print(classification_report(y_true, y_pred)) print()   Output:\nDetailed classification report: The model is trained on the full development set. The scores are computed on the full evaluation set. precision recall f1-score support 0 1.00 1.00 1.00 12 1 0.73 0.92 0.81 12 2 0.91 0.71 0.80 14 avg / total 0.88 0.87 0.87 38  Apart from accuracy, three major metrics to understand the task for classification are: precision, recall and f1-score.\nPrecision: The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\nRecall: The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\nF1-Score: It can be interpreted as a weighted harmonic mean of the precision and recall, where an f1-score reaches its best value at 1 and worst score at 0.\nSupport: Although not a scoring metric, it is an important quantity when looking at different metrics. It is the number of occurrences of each class in y_true.\nSVM for Regression Problems Let us first generate a random dataset where we want to generate a regression model. In order to have a good visualization of our results, it would be best to use a single feature as an example. In order to study effect of non-linear models, we will be generating our data from the sin() function.\nX = np.sort(5 * np.random.rand(200, 1), axis=0) y = np.sin(X).ravel() y[::5] \u0026#43;= 3 * (0.5 - np.random.rand(40))   Setup Below is the simplest implementation of a SVM for this regression problem. In sci-kit learn SVM regression models are implemented using the svm.SVR class.\nIn this example, we see the simplest implementation of SVM regressors with the linear, polynomial of degree 3 and the radial basis function (rbf) kernels.\nfrom sklearn.svm import SVR svr_rbf = SVR(kernel=\u0026#39;rbf\u0026#39;, C=1e3, gamma=0.1) svr_lin = SVR(kernel=\u0026#39;linear\u0026#39;, C=1e3) svr_poly = SVR(kernel=\u0026#39;poly\u0026#39;, C=1e3, degree=3) y_rbf = svr_rbf.fit(X, y).predict(X) y_lin = svr_lin.fit(X, y).predict(X) y_poly = svr_poly.fit(X, y).predict(X) lw = 2 plt.figure(figsize=(12, 7)) plt.scatter(X, y, color=\u0026#39;darkorange\u0026#39;, label=\u0026#39;data\u0026#39;) plt.plot(X, y_rbf, color=\u0026#39;navy\u0026#39;, lw=lw, label=\u0026#39;RBF model\u0026#39;) plt.plot(X, y_lin, color=\u0026#39;c\u0026#39;, lw=lw, label=\u0026#39;Linear model\u0026#39;) plt.plot(X, y_poly, color=\u0026#39;cornflowerblue\u0026#39;, lw=lw, label=\u0026#39;Polynomial model\u0026#39;) plt.xlabel(\u0026#39;data\u0026#39;) plt.ylabel(\u0026#39;target\u0026#39;) plt.title(\u0026#39;Support Vector Regression\u0026#39;) plt.legend() plt.show()   Output:  Parameter Tuning The common hyper-parameters in the case of SVM regressors are: $C$ (the error term), $\\epsilon$ (specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value) and $\\gamma$ (Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’ kernels). Given our example is extremely simplified, we won\u0026rsquo;t be able to observe any significant impact of any of these parameters. In general, similar to classification case, GridSearchCV can be used to tune SVM regression models as well.\nConcluding Remarks So that brings us to an end to the different aspects of Support Vector Machine algorithms. In the first post on the topic, I described the theory and the mathematical formulation of the algorithm. In this post, I discussed the implementation details in Python and ways to tune various hyper-parameters in both classification and regression cases. From practical experience, SVMs are great for:\n Small to medium data sets only. Training becomes extremely slow in the case of larger datasets. Data sets with low noise. When the data set has more noise i.e. target classes are overlapping, SVM perform very poorly. When feature dimensions are very large. SVMs are extremely helpful specially when no. of features is larger than no. of samples. Since only a subset of training points are used in the decision function (called support vectors), it is quite memory efficient. This also leads to extremely fast prediction.  An important point to note is that the SVM doesn’t directly provide probability estimates, these are calculated using an expensive cross-validation in scikit-learn implementation.\nFinally, as would be the case with any machine learning algorithm, I would suggest you to use SVM and analyze the power of SVMs by tuning various hyper-parameters. I want to hear your experience with SVM, how have you tuned SVM models to avoid over-fitting and reduce the training time? Please share your views and experiences in the comments below.\n","title":"Understanding Support Vector Machine via Examples","url":"https://sadanand-singh.github.io/posts/svmpython/"},{"tags":"Blog, Hugo, golang","text":"I have been using Nikola to build this Blog. Its a great static site build system that is based on Python. However, It has some crazy amount of dependencies (to have reasonable looking site). It uses restructured text (rst) as the primary language for content creation. Personally, I use markdown for almost every thing else - taking notes, making diary, code documentation etc. Furthermore, given Nikola tries to support almost everything in a static site builder, lately its is becoming more and more bloated.\n\nCase in point, recently it got support for shortcodes and although that did enable me to write posts in Markdown, but it is so difficult to develop them (It does not help to have almost no documentation/guide for their development). They are heavily tied to the plugin system with light support for template based shortcodes.\nI really have nothing against Nikola. However, I did not feel at home - I wanted a light system that focused on markdown and had all the flexibilities that I wanted. My research soon brought me to Hugo.\n Hugo - the blazing fast site generator Hugo is a light weight, fast and modern static website engine written in go. It literally takes just milliseconds to build your entire site. For the given lightness, it is highly flexible as well. You can organize your content however you want with any URL structure, group your content using your own indexes and categories and define your own metadata in any format: YAML, TOML or JSON. I was impressed! Keep in my mind, python is still my primary language of programming for scripting and machine learning. And, I have almost no programming experience with go.\nI chose YAML for all the configuration as well as metadata. In my next step to make this move, I had to choose the theme for my blog. If you have been following me, I have been using several flavors of Bootswatch themes. So my first goal was implement my heavily modified version of bootswatch theme in Hugo.\nBootswatch Theme Developing a theme from scratch (Well, the implementation from scratch, as I all I am trying to do is mimic/improvise my current theme from Nikola) turned out to be a great adventure and learning exercise. It helped me understand the Hugo architecture in great detail. It did help to have some good documentation written for developers, not users! Although, Hugo\u0026rsquo;s documentation can surely help itself with some cleaning and some fresher looks!\nHugo uses go templates with many extra functions and set of variables provided by Hugo. I personally feel Hugo\u0026rsquo;s template-ing system to be more flexible and easier than Mako - the one used by default by Nikola.\nI converted almost all of Mako theme from Nikola website to Hugo\u0026rsquo;s format and architecture with additions (copied features and code) from a nice theme called TranqilPeak. In particular, I liked their fonts, search feature for taxonomies pages. Copying these features also meant I had to learn a bit of javascript and css. You can find a working copy of my theme in the src branch of gihub repository of my blog. I plan to release this theme as a standalone theme in near future though.\nShortcodes Given I am using a bootstrap based theme, I like having a lot of its features available to me when I am writing in Markdown. The powerful template based shortcodes in Hugo provide a great way to write custom HTML code inside markdown. I feel Hugo shortcodes are so powerful, you could develop your own grammar of markup language in it! 😋\nSome of my shortcodes are basically based on bootstrap classes like panel, label, emphasis, highlighted text, and block quotes. I also liked the figure command provided by restructured text in Nikola. Luckily, same features are available in Hugo using a default shortcode called figure. Hugo also provides several other useful default shortcodes like youtube, ref/relref for referencing other posts etc.\nI have also some additional shortcodes for code-blocks and math. I will be detailing about them in a bit more detail in the next section. All of my shortcodes are available with the theme in the same github repo.\nOther Caveats and Fixes While converting to Hugo was fun, there were some caveats. The issues I faced were mainly with home page, site search, and ipython notebook posts.\nHome Page with Content and Post Lists Getting home page to work was very simple. Hugo documentation page provides a very clear details about order in which various templates are looked. For home page, you will need to provide a template for index.html. Then Inside the content folder, you can put the metadata and the content for the home page in a file named _index.md.\nI also added following template code in the index.html template to get list of posts with machine learning related tags:\n{{ $.Scratch.Add \u0026#34;mlposts\u0026#34; slice }} {{ $tags := (slice \u0026#34;Machine Learning\u0026#34; \u0026#34;EDA\u0026#34; \u0026#34;Kaggle\u0026#34; \u0026#34;ML\u0026#34; \u0026#34;Deep Learning\u0026#34; \u0026#34;DL\u0026#34; \u0026#34;Data Science\u0026#34;) }} {{ range .Site.RegularPages }} {{ $page := . }} {{ $has_common_tags := intersect $tags .Params.tags | len | lt 0 }} {{ if $has_common_tags }} {{ $.Scratch.Add \u0026#34;mlposts\u0026#34; $page }} {{ end }} {{ end }} {{ $cand := .Scratch.Get \u0026#34;mlposts\u0026#34; }} {{ range first 10 $cand }} {{ .Render \u0026#34;li\u0026#34;}} {{ end }}   tipue Search Hugo has support for several output formats, including HTML and JSON. For implementing tipue search, we need to generate a JSON file with site content. This can be done by adding following to the configuration file:\n# Output formats outputs: home: [ \u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] page: [ \u0026#34;HTML\u0026#34;]   and, using the following index.json template:\n{{- $.Scratch.Add \u0026#34;index\u0026#34; slice -}} {{- range where .Site.RegularPages \u0026#34;Type\u0026#34; \u0026#34;not in\u0026#34; (slice \u0026#34;page\u0026#34; \u0026#34;json\u0026#34; \u0026#34;nosearch\u0026#34;) -}} {{- $.Scratch.Add \u0026#34;index\u0026#34; (dict \u0026#34;url\u0026#34; .Permalink \u0026#34;title\u0026#34; .Title \u0026#34;text\u0026#34; .Plain \u0026#34;tags\u0026#34; (delimit .Params.tags \u0026#34;, \u0026#34;)) -}} {{- end -}} {\u0026#34;pages\u0026#34;: {{- $.Scratch.Get \u0026#34;index\u0026#34; | jsonify -}}}   Now, include the following css in the \u0026lt;head\u0026gt; of your pages:\n\u0026lt;link href=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt;   The following modal code is needed to display the search results, preferably at the end of the body of the HTMLpage:\n\u0026lt;div id=\u0026#34;search-resuts\u0026#34; class=\u0026#34;modal fade\u0026#34; role=\u0026#34;dialog\u0026#34; style=\u0026#34;height: 80%;\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-dialog\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;modal-header\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;close\u0026#34; data-dismiss=\u0026#34;modal\u0026#34;\u0026gt;×\u0026lt;/button\u0026gt; \u0026lt;h4 class=\u0026#34;modal-title\u0026#34;\u0026gt;Search Results:\u0026lt;/h4\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;modal-body\u0026#34; id=\u0026#34;tipue_search_content\u0026#34; style=\u0026#34;max-height: 600px; overflow-y: auto;\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;modal-footer\u0026#34;\u0026gt; \u0026lt;button type=\u0026#34;button\u0026#34; class=\u0026#34;btn btn-default\u0026#34; data-dismiss=\u0026#34;modal\u0026#34;\u0026gt;Close\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt;   Finally, the following javascript in the lower end of the body of HTML pages:\n\u0026lt;script\u0026gt; $(document).ready(function() { var url1 = \u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch_set.js\u0026#34;; var url2 = \u0026#34;https://cdnjs.cloudflare.com/ajax/libs/Tipue-Search/5.0.0/tipuesearch.min.js\u0026#34;; $.when( $.getScript( url1 ), $.getScript( url2 ), $.Deferred(function( deferred ){ $( deferred.resolve ); }) ).done(function() { $(\u0026#39;#tipue_search_input\u0026#39;).tipuesearch({ \u0026#39;mode\u0026#39;: \u0026#39;json\u0026#39;, \u0026#39;contentLocation\u0026#39;: \u0026#39;/index.json\u0026#39; }); $(\u0026#39;#tipue_search_input\u0026#39;).keyup(function (e) { if (e.keyCode == 13) { $(\u0026#39;#search-results\u0026#39;).modal() } }); }); }); \u0026lt;/script\u0026gt;   And, of course you will need a form/input for performing the search:\n\u0026lt;span class=\u0026#34;navbar-form navbar-right\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;tipue_search_input\u0026#34; class=\u0026#34;form-control\u0026#34; placeholder=\u0026#34;Search\u0026#34;\u0026gt; \u0026lt;/span\u0026gt;   Code Highlighting Although, by default Hugo provides code highlighting using the pygments, I prefer to use client-side highlighting using prism.js. I also use the following plugins of prism.js for line numbers, highlighting and cleanup of white space:\n Line Highlight Line Numbers Normalize Whitespace  Finally, I create a shortcode called code-block to add relevant classes and variables around \u0026lt;code\u0026gt; and \u0026lt;pre\u0026gt; tags so that prism could highlight code correctly.\njupyter Notebooks as Posts One of the advantages of using Nikola is that, it provides native support for writing Blog posts in jupyter notebooks.\nBut, on some Google search, I found this neat solution.\nIn summary, the setup is very simple - Use the linked jupyter.css file in your template, then add this css file to relevant pages. I do the this based on a metadata variable notebook: true via the following template code:\n{{ if .Params.notebook }} \u0026lt;link href=\u0026#34;{{ $.Site.BaseURL }}css/jupyter.css\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; {{ end }}   Then for any jupyter notebook, convert it to basic HTML using the following command:\njupyter nbconvert --to html --template basic *source_file.ipynb*   Finally, create a markdown file for your post to put the contents of this HTML file. Works like charm since markdown supports including raw HTML code!\nLatex Math Equations I used katex for using math in markdown. I was having some issue with the multi-line display math equations, so I created a shortcode called tex to write HTML code explicitly so that katex could handle that easily.\nI added following code in the \u0026lt;head\u0026gt; of all of posts:\n{{ if .Params.hasMath }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css\u0026#34; integrity=\u0026#34;sha384-wITovz90syo1dJWVh32uuETPVEtGigN07tkttEqPv\u0026#43;uR2SE/mbQcG7ATL28aI9H0\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; {{ end }}   And the following script at the end of the \u0026lt;body\u0026gt; section:\n{{ if .Params.hasMath }} \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js\u0026#34; integrity=\u0026#34;sha384-/y1Nn9\u0026#43;QQAipbNQWU65krzJralCnuOasHncUFXGkdwntGeSvQicrYkiUBwsgUqc1\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha256-ExtbCSBuYA7kq1Pz362ibde9nnsHYPt6JxuxYeZbU\u0026#43;c=\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; renderMathInElement(document.body, { delimiters: [ {left: \u0026#34;\\\\\\\\begin{equation*}\u0026#34;, right: \u0026#34;\\\\\\\\end{equation*}\u0026#34;, display: true}, {left: \u0026#34;$$\u0026#34;, right: \u0026#34;$$\u0026#34;, display: true}, {left: \u0026#34;\\\\\\[\u0026#34;, right: \u0026#34;\\\\\\]\u0026#34;, display: true}, {left: \u0026#34;$\u0026#34;, right: \u0026#34;$\u0026#34;, display: false}, {left: \u0026#34;\\\\\\(\u0026#34;, right: \u0026#34;\\\\\\)\u0026#34;, display: false} ] } ); \u0026lt;/script\u0026gt; {{ end }}   Now, whenever, I need to add math equations in a post, enable the hasMath: true parameter in its metadata.\nSo there you have it. I have my Blog now up and running with Hugo. Hope I will be more active here, since it now takes only seconds to deploy once I have a post written. No excuses now! 😜\n","title":"Switching to Hugo from Nikola","url":"https://sadanand-singh.github.io/posts/nikola2hugo/"},{"tags":"Data Science, EDA","text":" One of the first tasks involved in any data science project is to get to understand the data. This can be extremely beneficial for several reasons:\n Catch mistakes in data See patterns in data Find violations of statistical assumptions Generate hypotheses etc.  We can think of this task as an exercise in summarization of the data. To summarize the main characteristics of the data, often two methods are used: numerical and graphical.\nThe numerical summary of data is done through descriptive statistics. While the graphical summary of the data is done through exploratory data analysis (EDA). In this post, we will look at both of these fundamental data science techniques in more detail using some examples.\n Descriptive Statistics Descriptive statistics are statistics that quantitatively describe or summarize features of a collection of information. Some measures that are commonly used to describe a data set are:\n Measures of Central Tendency or Measure of Location, such as mean Measures of Variability or Dispersion, such as standard deviation Measure of the shape of the distribution, such as skewness or kurtosis Relative Standing Measures, such as z-score, Quartiles etc.  Measures of Central Tendency Central tendency (or measure of central tendency) is a central or typical value for a probability distribution. Measures of central tendency are often called averages. The most common measures of central tendency are the arithmetic mean, the median and the mode.\nMean The arithmetic mean (or mean or average) is the most commonly used and readily understood measure of central tendency. In statistics, however, the term average refers to any of the measures of central tendency. If we have a data set containing the values \\(a_{1},a_{2},\\ldots ,a_{n}\\)   , then the arithmetic mean, \\(A\\)    is defined by the formula:\n$$A = \\frac{1}{n}\\sum_{i=1}^{n} a_i = \\frac{a_1 \u0026#43; a_2 \u0026#43; \\ldots \u0026#43; a_n}{n}$$     If the data set is a statistical population (i.e., consists of every possible observation and not just a subset of them), then the mean of that population is called the population mean. If the data set is a statistical sample (a subset of the population), we call the statistic resulting from this calculation a sample mean.\nAlthough, arithmetic mean is the most common definition of mean, several other types are means also common. Some examples are: Weighted mean, Geometric mean, Harmonic mean and Trimmed mean etc.\nMedian The median is the midpoint of the data set. This midpoint value is the point at which half the observations are above the value and half the observations are below the value. The median is determined by ranking the observations and finding the observation that are at the number \\(\\frac{[N \u0026#43; 1]}{2}\\)    in the ranked order. If the number of observations are even, then the median is the average value of the observations that are ranked at numbers \\(\\frac{[N]}{2}\\)    and \\(\\frac{[N \u0026#43; 1]}{2} \u0026#43; 1\\)   .\nMean vs Median The median and the mean both measure central tendency. But unusual values, called outliers, affect the median less than they affect the mean. When you have unusual values, you can compare the mean and the median to decide which is the better measure to use. If your data are symmetric, the mean and median are similar.  The concept of median can be generalized as quartiles. Quartiles are the three values – the first quartile at 25% ($Q_1$), the second quartile at 50% ($Q_2$ or median), and the third quartile at 75% ($Q_3$) – that divide a sample of ordered data into four equal parts.\nMode The mode is the value that appears most often in a set of data. The mode of a discrete probability distribution is the value x at which its probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled.\nThe mode can be used with mean and median to provide an overall characterization of your data distribution. The mode can also be used to identify problems in your data. For example, a distribution that has more than one mode may identify that your sample includes data from two populations. If the data contain two modes, the distribution is bimodal. If the data contain more than two modes, the distribution is multi-modal.\nMinimum and Maximum Many a times looking at the smallest and largest data and their relative positioning wrt to other central tendencies are also quite helpful.\nUse the maximum/minimum to identify a possible outliers or any data- entry errors. One of the simplest ways to assess the spread of your data is to compare the minimum and maximum. If the maximum value is very high, even when you consider the center, the spread, and the shape of the data, investigate the cause of the extreme value.\nMeasures of Variability or Dispersion Dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched. A measure of statistical dispersion is a nonnegative real number that is zero if all the data are the same and increases as the data become more diverse. Some common examples of dispersion measures are: Standard Deviation, Interquartile Range (IQR), Mean Absolute Difference and Median Absolute Difference etc.\nStandard Deviation The standard deviation is a measure of how spread out the data are about the mean. The symbol $\\sigma$ is often used to represent the standard deviation of a population, while $s$ is used to represent the standard deviation of a sample.\nIf we have a data set containing the values \\(a_{1},a_{2},\\ldots ,a_{n}\\)   , then the standard deviation, $\\sigma$ is defined by the formula:\n$$\\sigma = \\sqrt{\\frac{1}{n}\\Big[\\big(a_1 - A\\big)^2 + \\big(a_2 - A\\big)^2 + \\ldots + \\big(a_n - A\\big)^2\\Big]}, \\text{ where } A \\text{ is the Mean}$$\nA higher standard deviation value indicates greater spread in the data. A good rule of thumb for a [normal distribution][normal] is that approximately 68% of the values fall within one standard deviation of the mean, 95% of the values fall within two standard deviations, and 99.7% of the values fall within three standard deviations.\nInterquartile Range (IQR) The interquartile range (IQR) is the distance between the first quartile ($Q_1$) and the third quartile ($Q_3$). 50% of the data are within this range.\n$$IQR = Q_3 - Q_1$$\nThe interquartile range can be used to describe the spread of the data. As the spread of the data increases, the IQR becomes larger. It is also used to build box plots.\nRange The range is the difference between the largest and smallest data values in the sample. The range represents the interval that contains all the data values.\nThe range can be used to understand the amount of dispersion in the data. A large range value indicates greater dispersion in the data. A small range value indicates that there is less dispersion in the data. Because the range is calculated using only two data values, it is more useful with small data sets.\nMeasure of the Shape of the Distribution Generally speaking, a moment is a specific quantitative measure, used in both mechanics and statistics, of the shape of a set of points. If the points represent probability density, then the zeroth moment is the total probability (i.e. one), the first moment is the mean, the second central moment is the variance, the third central moment is the skewness, and the fourth central moment (with normalization and shift) is the kurtosis.\nWe have already seen the use of first and second moments in describing statistics. The shape of distributions are further described using higher moments as described below.\nSkewness skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its measure of central tendency. The skewness value can be positive or negative, or even undefined.\nFor a unimodal distribution, negative skew indicates that the tail on the left side of the probability density function is longer or fatter than the right side – it does not distinguish these two kinds of shape. Conversely, positive skew indicates that the tail on the right side is longer or fatter than the left side. In multi-modal distributions and discrete distributions, skewness is very difficult to interpret.\nThere are two common definitions of skewness:\nA. Pearson Moment Coefficient of Skewness: Pearson Moment Coefficient of Skewness refers to the third standardized moment, defined as:\n$$S_{Pearson}=\\frac{E\\big[X^3\\big]-3\\mu\\sigma^2-\\mu^3}{\\sigma^3}$$\nwhere, $\\mu$ is the mean, $\\sigma$ is the standard deviation, $E$ is the expectation operator, and $X$ refers to the data points.\nB. Bowley Skewness:\nBowley skewness is a way to measure skewness purely from quartiles. One of the most popular ways to find skewness is the Pearson Mode Skewness formula. However, in order to use it you must know the mean, mode and standard deviation for your data. Sometimes you might not have that information; Instead you might have information about your quartiles.\nBowley skewness is an important quantity, if you have extreme data values (outliers) or if you have an open-ended distribution.\nMathematically, Bowley Skewness is defined as :\n$$S_{Bowley} = \\frac{Q_3 + Q_1 - 2Q_2}{Q_3 - Q_1}$$\nwhere, $Q_1$, $Q_2$ and $Q_3$, represent, first, second and third quartiles, respectively. Bowley Skewness is an absolute measure of skewness. In other words, it’s going to give you a result in the units that your distribution is in. That’s compared to the Pearson Mode Skewness, which gives you results in a dimensionless unit — the standard deviation. This means that you cannot compare the skewness of different distributions with different units using Bowley Skewness.\nKurtosis Kurtosis indicates how the peak and tails of a distribution differ from the normal distribution. Mathematically, it is the fourth standardized moment, defined as,\n$$Kurtosis = \\frac{E\\Big[\\big(X-\\mu\\big)^4\\Big]}{\\sigma^4} - 3$$\nwhere, $\\mu$ is the mean, $\\sigma$ is the standard deviation, $E$ is the expectation operator, and $X$ refers to the data points.\nUse kurtosis to initially understand general characteristics about the distribution of your data. Normally distributed data establish the baseline for kurtosis. A kurtosis value of 0 indicates that the data follow the normal distribution perfectly. A kurtosis value that significantly deviates from 0 may indicate that the data are not normally distributed.\nA distribution that has a positive kurtosis value indicates that the distribution has heavier tails and a sharper peak than the normal distribution. For example, data that follow a t-distribution have a positive kurtosis value.\nA distribution with a negative kurtosis value indicates that the distribution has lighter tails and a flatter peak than the normal distribution. For example, data that follow a beta distribution with first and second shape parameters equal to 2 have a negative kurtosis value.\nMeasures of Relative Standing A measure of relative standing is a measure of where a data value stands relative to the distribution of the whole data set. With an idea of relative standing, we can say things like, “You got a really high score compared to the rest of the class” or, “that basketball player is unusually short” etc. Some of the common measures of relative standings are: z-score, quartile and percentile.\nz-scores The z-score (or standard score) is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. Observed values above the mean have positive standard scores, while values below the mean have negative standard scores.\nMathematically, z-score of a raw score $x$ is given by,\n$$z = \\frac{x - \\mu}{\\sigma}$$\nwhere, $\\mu$ is the mean and $\\sigma$ is the standard deviation of the population.\nThe z-score is often used in the z-test in standardized testing – the analog of the Student\u0026rsquo;s t-test for a population whose parameters are known, rather than estimated. As it is very unusual to know the entire population, the t-test is much more widely used.\nQuartiles and Percentiles A percentile is a measure used in statistics indicating the value below which a given percentage of observations in a group of observations fall. For example, the 20th percentile is the value (or score) below which 20 percent of the observations may be found. The term percentile and the related term, percentile rank, are often used in the reporting of scores from norm-referenced tests. For example, if a score is in the 86th percentile, it is higher than 86% of the other scores. The 25th percentile is also known as the first quartile ($Q_1$), the 50th percentile as the median or second quartile ($Q_2$), and the 75th percentile as the third quartile ($Q_3$).\nCorrelations Often the data that we deal with is multi-dimensional in nature. Correlation most often refers to the extent to which two variables have a linear relationship with each other. Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.\nThe most familiar measure of dependence between two quantities is the Pearson product-moment correlation coefficient, or \u0026ldquo;Pearson\u0026rsquo;s correlation coefficient\u0026rdquo;, commonly called simply \u0026ldquo;the correlation coefficient\u0026rdquo;.\nThe population correlation coefficient $\\rho_{X, Y}$ between two variates $X$ and $Y$ with means $\\mu_X$ and $\\mu_Y$ and standard deviations $\\sigma_X$ and $\\sigma_Y$ is defined as:\n$$ \\rho_{X, Y} = \\frac{cov(X, Y)}{\\sigma_X \\sigma_Y} = \\frac{E\\Big[\\big(X-\\mu_X\\big)\\big(Y-\\mu_Y\\big)\\Big]}{\\sigma_X \\sigma_Y} $$\nwhere $E$ is the expectation operator, and $cov$ means covariance.\nThere are additional alternative ways to measures of correlations. Some common examples are: Rank Correlation, Distance Correlation, polychoric correlation and correlation ratio etc. Each of such measures capture different aspects of the data and should be used with care depending on the situation.\nMost correlation measures are sensitive to the manner in which $X$ and $Y$ are sampled. Dependencies tend to be stronger if viewed over a wider range of values. Sensitivity to the data distribution can be used to an advantage. For example, scaled correlation is designed to use the sensitivity to the range in order to pick out correlations between fast components of time series.\n🔥Correlation does not imply causation.🔥 If a strong correlation is observed between two variables A and B, there are several possible explanations: (a) A influences B; (b) B influences A; \u0026copy; A and B are influenced by one or more additional variables; (d) the relationship observed between A and B was a chance error.\nSmall correlation values do not necessarily indicate that two variables are unassociated. For example, Pearson\u0026rsquo;s coefficients will underestimate the association between two variables that show a quadratic relationship. You should always examine the scatter plot in the EDA.\nThe correlation of two variables that both have been recorded repeatedly over time can be misleading and spurious. Time trends should be removed from such data before attempting to measure correlation. Caution should be used in interpreting results of correlation analysis when large numbers of variables have been examined, resulting in a large number of correlation coefficients.\nExploratory Data Analysis (EDA) Exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. The objectives of EDA are to:\n Suggest hypotheses about the causes of observed phenomena Assess assumptions on which statistical inference will be based Support the selection of appropriate statistical tools and techniques Provide a basis for further data collection through surveys or experiments  Typical graphical techniques used in EDA are:\n Box Plot Histogram Multi-Vari Chart Run Chart Pareto Chart Scatter Plot Stem-and-Leaf Plot Parallel Coordinates Odd Ratio Multidimensional Scaling Targeted Projection Pursuit Principal Component Analysis (PCA) Multi-linear PCA Dimensionality Reduction Nonlinear Dimensionality Reduction (NLDR)  Typical quantitative techniques used in EDA are:\n Median Polish Trimean Ordination  I have already covered some examples of many of these techniques in my past posts on EDA of Single Variable , Two variables and Multiple Variables.\nI will be going through mathematical details of some of others in future posts.\n","title":"Descriptive Statistics","url":"https://sadanand-singh.github.io/posts/descriptivestats/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":" Arch Linux is a general purpose GNU/Linux distribution that provides most up-to-date software by following the rolling-release model. Arch Linux allows you to use updated cutting-edge software and packages as soon as the developers released them. KDE Plasma 5 is the current generation of the desktop environment created by KDE primarily for Linux systems.\nIn this post, we will do a complete installation of Arch Linux with Plasma 5 as the desktop environment. Our setup will also involve encryption of the root partition that will be formatted in btrfs. This post is an updated and a more complete version of my previous posts on Arch Linux and Plasma 5 Installation.\n  System Details For reference, my installation system is a slightly upgraded form of my original desktop:\n i7 4790 3.6 GHz (Haswell) ASRock Z97 Extreme6 LGA 1150 Intel Z97 HDMI SATA USB 3.0 ADATA XPG V1.0 DDR3 1866 4x4 GB RAM OCZ Vertex 460A Series 2.5\u0026rdquo; 240 GB WD Blue 1TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache WD Blue 3TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache Ultra LSP V2 650 Watt PSU Cooler Master - MasterCase Pro 5 Asus BW-12B1ST/BLK/G/AS Blue Ray Burner Samsung U28E590D 28-Inch UHD LED-Lit 4K Monitor Nvidia GeForce GTX 750 Ti GPU  Base Installation NOTE I do not wish to repeat Arch Installation Guide here.\nDo not forget about Arch Wiki, the best documentation in the world! Most of the content in this post has been compiled from the Arch wiki.\n  Before beginning this guide, I would assume that you have a bootable USB of the latest Arch Linux Installer. If not, please follow the Arch wiki guide to create one for you.\nOnce you login in the installer disk, You will be logged in on the first virtual console as the root user, and presented with a zsh shell prompt. I will assume you have an Ethernet connection and hence will be connected to Internet by default. If you have to rely on wifi, please refer to the Wireless Network Configuration wiki page for the detailed setup. You must have Internet connection at this stage before proceeding any further.\nYou should boot into UEFI mode if you have a UEFI motherboard and UEFI mode enabled.\nTo verify you have booted in UEFI mode, run:\n$ efivar -l   This should give you a list of set UEFI variables. Please look at the Arch Installation Guide in case you do not get any list of UEFI variables.\nThe very first thing that annoys me in the virtual console is how tiny all the fonts are. We will fix that by running the following commands:\n$ pacman -Sy $ pacman -S terminus-font $ setfont ter-132n   We are all set to get started with the actual installation process.\nHDDs Partitioning First find the hard drive that you will be using as the main/root disk.\n$ cat /proc/partitions # OUTPUT eg. # major minor #blocks name # 8 0 268435456 sda # 9 0 268435456 sdb # 19 0 268435456 sdc # 11 0 759808 sr0 # 7 0 328616 loop0   Say, we will be using /dev/sda as the main disk and /dev/sdb as /data and /dev/sdc as /media .\nBecause we are creating an encrypted file system it’s a good idea to overwrite it with random data.\nWe’ll use badblocks for this. Another method is to use dd if=/dev/urandom of=/dev/xxx, the dd method is probably the best method, but is a lot slower. The following step should take about 20 minutes on a 240 GB SSD.\n$ badblocks -c 10240 -s -w -t random -v /dev/sda   Next, we will create GPT partitions on all disks using gdisk command.\n$ dd if=/dev/zero of=/dev/sda bs=1M count=5000 $ gdisk /dev/sda Found invalid MBR and corrupt GPT. What do you want to do? (Using the GPT MAY permit recovery of GPT data.) 1 - Use current GPT 2 - Create blank GPT   Then press 2 to create a blank GPT and start fresh\nZAP: $ press x - to go to extended menu $ press z - to zap $ press Y - to confirm $ press Y - to delete MBR   It might now kick us out of gdisk, so get back into it:\n$ gdisk /dev/sda $ Command (? for help): m $ Command (? for help): n $ Partition number (1-128, default 1): $ First sector (34-500118158, default = 2048) or {\u0026#43;-}size{KMGTP}: $ Last sector (2048-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: 512M $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): ef00 $ Changed type of partition to \u0026#39;EFI System\u0026#39; $ Partition number (2-128, default 2): $ First sector (34-500118, default = 16779264) or {\u0026#43;-}size{KMGTP}: $ Last sector (16779264-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): $ Changed type of partition to \u0026#39;Linux filesystem\u0026#39; $ Command (? for help): p $ Press w to write to disk $ Press Y to confirm   Repeat the above procedure for /dev/sdb and /dev/sdc, but create just one partition with all values as default. At the end we will have three partitions: /dev/sda1, /dev/sda2, /dev/sdb1 and /dev/sdc1.\nSetup Disk Encryption Our /boot partition will be on /dev/sda1, while the main installation will be on /dev/sda2. In this setup, we will be enabling full encryption on /dev/sda2 only.\nIn order to enable disk encryption, we will first create a root luks volume, open it and then format it.\n# first, we need to prepare the encrypted (outer) volume $ cryptsetup --cipher aes-xts-plain64 --hash sha512 --use-random --verify-passphrase luksFormat /dev/sda2 # I really hope I don\u0026#39;t have to lecture you on NOT LOSING this # password, lest all of your data will be forever inaccessible, # right? # then, we actually open it as a block device, and format the # inner volume later $ cryptsetup luksOpen /dev/sda2 root   Automatic Key Login from an USB/SD Card If you want to automatically login the encrypted disk password from an externally attached USB or SD card, you will first need to create a key file.\n$ dd bs=512 count=4 if=/dev/urandom of=KEYFILE   Then, add this key to the luks container, so that it can be later used to open the encrypted drive.\n$ cryptsetup luksAddKey /dev/sda2 KEYFILE   Note that the KEYFILE here should be kept on a separate USB drive or SD card. The recommended way of using such a disk would be as follows:\n# assuming our USB of interest is /dev/sdd and can be format # # Format the drive $ dd if=/dev/zero of=/dev/sdd bs=1M # Create partitions using gdisk # $ gdisk /dev/sdd # # Follow along to create one partition (/dev/sdd1) of type 0700 # # format /dev/sdd1 $ mkfs.fat /dev/sdd1 # mount the newly format disk on /mnt and then copy the KEYFILE $ mount /dev/sdd1 /mnt $ mv KEYFILE /mnt/KEYFILE $ umount /mnt   We will be later using this KEYFILE in boot loader setup.\n  Format HDDs At this point, we have following drives ready for format: /dev/sda1, /dev/mapper/root, /dev/sdb1 and /dev/sdc1.\nThese can be format as follows:\n$ mkfs.vfat -F32 /dev/sda1 $ mkfs.btrfs -L arch /dev/mapper/root $ mkfs.btrfs -L data /dev/sdb1 $ mkfs.btrfs -L media /dev/sdc1   Now, we will create btrfs subvolumes and mount them properly for installation and final setup.\n$ mount /dev/mapper/root /mnt $ btrfs subvolume create /mnt/ROOT $ btrfs subvolume create /mnt/home $ umount /mnt $ mount /dev/sdb1 /mnt $ btrfs subvolume create /mnt/data $ umount /mnt $ mount /dev/sdc1 /mnt $ btrfs subvolume create /mnt/media $ umount /mnt   Now, once the sub-volumes have been created, we will mount them in appropriate locations with optimal flags.\n$ SSD_MOUNTS=\u0026#34;rw,noatime,nodev,compress=lzo,ssd,discard, space_cache,autodefrag,inode_cache\u0026#34; $ HDD_MOUNTS=\u0026#34;rw,nosuid,nodev,relatime,space_cache\u0026#34; $ EFI_MOUNTS=\u0026#34;rw,noatime,discard,nodev,nosuid,noexec\u0026#34; $ mount -o $SSD_MOUNTS,subvol=ROOT /dev/mapper/root /mnt $ mkdir -p /mnt/home $ mkdir -p /mnt/data $ mkdir -p /mnt/media $ mount -o $SSD_MOUNTS,nosuid,subvol=home /dev/mapper/root /mnt/home $ mount -o $HDD_MOUNTS,subvol=data /dev/sdb1 /mnt/data $ mount -o $HDD_MOUNTS,subvol=media /dev/sdc1 /mnt/media $ mkdir -p /mnt/boot $ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot   Save the current /etc/resolv.conf file for future use! $ cp /etc/resolv.conf /mnt/etc/resolv.conf   Base System Installation Now, we will do the actually installation of base packages.\n$ pacstrap /mnt base base-devel btrfs-progs $ genfstab -U -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab   Initial System Setup Edit the /mnt/ect/fstab file to add following /tmp mounts.\ntmpfs /tmp tmpfs rw,nodev,nosuid 0 0 tmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0   Finally bind root for installation.\n$ arch-chroot /mnt \u0026#34;bash\u0026#34; $ pacman -Syy $ pacman -Syu $ pacman -S sudo vim $ vim /etc/locale.gen ... # en_SG ISO-8859-1 en_US.UTF-8 UTF-8 # en_US ISO-8859-1 ... $ locale-gen $ echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf $ export LANG=en_US.UTF-8 $ ls -l /usr/share/zoneinfo $ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime $ hwclock --systohc --utc $ sed -i \u0026#34;s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\u0026#34; /etc/sudoers $ HOSTNAME=euler $ echo $HOSTNAME \u0026gt; /etc/hostname $ passwd   We will also add hostname to our /etc/hosts file:\n$ vim /etc/hosts ... 127.0.0.1 localhost.localdomain localhost ::1 localhost.localdomain localhost 127.0.0.1 $HOSTNAME.localdomain $HOSTNAME ...   We also need to fix the mkinitcpio.conf to contain what we actually need.\n$ vi /etc/mkinitcpio.conf # on the MODULES section, add \u0026#34;vfat aes_x86_64 crc32c-intel\u0026#34; # (and whatever else you know your hardware needs. Mine needs i915 too) # on the BINARIES section, add \u0026#34;/usr/bin/btrfsck\u0026#34;, since it\u0026#39;s useful # to have in case your filesystem has troubles # on the HOOKS section: # - add \u0026#34;encrypt\u0026#34; before \u0026#34;filesystems\u0026#34; # - remove \u0026#34;fsck\u0026#34; and # - add \u0026#34;btrfs\u0026#34; at the end # # re-generate your initrd images mkinitcpio -p linux   Boot Manager Setup systemd-boot, previously called gummiboot, is a simple UEFI boot manager which executes configured EFI images. The default entry is selected by a configured pattern (glob) or an on-screen menu. It is included with the systemd, which is installed on an Arch systems by default.\nAssuming /boot is your boot drive, first run the following command to get started:\n$ bootctl --path=/boot install   It will copy the systemd-boot binary to your EFI System Partition ( /boot/EFI/systemd/systemd-bootx64.efi and /boot/EFI/Boot/BOOTX64.EFI - both of which are identical - on x64 systems ) and add systemd-boot itself as the default EFI application (default boot entry) loaded by the EFI Boot Manager.\nFinally to configure out boot loader, we will need the UUID of some of our hard drives. These can be easily done using the blkid command.\n$ blkid /dev/sda1 \u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/sda2 \u0026gt;\u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/mapper/root \u0026gt;\u0026gt; /boot/loader/entries/arch.conf $ blkid /dev/sdd1 \u0026gt;\u0026gt; /boot/loader/entries/arch.conf # for this example, I\u0026#39;m going to mark them like this: # /dev/sda1 LABEL=\u0026#34;EFI\u0026#34; UUID=11111111-1111-1111-1111-111111111111 # /dev/sda2 LABEL=\u0026#34;arch\u0026#34; UUID=33333333-3333-3333-3333-333333333333 # /dev/mapper/root LABEL=\u0026#34;Arch Linux\u0026#34; UUID=44444444-4444-4444-4444-444444444444 # /dev/sdd1 LABEL=\u0026#34;USB\u0026#34; UUID=0000-0000 # this is the drive where KEYFILE exists   Now, make sure that the following two files look as follows, where UUIDs is the value obtained from above commands.\nDo not forget to modify UUIDs and KEYFIL entries! $ vim /boot/loader/loader.conf ... timeout 3 default arch ... $ vim /boot/loader/entries/arch.conf ... title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options ro cryptdevice=UUID=33333333-3333-3333-3333-333333333333:luks-33333333-3333-3333-3333-333333333333 root=UUID=44444444-4444-4444-4444-444444444444 rootfstype=btrfs rootflags=subvol=ROOT cryptkey=UUID=0000-0000:vfat:KEYFILE ...   Network Setup At first we will need to figure out the Ethernet controller on which cable is connected.\n$ networkctl # # IDX LINK TYPE OPERATIONAL SETUP # 1 lo loopback carrier unmanaged # 2 enp3s0 ether no-carrier unmanaged # 3 wlp6s0 wlan no-carrier unmanaged # 4 enp0s25 ether routable configured #   In my case, the name of the device is enp0s25.\nUsing this name of the device, we need to configure, and enable the systemd-networkd.service service.\nNote that we will using the resolv.conf that we saved from this session.\nNetwork configurations are stored as *.network in /etc/systemd/network. We need to create ours as follows.:\n$ vim /etc/systemd/network/50-wired.network $ ... [Match] Name=enp0s25 [Network] DHCP=ipv4 ... $   Now enable the networkd services:\nsystemctl enable systemd-networkd.service   Your network should be ready for the first use!\nSync time automatically using the systemd service:\n$ vim /etc/systemd/timesyncd.conf $ ... [Time] NTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org FallbackNTP=0.pool.ntp.org 1.pool.ntp.org 0.fr.pool.ntp.org ... $ $ timedatectl set-ntp true $ timedatectl status $ ... Local time: Tue 2016-09-20 16:40:44 PDT Universal time: Tue 2016-09-20 23:40:44 UTC RTC time: Tue 2016-09-20 23:40:44 Time zone: US/Pacific (PDT, -0700) Network time on: yes NTP synchronized: yes RTC in local TZ: no ... $   Avahi is a tool that allows programs to publish and discover services and hosts running on a local network with no specific configuration. For example you can plug into a network and instantly find printers to print to, files to look at and people to talk to.\nWe can easily set it up it as follows:\n$ pacman -S avahi nss-mdns $ systemctl enable avahi-daemon.service   We will also install terminus-font on our system to work with proper fonts on first boot.\n$ pacman -S terminus-font   First Boot Installations Now we are ready for the first boot! Run the following command:\n$ exit $ umount -R /mnt $ reboot   After your new system boots, Network should be setup at the start. Check the status of network using:\n# Set readable font first! setfont ter-132n ping google.com -c 2 # # PING google.com (10.38.24.84) 56(84) bytes of data. # 64 bytes from google.com (10.38.24.84): icmp_seq=1 ttl=64 time=0.022 ms # 64 bytes from google.com (10.38.24.84): icmp_seq=2 ttl=64 time=0.023 ms # # --- google.com ping statistics --- # 2 packets transmitted, 2 received, 0% packet loss, time 999ms # rtt min/avg/max/mdev = 0.022/0.022/0.023/0.004 ms #   If you do not get this output, please follow the troubleshooting links at Arch Wiki on setting up network.\nAdding New User Choose $USERNAME per your liking. I chose ssingh, so in future commands whenever you see ssingh please replace it with your $USERNAME.\n$ pacman -S zsh $ useradd -m -G wheel -s usr/bin/zsh $USERNAME $ chfn --full-name \u0026#34;$FULL_NAME\u0026#34; $USERNAME $ passwd $USERNAME   GUI Installation with nvidia I will be assuming you have an NVIDIA card for graphics installation.\nTo setup a graphical desktop, first we need to install some basic X related packages, and some essential packages (including fonts):\n$ pacman -S xorg-server nvidia nvidia-libgl nvidia-settings mesa   To avoid the possibility of forgetting to update your initramfs after an nvidia upgrade, you have to use a pacman hook like this:\n$ vim /etc/pacman.d/hooks/nvidia.hook $ ... [Trigger] Operation=Install Operation=Upgrade Operation=Remove Type=Package Target=nvidia [Action] Depends=mkinitcpio When=PostTransaction Exec=/usr/bin/mkinitcpio -p linux ... $   Nvidia has a daemon that is to be run at boot. To start the persistence daemon at boot, enable the nvidia-persistenced.service.\n$ systemctl enable nvidia-persistenced.service $ systemctl start nvidia-persistenced.service   How to Avoid Screen Tearing Tearing can be avoided by forcing a full composition pipeline, regardless of the compositor you are using.\nIn order to make this change permanent, We will need to edit nvidia configuration file. Since, by default there aren\u0026rsquo;t any, we will first need to create one.\n$ nvidia-xconfig $ mv /etc/X11/xorg.cong /etc/X11/xorg.conf.d/20-nvidia.conf # # Edit this file as follows: vim /etc/X11/xorg.conf.d/20-nvidia.conf # ------------------------------------------- # Section \u0026#34;Screen\u0026#34; # Identifier \u0026#34;Screen0\u0026#34; # Option \u0026#34;metamodes\u0026#34; \u0026#34;nvidia-auto-select \u0026#43;0\u0026#43;0 { ForceFullCompositionPipeline = On }\u0026#34; # Option \u0026#34;AllowIndirectGLXProtocol\u0026#34; \u0026#34;off\u0026#34; # Option \u0026#34;TripleBuffer\u0026#34; \u0026#34;on\u0026#34; # EndSection [...] # Section \u0026#34;Device\u0026#34; # [...] # Option \u0026#34;TripleBuffer\u0026#34; \u0026#34;True\u0026#34; # [...] # EndSection # [...] # ------------------------------------------------     Specific for Plasma 5, we will also create the following file to avoid any tearing in Plasma.\n$ vim /etc/profile.d/kwin.sh $ ... export KWIN_TRIPLE_BUFFER=1 ...   How to Enable Better Resolution During Boot The kernel compiled in efifb module supports high-resolution nvidia console on EFI systems. This can enabled by enabling the DRM kernel mode setting.\nFirst, we will need to add following to MODULES section of the mkinitcpio.conf file:\n nvidia nvidia_modeset nvidia_uvm nvidia_drm  We will also need to pass the nvidia-drm.modeset=1 kernel parameter during the boot.\n$ vim /etc/mkinitcpio.conf $ ... MODULES=\u0026#34;vfat aes_x86_64 crc32c-intel nvidia nvidia_modeset nvidia_uvm nvidia_drm\u0026#34; ... $ $ vim /boot/loader/entries/arch.conf $ ... options ro cryptdevice=UUID=:luks- root=UUID= rootfstype=btrfs rootflags=subvol=ROOT cryptkey=UUID=:vfat:deepmind20170602 nvidia-drm.modeset=1 ... $ $ mkinitcpio -p linux     Plasma 5 Installation and Setup We can now proceed with the installation of Plasma 5. In the process, we will also install some useful fonts.\n$ pacman -S ttf-hack ttf-anonymous-pro $ pacman -S ttf-dejavu ttf-freefont ttf-liberation $ pacman -S plasma-meta dolphin kdialog kfind $ pacman -S konsole gwenview okular spectacle kio-extras $ pacman -S kompare dolphin-plugins kwallet kwalletmanager $ pacman -S ark yakuake flite   We will also need to select proper themes for the Plasma 5 display manager sddm and then enable its systemd service.\n$ vim /etc/sddm.conf .... [Theme] # Current theme name Current=breeze # Cursor theme used in the greeter CursorTheme=breeze_cursors ... $ systemctl enable sddm $ reboot   Once, we boot into the new system, we should have a basic Plasma 5 desktop waiting for you. In the following section, we will be do installation and modifications to the system that I prefer.\nPost Installation Setup Plasma 5 provides a handy network manager applet. However, in order to use it properly we will need the NetworkManager service to be enabled. This applet allows user specific enabling of wifi, ethernet or even VPN connections.\n$ sudo pacman -S networkmanager $ systemctl enable NetworkManager.service $ systemctl start NetworkManager.service   We can also automate the hostname setup using the following systemd command:\n$ hostnamectl set-hostname $HOSTNAME   Selecting pacman Mirrors The pacman package provides a \u0026ldquo;bash\u0026rdquo; script, /usr/bin/rankmirrors, which can be used to rank the mirrors according to their connection and opening speeds to take advantage of using the fastest local mirror.\nWe will do this only on the US based mirrors. First make a copy of the mirrors list file and then delete all non-US mirrors. We will then rankmirrors script on the modified list to get the top 6 mirrors for our regular use.\n$ cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.backup $ cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.us $ vim /etc/pacman.d/mirrorlist.us .... # Delete all non-US servers .... $ rankmirrors -n 6 /etc/pacman.d/mirrorlist.us \u0026gt; /etc/pacman.d/mirrorlist   Setup AUR AUR is a community-driven repository for Arch users. This allows you to install many popular packages that are otherwise not available through core repositories.\nIn order to make all types of installations uniform, I use pacaur as the preferred tool for installing all packages. One the biggest advantages of pacaur is that is uses exactly the same options that regular pacman uses.\nIn order to install pacuar, first install dependencies.\n$ sudo pacman -S expac yajl curl gnupg --noconfirm   Create a temp directory for building packages:\n$ mkdir ~/temp $ cp ~ temp   Install cower first and then pacaur:\n$ gpg --recv-keys --keyserver hkp://pgp.mit.edu 1EB2638FF56C0C53 $ curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=cower $ makepkg -i PKGBUILD --noconfirm $ curl -o PKGBUILD https://aur.archlinux.org/cgit/aur.git/plain/PKGBUILD?h=pacaur $ makepkg -i PKGBUILD --noconfirm # Finally cleanup and remove the temp directory $ cd ~ $ rm -r ~/temp   Audio Setup This is pretty simple. Install following packages and you should be done:\n$ sudo pacaur -S alsa-utils pulseaudio pulseaudio-alsa mpv $ sudo pacaur -S libcanberra-pulse libcanberra-gstreamer $ sudo pacaur -S vlc-qt5   Now start the pulseaudio service.\n$ systemctl --user enable pulseaudio.socket   Web Browsers My preferred choice of browsers is google chrome. However, it is also good to have the KDE native qupzilla.\n$ sudo pacaur -S google-chrome qupzilla   Profile-sync-daemon (psd) is a tiny pseudo-daemon designed to manage browser profile(s) in tmpfs and to periodically sync back to the physical disc (HDD/SSD). This is accomplished by an innovative use of rsync to maintain synchronization between a tmpfs copy and media-bound backup of the browser profile(s). These features of psd leads to following benefits:\n Transparent user experience Reduced wear to physical drives, and Speed  To setup. first install the profile-sync-daemon package.\nsudo pacaur -S profile-sync-daemon   Run psd the first time which will create a configuration file at \\$XDG_CONFIG_HOME/psd/psd.conf which contains all settings.\n$ psd # First time running psd so please edit # /home/$USERNAME/.config/psd/psd.conf to your liking and run again.   In the config file change the BROWSERS variables to google-chrome qupzilla. Also, enable the use of overlayfs to improve sync speed and to use a smaller memory footprint. Do this in the USE_OVERLAYFS=\u0026ldquo;yes\u0026rdquo; variable.\nNote: USE_OVERLAYFS feature requires a Linux kernel version of 3.18.0 or greater to work. In order to use the OVERLAYFS feature, you will also need to give sudo permissions to psd-helper as follows (replace $USERNAME accordingly):\n$ vim /etc/sudoers ... $USERNAME ALL=(ALL) NOPASSWD: /usr/bin/psd-overlay-helper ...   Verify the working of configuration using the preview mode of psd:\npsd p   Google Chrome by default uses kdewallet to manage passwords, where as Qupzilla does not. You can change that in its settings.\ngit Setup Install git and setup some global options as below:\n$ sudo pacaur -S git $ $ vim ~/.gitconfig ... [user] name = Sadanand Singh email = EMAIL_ADDRESS [color] ui = auto [status] showuntrackedfiles = no [alias] gist = log --graph --oneline --all --decorate --date-order find = log --graph --oneline --all --decorate --date-order --regexp-ignore-case --extended-regexp --grep rfind = log --graph --oneline --all --decorate --date-order --regexp-ignore-case --extended-regexp --invert-grep --grep search = grep --line-number --ignore-case -E -I [pager] status = true [push] default = matching [merge] tool = meld [diff] tool = meld [help] autocorrect = 1 ...   ssh Setup To get started first install the openssh package.\nsudo pacaur -S openssh   The ssh server can be started using the systemd service. Before starting the service, however, we want to generate ssh keys and setup the server for login based only on keys.\n$ ssh-keygen -t ed25519 $ # Create a .ssh/config file for rmate usage in sublime text $ vim ~/.ssh/config ... RemoteForward 52698 localhost:52698 ... $ # Create ~/.ssh/authorized_keys file with list of machines that # are allowed to login to this machine. $ touch ~/.ssh/authorized_keys $ # Finally edit the /etc/ssh/sshd_config # file to disable Password based logins $ sudo vim /etc/ssh/sshd_config ... PasswordAuthentication no ...   Furthermore, before enabling the sshd service, please also ensure to copy your keys to all your relevant other servers and places like github.\nWe can now use systemd to start the ssh service.\n$ systemctl enable sshd.socket $ systemctl start sshd.socket   zsh Setup During the user creation, we already installed the zsh shell. We have also activated a basic setup at first login by the user.\nIn this section, we will be installing my variation of zprezto package to manage zsh configurations.\nFirst install the main zprezto package:\n$ git clone --recursive https://github.com/sorin-ionescu/prezto.git \u0026#34;${ZDOTDIR:-$HOME}/.zprezto\u0026#34; $ $ setopt EXTENDED_GLOB $ for rcfile in \u0026#34;${ZDOTDIR:-$HOME}\u0026#34;/.zprezto/runcoms/^README.md(.N); do ln -sf \u0026#34;$rcfile\u0026#34; \u0026#34;${ZDOTDIR:-$HOME}/.${rcfile:t}\u0026#34; done $   Now, We will add my version of prezto to the same git repo.\n$ cd ~/.zprezto $ git remote add personal git@github.com:sadanand-singh/My-Zprezto.git $ git pull personal arch $ git checkout arch $ git merge master   And we are all setup for using zsh!\ngpg Setup We have already installed the gnupg package during the pacaur installation. We will first either import our already existing private keys(s) or create one.\nOnce We have our keys setup, edit keys to change trust level.\nOnce all keys are setup, we need to gpg-agent configuration file:\n$ vim ~/.gnupg/gpg-agent.conf .. enable-ssh-support default-cache-ttl-ssh 10800 default-cache-ttl 10800 max-cache-ttl-ssh 10800 ... $   Also, add following to your .zshrc or .\u0026ldquo;bash\u0026rdquo;rc file. If you are using my zprezto setup, you already have this!\n$ vim ~/.zshrc ... # set GPG TTY export GPG_TTY=$(tty) # Refresh gpg-agent tty in case user switches into an X Session gpg-connect-agent updatestartuptty /bye \u0026gt;/dev/null # Set SSH to use gpg-agent unset SSH_AGENT_PID if [ \u0026#34;${gnupg_SSH_AUTH_SOCK_by:-0}\u0026#34; -ne $$ ]; then export SSH_AUTH_SOCK=\u0026#34;/run/user/$UID/gnupg/S.gpg-agent.ssh\u0026#34; fi ... $   Now, simply start the following systemd sockets as user:\n$ systemctl --user enable gpg-agent.socket $ systemctl --user enable gpg-agent-ssh.socket $ systemctl --user enable dirmngr.socket $ systemctl --user enable gpg-agent-browser.socket $ $ systemctl --user start gpg-agent.socket $ systemctl --user start gpg-agent-ssh.socket $ systemctl --user start dirmngr.socket $ systemctl --user start gpg-agent-browser.socket   Finally add your ssh key to ssh agent.\n$ ssh-add ~/.ssh/id_ed25519   User Wallpapers You can store your own wallpapers at the following location. A good place to get some good wallpapers are KaOS Wallpapers.\n$ mkdir -p $ $HOME/.local/wallpapers $ cp SOME_JPEG $HOME/.local/wallpapers/   conky Setup First installed the conky package with lua and nvidia support:\n$ paci conky-lua-nv   Then, copy your conky configuration at \\$HOME/.config/conky/conky.conf.\n$ mkdir -p $HOME/.config/conky # Generate sample conky config file $ conky -C \u0026gt; $HOME/.config/conky/conky.conf $ # start conky in background $ conky \u0026amp;   Here, I have also put my simple configuration file:\nconky.config = { background = true, use_xft = true, xftalpha = 0.2, update_interval = 1, total_run_times = 0, own_window_argb_visual = true, own_window = true, own_window_type = \u0026#39;dock\u0026#39;, own_window_transparent = true, own_window_hints = \u0026#39;undecorated,below,sticky,skip_taskbar,skip_pager\u0026#39;, double_buffer = true, draw_shades = false, draw_outline = false, draw_borders = false, draw_graph_borders = false, stippled_borders = 0, border_width = 0, default_color = \u0026#39;white\u0026#39;, default_shade_color = \u0026#39;#000000\u0026#39;, default_outline_color = \u0026#39;#000000\u0026#39;, minimum_width = 2500, minimum_height = 3500, maximum_width = 2500, gap_x = 2980, gap_y = 0, alignment = \u0026#39;top_left\u0026#39;, no_buffers = true, uppercase = false, cpu_avg_samples = 2, net_avg_samples = 2, --short_units = true, text_buffer_size = 2048, use_spacer = \u0026#39;none\u0026#39;, override_utf8_locale = true, color1 = \u0026#39;#424240\u0026#39;, color2 = \u0026#39;2a2b2f\u0026#39;, color3 = \u0026#39;#FF4B4C\u0026#39;,--0E87E4 color4 = \u0026#39;#73bcca\u0026#39;, own_window_argb_value = 0, --own_window_colour = \u0026#39;#000000\u0026#39;, --lua_load rings-v1.2.1.lua lua_draw_hook_pre = \u0026#39;ring_stats\u0026#39;, --lua_load lilas_rings.lua lua_draw_hook_post = \u0026#39;main\u0026#39;, }; conky.text = [[ ${goto 200}${voffset 100}${color2}${font Nothing You Could Do:size=50}${time %I:%M}${font Nothing You Could Do:size=20}${time %p} ${goto 185}${voffset 10}${color4}${font Bad Script:size=30}${time %A} ${goto 185}${voffset -35}${font Bad Script:size=18}${time %d %B, %Y} ${goto -80}${voffset -35}${font Pompiere:size=11}${color 3eafe8}//${color4} CPU: ${execi 1000 cat /proc/cpuinfo | grep \u0026#39;model name\u0026#39; | sed -e \u0026#39;s/model name.*: //\u0026#39;| uniq | cut -c 19-25} ${color ff3d3d}${hwmon 0 temp 1}°C ${color 3eafe8}//${color4} Load: ${color ff3d3d} ${cpu cpu0}% ${color 3eafe8}// RAM:${color ff3d3d} ${memperc}% / $memmax ${color 3eafe8}// ${goto -80}${voffset -35}${font Pompiere:size=11}${color 3eafe8}//${color4} GPU: ${execi 1000000 nvidia-smi --query-gpu=\u0026#34;name,driver_version\u0026#34; --format=\u0026#34;csv,noheader\u0026#34; | cut -c 9-18} ${color ff3d3d} ${nvidia temp}°C ${color 3eafe8}//${color4} Load: ${color ff3d3d}${exec nvidia-smi --query-gpu=\u0026#34;utilization.gpu\u0026#34; --format=\u0026#34;csv,noheader\u0026#34;} ${color 3eafe8}// Free: ${color ff3d3d} ${exec nvidia-smi --query-gpu=\u0026#34;memory.free\u0026#34; --format=\u0026#34;csv,noheader\u0026#34;} ${color 3eafe8}// ]];   Software Installations Here is a running list of other common softwares that I install.\n$ paci spotify tmux tree dropbox thesilver_searcher $ paci digikam imagemagick   I also add the following repository to install the Sublime Text editor. Refer to my previous post  for details on setting up Sublime Text.\n$ curl -O https://download.sublimetext.com/sublimehq-pub.gpg $ sudo pacman-key --add sublimehq-pub.gpg $ sudo pacman-key --lsign-key 8A8F901A $ rm sublimehq-pub.gpg $ $ echo -e \u0026#34;\\n[sublime-text]\\nServer = https://download.sublimetext.com/arch/dev/x86_64\u0026#34; | sudo tee -a /etc/pacman.conf   Now we can install sublime-text as:\n$ paci sublime-text/sublime-text   This brings us to the conclusion of this installation guide. Hope many of you find it useful. Please drop your comments below if you have any suggestions for improvements etc.\n","title":"My Arch Linux Setup with Plasma 5","url":"https://sadanand-singh.github.io/posts/completesetuparchplasma/"},{"tags":"Editor","text":"I have been using Sublime text as my primary editor for some time now. Here I wanted to share my current setup for the editor including all settings, packages, shortcut keys and themes.\n\n  Packages First thing you will need to install is the Package Control. This can be easily done by following the directions at their installation instructions.\nOnce you have installed the package manager and restarted sublime text, now you can install all other packages using the powerful command pallet. Hit ctrl + shift + P and type Install, choose Package Control : Install Package. Now you can search for any package that you wish to install, and then press Enter to install it.\nHere is a list of packages that I currently use:\n Alignment Bracket Highlighter C++11 Column Select DocBlockr_Python GitGutter MagicPython rsub Search In Project SublimeLinter SublimeLinter-flake8  Alignment provides a simple key-binding for aligning multi-line and multiple selections. Bracket Highlighter, as the name suggests, matches a variety of brackets such as: [], (), {}, \u0026quot;\u0026quot;, '', \u0026lt;tag\u0026gt;\u0026lt;/tag\u0026gt;, and even custom brackets. C++11 provides better coloring scheme and syntax highlighting for C++11 syntax.\nColumn Select plug-in provides an alternate behavior for Sublime keyboard column selection. The differences are:\n Allows reversing direction (go down too far, just go back up). Added PageUp/PageDown, Home/End, and mouse selection. Skip rows that are too short. If you start at the end of a line, then it will stay at the end of each line.  DocBlockr_Python makes writing documentation a breeze for python code. GitGutter is a handy plug-in to show information about files in a git repository. Main Features are:\n Gutter Icons indicating inserted, modified or deleted lines Diff Popup with details about modified lines Status Bar Text with information about file and repository Jumping Between Changes to easily navigate between modified lines  MagicPython is a package with preferences and syntax highlighter for cutting edge Python 3. It is meant to be a drop-in replacement for the default Python package. MagicPython correctly highlights all Python 3.5 and 3.6 syntax features, including type annotations, f-strings and regular expressions. It is built from scratch for robustness with an extensive test suite.\nrsub is an implementation of TextMate 2\u0026rsquo;s rmate feature for Sublime Text, allowing files to be edited on a remote server using ssh port forwarding / tunneling. Please make sure you have installed a version of rmate and are using correct port forwarding.\nSearch in Project lets you use your favorite search tool (grep, ack, ag, pt, rg, git grep, or findstr) to find strings across your entire current Sublime Text project. I personally use the silver_seracher (ag) for this purpose.\nSublimeLinter and SublimeLinter-flake8 is plug-in that provides an interface to flake8. It will be used with files that have the Python syntax.\nShortcut Keys Here is a summary of my key map:\n[ { \u0026#34;keys\u0026#34;: [\u0026#34;shift\u0026#43;alt\u0026#43;a\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;find_all_under\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;control\u0026#43;v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste_and_indent\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;control\u0026#43;shift\u0026#43;v\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;paste\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;;\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;alignment\u0026#34; }, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;up\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;lines\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;down\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;lines\u0026#34;, \u0026#34;forward\u0026#34;: true}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;pageup\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;pages\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;pagedown\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;pages\u0026#34;, \u0026#34;forward\u0026#34;: true}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;home\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;forward\u0026#34;: false}}, { \u0026#34;keys\u0026#34;: [\u0026#34;ctrl\u0026#43;alt\u0026#43;end\u0026#34;], \u0026#34;command\u0026#34;: \u0026#34;column_select\u0026#34;, \u0026#34;args\u0026#34;: {\u0026#34;by\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;forward\u0026#34;: true}} ]   Theme and Color Scheme I like using the material theme. In particular, I use the \u0026ldquo;Materialize\u0026rdquo; theme. You can use this by installing the following packages:\n Materialize Materialize-Appbar Materialize-White-Panels  With these installation, you will also get a lot of color schemes. I prefer to use the Material Oceanic Next color scheme. All other settings for this theme can be seen in my settings below.\nUser Settings / Preferences Here is my complete set of settings for Sublime Text. Please feel free to leave comments below for any questions or suggestions.\n{ \u0026#34;always_show_minimap_viewport\u0026#34;: true, \u0026#34;auto_complete\u0026#34;: true, \u0026#34;bold_folder_labels\u0026#34;: true, \u0026#34;caret_extra_width\u0026#34;: 1.5, \u0026#34;color_scheme\u0026#34;: \u0026#34;Packages/Materialize/schemes/Material Oceanic Next.tmTheme\u0026#34;, \u0026#34;default_line_ending\u0026#34;: \u0026#34;unix\u0026#34;, \u0026#34;drag_text\u0026#34;: false, \u0026#34;draw_white_space\u0026#34;: \u0026#34;all\u0026#34;, \u0026#34;enable_tab_scrolling\u0026#34;: false, \u0026#34;font_face\u0026#34;: \u0026#34;Hack\u0026#34;, \u0026#34;font_options\u0026#34;: [ \u0026#34;directwrite\u0026#34;, \u0026#34;gray_antialias\u0026#34;, \u0026#34;subpixel_antialias\u0026#34; ], \u0026#34;font_size\u0026#34;: 13, \u0026#34;hot_exit\u0026#34;: false, \u0026#34;ignored_packages\u0026#34;: [ \u0026#34;C\u0026#43;\u0026#43;\u0026#34;, \u0026#34;Python\u0026#34;, \u0026#34;Vintage\u0026#34; ], \u0026#34;indent_guide_options\u0026#34;: [ \u0026#34;draw_normal\u0026#34;, \u0026#34;draw_active\u0026#34; ], \u0026#34;line_padding_bottom\u0026#34;: 1, \u0026#34;line_padding_top\u0026#34;: 1, \u0026#34;material_theme_bold_tab\u0026#34;: true, \u0026#34;material_theme_compact_panel\u0026#34;: true, \u0026#34;material_theme_compact_sidebar\u0026#34;: false, \u0026#34;material_theme_contrast_mode\u0026#34;: true, \u0026#34;material_theme_disable_fileicons\u0026#34;: false, \u0026#34;material_theme_disable_folder_animation\u0026#34;: true, \u0026#34;material_theme_disable_tree_indicator\u0026#34;: true, \u0026#34;material_theme_panel_separator\u0026#34;: true, \u0026#34;material_theme_small_statusbar\u0026#34;: true, \u0026#34;material_theme_small_tab\u0026#34;: true, \u0026#34;material_theme_tabs_autowidth\u0026#34;: true, \u0026#34;material_theme_tabs_separator\u0026#34;: true, \u0026#34;material_theme_tree_headings\u0026#34;: true, \u0026#34;overlay_scroll_bars\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;remember_open_files\u0026#34;: false, \u0026#34;rulers\u0026#34;: [ 80 ], \u0026#34;scroll_past_end\u0026#34;: true, \u0026#34;soda_classic_tabs\u0026#34;: true, \u0026#34;soda_folder_icons\u0026#34;: true, \u0026#34;tab_completion\u0026#34;: false, \u0026#34;tab_size\u0026#34;: 4, \u0026#34;theme\u0026#34;: \u0026#34;Material Oceanic Next.sublime-theme\u0026#34;, \u0026#34;translate_tabs_to_spaces\u0026#34;: true, \u0026#34;trim_trailing_white_space_on_save\u0026#34;: true, \u0026#34;word_wrap\u0026#34;: true }  ","title":"Sublime Text Setup","url":"https://sadanand-singh.github.io/posts/sublimetext/"},{"tags":"Algorithms, Machine Learning","text":"In this post we will explore a class of machine learning methods called Support Vector Machines also known commonly as SVM.\n\n Introduction SVM is a supervised machine learning algorithm which can be used for both classification and regression.\n In the simplest classification problem, given some data points each belonging to one of the two classes, the goal is to decide which class a new data point will be in. A simple linear solution to this problem can be viewed in a framework where a data point is viewed as a $p$-dimensional vector, and we want to know whether we can separate such points with a ($p-1$)-dimensional hyperplane.\nThere are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier; or equivalently, the perceptron of optimal stability.\nThe figure on the right is a binary classification problem (points labeled $y_i = \\pm 1$) that is linearly separable in space defined by the vector x. Green and purple line separate two classes with a small margin, whereas yellow line separates them with the maximum margin.\n Mathematically, for the linearly separable case, any point x lying on the separating hyperplane satisfies: \\(\\mathbf{x}^T\\mathbf{w} \u0026#43; b = 0\\)   , where $\\mathbf{w}$ is the vector normal to the hyperplane, and $b$ is a constant that describes how much plane is shifted relative to the origin. The distance of the hyperplane from the origin is \\(\\frac{b}{\\lVert \\mathbf{w} \\rVert}\\)   .\nNow draw parallel planes on either side of the decision boundary, so we have what looks like a channel, with the decision boundary as the central line, and the additional planes as gutters. The margin, i.e. the width of the channel, is \\((d_\u0026#43; \u0026#43; d_-)\\)    and is restricted by the data points closest to the boundary, which lie on the gutters. The two bounding hyperplanes of the channel can be represented by a constant shift in the decision boundary. In other words, these planes ensure that all the points are at least a signed distance d away from the decision boundary. The channel region can be also represented by the following equations:\n$$\\begin{aligned} \u0026amp; \\mathbf{x}_i^T\\mathbf{w} \u0026#43; b \\ge \u0026#43;a, \\text{for } y_i = \u0026#43;1 \\\\ \u0026amp; \\mathbf{x}_i^T\\mathbf{w} \u0026#43; b \\le -a, \\text{for } y_i = -1 \\end{aligned}$$     These conditions can be put more succinctly as:\n$$y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge a, \\forall i$$     Using the formulation of distance from origin of three hyper planes, we can show that, the margin, M is equivalent to $d_+ + d_- = 2a / \\lVert \\mathbf{w} \\rVert$. Without any loss of generality, we can set \\(a = 1\\)   , since it only sets the scale (units) of \\(b\\)    and $\\mathbf{w}$. So to maximize the margin, we have to maximize \\(1 / \\lVert \\mathbf{w} \\rVert\\)   . Such a non-convex objective function can be avoided if we choose in stead to minimize \\({\\lVert \\mathbf{w} \\rVert}^2\\)   .\nIn summary, for a problem with m numbers of training data points, we need to solve the following quadratic programming problem:\n$$\\begin{aligned} \u0026amp; {\\text{maximize }} M \\\\\u0026amp; \\text{subject to } y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge M, \\forall \\text{ } i = 1 \\ldots m \\end{aligned}$$     This can be more conveniently put as:\n$$\\begin{aligned} \u0026amp; {\\text{minimize }} f(w) \\equiv \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \\\\ \u0026amp; \\text{subject to } g(\\mathbf{w}, b) \\equiv -y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \u0026#43; 1 \\le 0, i = 1 \\ldots m \\end{aligned}$$     The maximal margin classifier is a very natural way to perform classification, if a separating hyper plane exists. However, in most real-life cases no separating hyper plane exists, and so there is no maximal margin classifier.\nSupport Vector Classifier  We can extend the concept of a separating hyper plane in order to develop a hyper plane that almost separates the classes, using a so-called soft margin. The generalization of the maximal margin classifier to the non-separable case is known as the support vector classifier.\nAssuming the classes overlap in the given feature space. One way to deal with the overlap is to still maximize M, but allow for some points to be on the wrong side of the margin. In order to allow these, we can define the slack variables as, $\\xi = ( \\xi_1, \\xi_2 \\ldots \\xi_m)$. Now, keeping the above optimization problem as a convex problem, we can modify the constraints as:\n$$\\begin{aligned} \u0026amp; y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge M(1-\\xi_i), \\forall \\text{ } i = 1 \\ldots m, \\\\ \u0026amp; \\xi_i \\ge 0 \\text{ and } \\sum_{i=1}^{m}\\xi_i \\le C \\text{ }\\forall \\text{ } i = 1 \\ldots m, \\end{aligned}$$     We can think of this formulation in the following context. The value $\\xi_i$ in the constraint $y_i (\\mathbf{x}_i^T\\mathbf{w} + b) \\ge M(1-\\xi_i)$ is the proportional amount by which the prediction $f(x_i)=x_i^T\\mathbf{w} + b$ is on the wrong side of its margin. Hence by bounding the sum $\\sum \\xi_i$, we can bound the total proportional amount by which predictions fall on the wrong side of their margin. Mis-classifications occur when $\\xi_i \u0026gt; 1$, so bounding $\\sum \\xi_i$ at a value K say, bounds the total number of training mis-classifications at K.\nSimilar to the case of maximum margin classifier, we can rewrite the optimization problem more conveniently as,\n$$\\begin{aligned} \u0026amp; {\\text{minimize }} \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \u0026#43; C \\sum_{i}^{m} \\xi_i\\\\ \u0026amp; \\text{subject to } y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) \\ge 1 - \\xi_i, \\text{ } \\text{ and } \\xi_i \\ge 0, \\text{ } i = 1 \\ldots m \\end{aligned}$$     Now, the question before us is to find a way to solve this optimization problem efficiently.\nThe problem above is quadratic with linear constraints, hence is a convex optimization problem. We can describe a quadratic programming solution using Lagrange multipliers and then solving using the Wolfe dual problem.\nThe Lagrange (primal) function for this problem is:\n$$L_P = \\frac{1}{2} {\\lVert \\mathbf{w} \\rVert}^2 \u0026#43; C \\sum_{i}^{m} \\xi_i - \\sum_{i=1}^{m} \\alpha_i[y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i)] - \\sum_{i=1}^{m} \\mu_i \\xi_i,$$     which we can minimize w.r.t. $\\mathbf{w}$, b, and $\\xi_i$. Setting the respective derivatives to zero, we get,\n$$\\begin{aligned} \u0026amp; \\mathbf{w} = \\sum_{i=1}^{m} \\alpha_i y_i \\mathbf{x_i} \\\\ \u0026amp; 0 = \\sum_{i=1}^{m} \\alpha_i y_i \\\\ \u0026amp; \\alpha_i = C - \\mu_i, \\forall i, \\end{aligned}$$     as well as the positivity constraints, $\\alpha_i$, $\\mu_i$, $\\xi_i \\ge 0, \\text{ } \\forall i$. By substituting these conditions back into the Lagrange primal function, we get the Wolfe dual of the problem as,\n$$L_D = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j x_i^T x_j$$     which gives a lower bound on the original objective function of the quadratic programming problem for any feasible point. We maximize \\(L_D\\)    subject to \\(0 \\le \\alpha_i \\le C\\)    and \\(\\sum_{i=1}^{m} \\alpha_i y_i = 0\\)   . In addition to above constraints, the Karush-Kuhn-Tucker (KKT) conditions include the following constraints,\n$$\\begin{aligned} \u0026amp; \\alpha_i[y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i)] = 0, \\\\ \u0026amp; \\mu_i \\xi_i = 0, \\\\ \u0026amp; y_i (\\mathbf{x}_i^T\\mathbf{w} \u0026#43; b) - (1 - \\xi_i) \\ge 0, \\end{aligned}$$     for $i = 1 \\ldots m$. Together these equations uniquely characterize the solution to the primal and the dual problem.\nLet us look at some special properties of the solution. We can see that the solution for $\\mathbf{w}$ has the for\n$$\\mathbf{\\hat{w}} = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x_i}$$     with nonzero coefficients $\\hat{\\alpha}_i$ only for those i for which $y_i (\\mathbf{x}_i^T\\mathbf{w} + b) - (1 - \\xi_i) = 0$. These i observations are called \u0026ldquo;support vectors\u0026rdquo; since $\\mathbf{w}$ is represented in terms of them alone. Among these support points, some will lie on the edge of the margin $(\\hat{\\xi}_i = 0)$, and hence characterized by $0 \u0026lt; \\hat{\\alpha}_i \u0026lt; C$; the remainder $(\\hat{\\xi}_i \u0026gt; 0)$ have $\\hat{\\alpha}_i = C$. Any of these margin points can be used to solve for b. Typically, once can use an average value from all of the solutions from the support points.\nIn this formulation, C is model hyper parameter and can be used as a regularizer to control the capacity and generalization error of the model.\nThe Kernel Trick The support vector classifier described so far finds linear boundaries in the input feature space. As with other linear methods, we can make the procedure more flexible by enlarging the feature space using basis expansions such as polynomials or splines. Generally linear boundaries in the enlarged space achieve better training-class separation, and translate to nonlinear boundaries in the original space. Once the basis functions $h_i(x), i=1 \\ldots m$ are selected, the procedure remains same as before.\nNow recall that in calculating the actual classifier, we needed only support vector points, i.e. we need smaller amount of computation if data has better training-class separation. Furthermore, if one looks closely, we can find an additional trick. The separating plane can be given by the function:\n$$\\begin{aligned} f(x) \u0026amp; = \\mathbf{x}^T \\mathbf{w} \u0026#43; b \\\\ \u0026amp; = \\mathbf{x}^T \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x_i} \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\mathbf{x}^T \\mathbf{x}_i \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\langle\\mathbf{x} \\mathbf{x}_i\\rangle \u0026#43; b \\end{aligned}$$     where, $\\langle \\mathbf{x} \\mathbf{y} \\rangle$ denotes inner product of vectors $\\mathbf{x}$ and $\\mathbf{y}$. This shows us that we can rewrite training phase operations completely in terms of inner products!\nIf we were to replace linear terms with a predefined non-linear operation $h(x)$, the above formulation of the separating plane will simply modify into:\n$$\\begin{aligned} f(x) \u0026amp; = h(\\mathbf{x})^T \\mathbf{w} \u0026#43; b \\\\ \u0026amp; = h(\\mathbf{x})^T \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i h(\\mathbf{x}_i) \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i h(\\mathbf{x})^T h(\\mathbf{x}_i) \u0026#43; b\\\\ \u0026amp; = \\sum_{i=1}^{m} \\hat{\\alpha_i} y_i \\langle h(\\mathbf{x}) h(\\mathbf{x}_i) \\rangle \u0026#43; b \\end{aligned}$$     As before, given $\\hat{\\alpha_i}$, b can be determined by solving $y_i f(\\mathbf{x}_i) = 1$ for any (or all) $x_i$ for which $0 \u0026lt; \\hat{\\alpha}_i \u0026lt; C$. More importantly, this tells us that we do not need to specify the exact nonlinear transformation $h(x)$ at all, rather only the knowledge of the Kernel function \\(K(x, x\u0026#39;) = \\langle h(x)h(x\u0026#39;) \\rangle\\)    that computes inner products in the transformed space is enough. Note that for the dual problem to be convex quadratic programming problem, $K$ would need to be symmetric positive semi-definite.\nSome common choices of kernels are:\n $d^{th}$ degree polynomial: \\(K(x, x\u0026#39;) = (1\u0026#43;\\langle x x\u0026#39; \\rangle )^d\\)   \n Radial basis: \\(K(x, x\u0026#39;) = \\exp (-\\gamma \\lVert \\mathbf{x - x\u0026#39;} \\rVert^2 )\\)   \n Neural network: \\(K(x, x\u0026#39;) = \\tanh (\\kappa_1 \\langle x x\u0026#39; \\rangle \u0026#43; \\kappa_2)\\)   \nThe role of the hyper-parameter $C$ is clearer in an enlarged feature space, since perfect separation is often achievable there. A large value of $C$ will discourage any positive $\\xi_i$, and lead to an over-fit wiggly boundary in the original feature space; a small value of $C$ will encourage a small value of $\\lVert w \\rVert$, which in turn causes $f(x)$ and hence the boundary to be smoother, potentially at the cost of more points as support vectors.\nCurse of Dimensionality\u0026hellip;. huh!!! With m training examples, $p$ predictors and M support vectors, the SVM requires $M^3 + Mm + mpM$ operations. This suggests the choice of the kernel and hence number of support vectors M will play a big role in feasibility of this method. For a really good choice of kernel that leads to very high training-class separation, i.e. $M \u0026lt;\u0026lt;\u0026lt; m$, the method can be viewed as linear in m. However, for a bad choice case, $M \\approx m$ we will be looking at an $O (m^3)$ algorithm.\nThe modern incarnation of deep learning was designed to overcome these limitations (large order of computations and clever problem-specific choice of kernels) of kernel machines. We will look at the details of a generic deep learning algorithm in a future post.\n","title":"Support Vector Machines","url":"https://sadanand-singh.github.io/posts/svmmodels/"},{"tags":"Programming, Python","text":"In the Week 1 we got started with Python. Now that we can interact with python, lets dig deeper into it.\nThis week we will go over some additional fundamental things common in any program - interactive input from users, adding comments to your code, use of conditional logic i.e. if - else conditions, loops, formatted output with strings and print() statements.\n\nPython Week 2 User Inputs There are hardly any programs without any input. Input can come in various ways, for example from a database, another computer, mouse clicks and movements or from the internet. Yet, in most cases the input stems from the keyboard. For this purpose, Python provides the function input(). input() has an optional parameter, which is the prompt string, i.e. the text that will be shown when asking for input.\nIn\u0026nbsp;[1]: name = input(\u0026quot;What\u0026#39;s your name? \u0026quot;) print(\u0026quot;Nice to meet you \u0026quot; + name + \u0026quot;!\u0026quot;) age = input(\u0026quot;Your age? \u0026quot;) print(\u0026quot;So, you are already \u0026quot; + age + \u0026quot; years old, \u0026quot; + name + \u0026quot;!\u0026quot;)    \n What\u0026#39;s your name? Sadanand Nice to meet you Sadanand! Your age? 30 So, you are already 30 years old, Sadanand!     \n  What if you try to do some mathematical operation on the age? You will get a TypeError as follows:\n   In\u0026nbsp;[2]: age = 12 + age \n  \n  --------------------------------------------------------------------------- TypeError Traceback (most recent call last) \u0026lt;ipython-input-2-3d9ce720d6f3\u0026gt; in \u0026lt;module\u0026gt;() ----\u0026gt; 1age = 12 + age TypeError: unsupported operand type(s) for +: \u0026#39;int\u0026#39; and \u0026#39;str\u0026#39;    \n  This says that by default all data is read as raw input i.e. strings. If we want numbers we need to convert them ourselves. For example:\n   In\u0026nbsp;[3]:  cities_canada = input(\u0026quot;Largest cities in Canada: \u0026quot;) \n  \n Largest cities in Canada: [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;]     \n In\u0026nbsp;[4]:  print(cities_canada, type(cities_canada)) \n  \n [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;] \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[5]: cities_canada = eval(input(\u0026quot;Largest cities in Canada: \u0026quot;)) \n  \n Largest cities in Canada: [\u0026#34;Montreal\u0026#34;, \u0026#34;Ottawa\u0026#34;, \u0026#34;Calgary\u0026#34;, \u0026#34;Toronto\u0026#34;]     \n In\u0026nbsp;[6]:  print(cities_canada, type(cities_canada)) \n  \n [\u0026#39;Montreal\u0026#39;, \u0026#39;Ottawa\u0026#39;, \u0026#39;Calgary\u0026#39;, \u0026#39;Toronto\u0026#39;] \u0026lt;class \u0026#39;list\u0026#39;\u0026gt;     \n In\u0026nbsp;[7]: population = input(\u0026quot;Population of Portland? \u0026quot;) \n  \n Population of Portland? 604596     \n In\u0026nbsp;[8]:  print(population, type(population)) \n  \n 604596 \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[9]: population = int(input(\u0026quot;Population of Portland? \u0026quot;)) \n  \n Population of Portland? 604596     \n In\u0026nbsp;[10]:  print(population, type(population)) \n  \n 604596 \u0026lt;class \u0026#39;int\u0026#39;\u0026gt;     \n In\u0026nbsp;[13]: pi = input(\u0026quot;Value of PI is?\u0026quot;) \n  \n Value of PI is?3.14     \n In\u0026nbsp;[14]:  print(pi, type(pi)) \n  \n 3.14 \u0026lt;class \u0026#39;str\u0026#39;\u0026gt;     \n In\u0026nbsp;[15]: pi = float(input(\u0026quot;Value of PI is?\u0026quot;)) \n  \n Value of PI is?3.14     \n In\u0026nbsp;[16]:  print(pi, type(pi)) \n  \n 3.14 \u0026lt;class \u0026#39;float\u0026#39;\u0026gt;     \n\nNotice the use of various methods like eval(), int() and float() to get user input in correct formats. In summary, eval() is used to get data into various native python formats, e.g. lists, dictionaries etc. We will look at these in more detail in next few tutorials. int() is used to convert input to integer numbers (numbers without decimals), while float() is used to get floating point numbers.\nAlso, of interest above is the type() method used in print statements. You can get the type of any variable in python using this method. In the output of this we see something like: \u0026lt; class \u0026lsquo;float\u0026rsquo;\u0026gt; - if variable is of float type. For the time being we will ignore the \u0026ldquo;class\u0026rdquo; in this.\nIndentation Blocks Python programs get structured through indentation, i.e. code blocks are defined by their indentation (The amount of blank space before any line). This principle makes it easier to read and understand other people\u0026rsquo;s Python code.\nAll statements with the same distance to the right belong to the same block of code, i.e. the statements within a block line up vertically. The block ends at a line less indented or the end of the file. If a block has to be more deeply nested, it is simply indented further to the right.\nIn the following sections below we will see extensive use of such indentation blocks. Consider the following example to calculate Pythagorean triples. You do not need to understand the full code right here. We will revisit this code at the end of this tutorial.\nIn\u0026nbsp;[18]: from math import sqrt n = input(\u0026quot;Maximum Number? \u0026quot;) n = int(n)+1 for a in range(1,n): for b in range(a,n): c_square = a**2 + b**2 c = int(sqrt(c_square)) if ((c_square - c**2) == 0): print(a, b, c)    \n Maximum Number? 10 3 4 5 6 8 10     \n\nIn the above code, we see three indentation blocks, first and second \u0026ldquo;for\u0026rdquo; loops and the third \u0026ldquo;if\u0026rdquo; condition. There is another aspect of structuring in Python, which we haven\u0026rsquo;t mentioned so far, which you can see in the example. Loops and Conditional statements end with a colon \u0026ldquo;:\u0026rdquo; - the same is true for functions and other structures introducing blocks. So, we should have said Python structures by colons and indentation.\nComments in Python Python has two ways to annotate/comment Python code. One is by using comments to indicate what some part of the code does. Single-line comments begin with the hash character (\u0026ldquo;#\u0026rdquo;) and are terminated by the end of line. Here is an example:\nIn\u0026nbsp;[19]: # This is a comment in Python before print statement print(\u0026quot;Hello World\u0026quot;) #This is also a comment in Python    \n Hello World     \n\nConditionals Conditionals, - mostly in the form of if statements - are one of the essential features of a programming language. A decision has to be taken when the script or program comes to a point where it has a choice of actions, i.e. different computations, to choose from.\nThe decision depends in most cases on the value of variables or arithmetic expressions. These expressions are evaluated to the Boolean values True or False. The statements for the decision taking are called conditional statements. Alternatively they are also known as conditional expressions or conditional constructs.\nConditional statements in Python use indentation blocks to conditionally execute certain code. The general form of the if statement in Python looks like this:\nIn\u0026nbsp;[12]: if condition_1: statement_block_1 elif condition_2: statement_block_2 ... elif another_condition: another_statement_block else: else_block    \n     \n\nIf the condition \u0026ldquo;condition_1\u0026rdquo; is True, the statements of the block statement_block_1 will be executed. If not, condition_2 will be evaluated. If condition_2 evaluates to True, statement_block_2 will be executed, if condition_2 is False, the other conditions of the following elif conditions will be checked, and finally if none of them has been evaluated to True, the indented block below the else keyword will be executed.\nTypical examples of \u0026ldquo;condition\u0026rdquo; statements follow some of following operations: mathematical comparisons like, \u0026ldquo;\u0026lt;\u0026rdquo;, \u0026ldquo;\u0026gt;\u0026rdquo;, \u0026ldquo;\u0026lt;=\u0026ldquo;, \u0026ldquo;\u0026gt;=\u0026ldquo;, \u0026ldquo;==\u0026rdquo; object comparisons like \u0026ldquo;is\u0026rdquo; i.e. this is exactly something or not. boolean logic operators like \u0026ldquo;not\u0026rdquo;, \u0026ldquo;or\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;xor\u0026rdquo; etc.\nThe following objects are evaluated by Python as False:\n numerical zero values (0, 0L, 0.0, 0.0+0.0j), the Boolean value False, empty strings, empty lists and empty tuples, empty dictionaries. the special value None.  All other values are considered to be True. Let us try to solve this simple DNA sequence problem: Given the an input DNA sequence, print the sequence if its length is less than equal to 20. Print \u0026ldquo;Error\u0026rdquo; if the sequence is empty or its length is larger than 25. If length is between 21 and 25, print the last 5 bases only.\n\nIn\u0026nbsp;[20]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNNAATTCCGG\u0026quot; if len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;)    \n ERROR!     \n In\u0026nbsp;[21]: dna = \u0026quot;ATGCAATGCN\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n ATGCAATGCN     \n In\u0026nbsp;[22]: dna = \u0026quot;\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n ERROR!     \n In\u0026nbsp;[23]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot;\nif len(dna) \u0026lt;= 20: if len(dna) \u0026gt; 0: print(dna) else: print(\u0026quot;ERROR!\u0026quot;) elif len(dna) \u0026lt;= 25: print(dna[-5:]) else: print(\u0026quot;ERROR!\u0026quot;) \n  \n CCNNN     \n\nif else conditions can also be combined in a regular assignment expression to assign values. For example, In the DNA case, we want to store length of DNA. However, we want length to number only if length of sequence is between 1 and 25. In all other cases, we want to store the length of sequence as -1. A typical way to do this would be:\nIn\u0026nbsp;[24]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; length = -1 if 0 \u0026lt; len(dna) \u0026lt;= 20: length = len(dna) print(length)    \n -1     \n In\u0026nbsp;[25]: dna = \u0026quot;CCGGGAACCTCACG\u0026quot; length = -1 if 0 \u0026lt; len(dna) \u0026lt;= 20: length = len(dna)\nprint(length) \n  \n 14     \n  This example can be written in a much shorter fashion as well. Such conditions are commonly called as ternary if statements.\n   In\u0026nbsp;[26]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; length = len(dna) if 0 \u0026lt; len(dna) \u0026lt;= 20 else -1 print(length) \n  \n -1     \n In\u0026nbsp;[27]: dna = \u0026quot;CCGGGAACCTCACG\u0026quot; length = len(dna) if 0 \u0026lt; len(dna) \u0026lt;= 20 else -1 print(length) \n  \n 14     \n\nLoops Many algorithms make it necessary for a programming language to have a construct which makes it possible to carry out a sequence of statements repeatedly. The code within the loop, i.e. the code carried out repeatedly, is called the body of the loop.\nThere are two types of loops in Python -\n while Loop for Loop  The while Loop\nThese are a type of loop called \u0026ldquo;Condition-controlled loop\u0026rdquo;. As suggested by the name, the loop will be repeated until a given condition changes, i.e. changes from True to False or from False to True, depending on the kind of loop.\nLet us consider the following example of DNA sequence: We want to print every base of a given sequence, until we have found 2 A\u0026rsquo;s.\nIn\u0026nbsp;[28]: dna = \u0026quot;ATGCCGATTTATCGGGAACCNNN\u0026quot; countA = 0 index = 0 while countA \u0026lt; 2: print(dna[index]) if dna[index] == \u0026#39;A\u0026#39;: countA = countA + 1 index = index + 1    \n A T G C C G A     \n\nIn the above example, the loop (code under the while block) was executed until countA \u0026lt; 2 statement remained true.\nThe loops can be made to exit before its actual completion using the break statements. Consider the following example of DNA sequence. We want to print every base of a given sequence, until we have found 2 A\u0026rsquo;s. However, we want to stop printing as soon as we have found an N base.\nIn\u0026nbsp;[29]: dna = \u0026quot;ATGCNCGATTTATCGGGAACCNNN\u0026quot; countA = 0 index = 0 while countA \u0026lt; 2: if dna[index] == \u0026#39;N\u0026#39;: break if dna[index] == \u0026#39;A\u0026#39;: countA = countA + 1 print(dna[index]) index = index + 1    \n A T G C     \n\nNow, let us consider another case while looping over something. We want to skip over a part of code at certain condition. In such cases, continue statement comes handy.\nConsider the following example wrt to DNA sequencing. Given a sequence of dna, we do NOT want to print the base name if it is \u0026lsquo;N\u0026rsquo;\nIn\u0026nbsp;[30]: dna = \u0026quot;ATGCNCN\u0026quot; index = 0 while index \u0026lt; len(dna): index = index + 1 if dna[index-1] == \u0026#39;N\u0026#39;: continue print(dna[index-1])    \n A T G C C     \n\nThe for Loop\nA for loop is similar to while loop, except it is used to loop over certain elements, unlike while loop that continues until certain condition is satisfied. In the case DNA sequences, say, one case of for loop would be to loop over all bases in a sequence.\nConsider the following example: Given a DNA sequence, we want to count the number of all \u0026lsquo;A\u0026rsquo;, and \u0026rsquo;T bases.\nIn\u0026nbsp;[31]: dna = \u0026quot;ATGCNCGATTTATCGGGAACCNNN\u0026quot; count = 0 for base in dna: if base == \u0026#39;A\u0026#39; or base == \u0026#39;T\u0026#39;: count += 1 print(\u0026quot;Number of A, T bases is:\u0026quot;, count)    \n Number of A, T bases is: 10     \n\nSimilar to while loops, we can use break and continue statements with for loops as well.\nLet us look at somewhat complicated use of for loop:\nGiven a DNA sequence, we want to count the number of doublets of bases, i.e. no. of times certain bases come twice exactly. If some base occur more than twice, we do not want to count that.\nIn\u0026nbsp;[32]: dna = \u0026quot;ATGGCNCGAATTTAAATCGGGAACCNNN\u0026quot; countPairs = 0 pairFound = 0 prevBase = \u0026#39;\u0026#39; for base in dna: if (base == prevBase): pairFound += 1 else: if pairFound == 1: countPairs += 1 pairFound = 0 prevBase = base print(\u0026quot;Number of paired bases is:\u0026quot;, countPairs)    \n Number of paired bases is: 4     \n\nFormatting of Output Final topic for this week is the formatting of text in the print statements. Consider the following case:\nWe have following variables: name = \u0026quot;Sadanand\u0026quot;, age = 30, and gender = \u0026quot;male\u0026quot;\nWe would like to print a quite cumbersome statement like as follows. This can be quite easily done using the format method.\nIn\u0026nbsp;[33]: name = \u0026quot;Sadanand\u0026quot; age = 30 gender = \u0026quot;male\u0026quot; msg = \u0026quot;Hi {0}, You are a {1}, and you have seen {2}winters as you are {2}years old! Thanks {0}!\u0026quot; print(msg.format(name, gender, age))    \n Hi Sadanand, You are a male, and you have seen 30 winters as you are 30 years old! Thanks Sadanand!     \n\nThus format method provides us with easy way to mix different types of variables in the strings.\nThats it for this week. Next we will look at strings and lists in Python in more detail.\nExercise\nGiven the following sequence of dna - \u0026ldquo;ATGGCNCGAATTTAAATCGGGAACCNNN\u0026rdquo;,\n Write a program to count number of all triplets in it. Write a program that prints all non \u0026rsquo;T\u0026rsquo; bases that come after \u0026rsquo;T\u0026rsquo;, but stops when two or more continuous \u0026rsquo;T\u0026rsquo; has been found. Write a program to generate new sequence with every 3rd base from the above sequence. Write a program to calculate sum of all numbers from 1 to 10. HINT: Please take a look at the range method. ","title":"Python Tutorial - Week 2","url":"https://sadanand-singh.github.io/posts/pythontutorialweek2/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In the last post we looked at some initial cleanup of the data. We will start from there by loading the pickled dataframe.\n\nIn\u0026nbsp;[1]: import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import numpy as np from matplotlib import pyplot as plt import seaborn as sns %matplotlib inline df = pd.read_pickle(\u0026quot;/home/ssingh/LendingClubData/Part1.pickle\u0026quot;)    \n  Lets first check what all columns are remaining in our dataframe. As there are still more than 100 variables left, we will initially focus on the first 25 ones only.\n   In\u0026nbsp;[2]: print(df.columns) print(df.columns.shape) \n  \n Index([\u0026#39;loan_amnt\u0026#39;, \u0026#39;funded_amnt\u0026#39;, \u0026#39;funded_amnt_inv\u0026#39;, \u0026#39;term\u0026#39;, \u0026#39;int_rate\u0026#39;, \u0026#39;installment\u0026#39;, \u0026#39;grade\u0026#39;, \u0026#39;sub_grade\u0026#39;, \u0026#39;emp_length\u0026#39;, \u0026#39;home_ownership\u0026#39;, ... \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;, \u0026#39;pct_tl_nvr_dlq\u0026#39;, \u0026#39;percent_bc_gt_75\u0026#39;, \u0026#39;pub_rec_bankruptcies\u0026#39;, \u0026#39;tax_liens\u0026#39;, \u0026#39;tot_hi_cred_lim\u0026#39;, \u0026#39;total_bal_ex_mort\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;total_il_high_credit_limit\u0026#39;], dtype=\u0026#39;object\u0026#39;, length=111) (111,)     \n  From the data dictionary, we can see that funded_amnt is total amount committed till now, and funded_amnt_inv is the amount funded by investors. It is difficult to think of a direct correlation between the charged interest rate and the actual funded amount. However, this amount can give us a range of risk that one will be taking when investing. Given the two committed amounts are very similar, we will drop the the \u0026ldquo;funded_amnt\u0026rdquo; column. The installment column gives us feel of how much burden the loan will be on the borrower. However, this will be direct function of term and rate of the loan and hence should be dropped from any further analysis. the \u0026ldquo;grade\u0026rdquo; and \u0026ldquo;sub_grade\u0026rdquo; are LC assigned grades to the loan. We can keep these as secondary variables to check the liability of models used by LC.\n   In\u0026nbsp;[3]: df.drop([\u0026#39;funded_amnt\u0026#39;, \u0026#39;installment\u0026#39;, \u0026quot;pymnt_plan\u0026quot;],1, inplace=True) \n  \n In\u0026nbsp;[4]: df.ix[:4,11:19] \n  \nOut[4]:    url desc purpose title zip_code addr_state dti delinq_2yrs     0 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 235xx VA 12.03 0   1 https://www.lendingclub.com/browse/loanDetail.... NaN credit_card Credit card refinancing 937xx CA 14.92 0   2 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 850xx AZ 34.81 0   3 https://www.lendingclub.com/browse/loanDetail.... NaN car Car financing 953xx CA 8.31 1   4 https://www.lendingclub.com/browse/loanDetail.... NaN debt_consolidation Debt consolidation 077xx NJ 25.81 0      \n \n  For our purpose, we will not be going into any kind of natural language processing, hence, the description and the url variables are of no use to us.\n   In\u0026nbsp;[5]: df.drop([\u0026#39;url\u0026#39;, \u0026#39;desc\u0026#39;],1, inplace=True) \n  \n  Let us check what are typical \u0026ldquo;purpose\u0026rdquo; used for requesting loans. We can view this as a histogram plot.\n   In\u0026nbsp;[6]: sns.set() sns.set_context(\u0026quot;notebook\u0026quot;, font_scale=1.5, rc={\u0026quot;lines.linewidth\u0026quot;: 2.5}) total = float(len(df.index)) ax = sns.countplot(x=\u0026quot;purpose\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); ax.set(yscale = \u0026quot;log\u0026quot;) plt.xticks(rotation=90) plt.show() \n  \n  \n \n  We can also look for any kind of correlation between the purpose and the interest rate of loan using a box plot. We can clearly see this could be useful for building our model!\n   In\u0026nbsp;[7]: sns.boxplot(x=\u0026quot;purpose\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) plt.xticks(rotation=90) plt.show() \n  \n  \n \n  Let also look for any kind of correlations between \u0026ldquo;employment length\u0026rdquo;, \u0026ldquo;rate\u0026rdquo; and \u0026ldquo;status\u0026rdquo; of loans. Status here, if you remember from the previous post refers to the risk factor involved with the loan.\n   In\u0026nbsp;[8]: sns.set(style=\u0026quot;ticks\u0026quot;, color_codes=True) sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;emp_length\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[8]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e52116278\u0026gt;  \n  \n \n  As expected, we find good loans to have larger employment length. Interestingly, interest rate tends to be all over the place for high risk loans. But, if you think about it, that is what we are trying to fix here!\nAnalyzing tile of loans could be tricky. Again, due to lack of any kind of natural language processing, let us drop this as well.\nThe location address of borrowers can say interesting pattern about the interest rates. First three letters of zip code can give much more information than states. However, if the zip info is missing, state can provide a reasonable approx. of the data. Lets check if we have any data where zip data is missing. If none, we can simply drop the state information.\n   In\u0026nbsp;[9]: df[\u0026#39;zip_code\u0026#39;] = df[\u0026#39;zip_code\u0026#39;].str.replace(\u0026#39;xx\u0026#39;,\u0026#39;\u0026#39;) \n  \n In\u0026nbsp;[10]: df.drop([\u0026#39;title\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[11]: df.zip_code.isnull().sum() \n  \nOut[11]: 0  \n \n In\u0026nbsp;[12]: df.drop([\u0026#39;addr_state\u0026#39;],1, inplace=True) \n  \n  The \u0026ldquo;dti\u0026rdquo; column in the data dictionary has been described as - \u0026ldquo;A ratio calculated using the borrower\u0026rsquo;s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower\u0026rsquo;s self-reported monthly income\u0026rdquo;. Based on this information, Debt_to_Income ratio is a direct measure of the loan risk.\nLets check effects of delinquency over last 2 years on interest rate using a box plot:\n   In\u0026nbsp;[13]: sns.boxplot(x=\u0026quot;delinq_2yrs\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) \n  \nOut[13]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e502c3a58\u0026gt;  \n  \n \n  We can see visualize effects of delinquency over last 2 years. Let us bin this data into three bins - Low, Medium and High. We will now move on to the next set of columns.\n   In\u0026nbsp;[14]: df[\u0026quot;delinq_2yrs\u0026quot;] = pd.cut(df.delinq_2yrs, bins=3, labels=[\u0026quot;Low\u0026quot;, \u0026quot;Medium\u0026quot;, \u0026quot;High\u0026quot;], include_lowest = True) \n  \n In\u0026nbsp;[15]: df.ix[:4,15:23] \n  \nOut[15]:    earliest_cr_line fico_range_low fico_range_high inq_last_6mths mths_since_last_delinq mths_since_last_record open_acc pub_rec     0 Aug-1994 750 754 0 NaN NaN 6 0   1 Sep-1989 710 714 2 42.0 NaN 17 0   2 Aug-2002 685 689 1 NaN NaN 11 0   3 Oct-2000 665 669 0 17.0 NaN 8 0   4 Nov-1992 680 684 0 NaN NaN 12 0      \n \n  Earliest credit line should play an important role in determining the rate. We will replace this column by something more quantitative - credit_age.\n   In\u0026nbsp;[16]: now = pd.Timestamp(\u0026#39;20160501\u0026#39;) df[\u0026quot;credit_age\u0026quot;] = pd.to_datetime(df.earliest_cr_line, format=\u0026quot;%b-%Y\u0026quot;) df[\u0026#39;credit_age\u0026#39;] = (now - df[\u0026#39;credit_age\u0026#39;]).dt.days.divide(30).astype(\u0026quot;int64\u0026quot;) df.drop([\u0026#39;earliest_cr_line\u0026#39;],1, inplace=True) \n  \n  Let us try to find a trend between interest rate, fico ranges and loan status.\n   In\u0026nbsp;[17]: sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;fico_range_low\u0026quot;, \u0026quot;fico_range_high\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[17]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e5239cef0\u0026gt;  \n  \n \n  We find 2 FICO scores to be highly collinear. Further, high risk loans have much larger lower values of fico scores. We can safely replace these with the mean values of fico scores.\n   In\u0026nbsp;[18]: df[\u0026#39;fico\u0026#39;] = 0.5*(df[\u0026#39;fico_range_high\u0026#39;] + df[\u0026#39;fico_range_low\u0026#39;]) df.drop([\u0026#39;fico_range_high\u0026#39;],1, inplace=True) df.drop([\u0026#39;fico_range_low\u0026#39;],1, inplace=True) \n  \n  Similar to the 2 year delinquency, let us also look at the 6 month inquiry data. Other data like mths_since_last_delinq and mths_since_last_record can be safely removed, as they will be correlated to 2 year delinquency data.\n   In\u0026nbsp;[19]: sns.boxplot(x=\u0026quot;inq_last_6mths\u0026quot;, y=\u0026quot;int_rate\u0026quot;, data=df) \n  \nOut[19]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e50ff59b0\u0026gt;  \n  \n \n  Let us find correlations between many of these similar variables.\n   In\u0026nbsp;[20]: sns.pairplot(df, vars=[\u0026quot;int_rate\u0026quot;, \u0026quot;pub_rec\u0026quot;, \u0026quot;open_acc\u0026quot;, \u0026quot;inq_last_6mths\u0026quot;], hue=\u0026quot;delinq_2yrs\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) \n  \nOut[20]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e50dea1d0\u0026gt;  \n  \n \n  Both open_acc and inq_last_6_mnths have a strong correlation with delinq_2year, and hence can be safely dropped. pub_rec too has a distinct shape for each levels of delinq_2yrs showing interdependence and hence we can drop this as well.\n   In\u0026nbsp;[21]: df.drop([\u0026#39;pub_rec\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_acc\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_last_6mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_delinq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_record\u0026#39;],1, inplace=True) \n  \n  We will now move on to the next set of columns.\n   In\u0026nbsp;[22]: df.ix[:4,15:25] \n  \nOut[22]:    revol_bal revol_util total_acc initial_list_status out_prncp out_prncp_inv total_pymnt total_pymnt_inv total_rec_prncp total_rec_int     0 138008 29% 17 w 12484.99 12484.99 4364.64 4364.64 2515.01 1849.63   1 6133 31.6% 36 w 6892.58 6892.58 4163.94 4163.94 3507.42 656.52   2 16822 91.9% 20 f 0.00 0.00 2281.98 2281.98 704.38 339.61   3 5753 100.9% 13 w 10868.67 10868.67 4117.57 4117.57 1931.33 2186.24   4 16388 59.4% 44 f 0.00 0.00 9973.43 9973.43 9600.00 373.43      \n \n  Revolving balance and revolving utilization, is a measure of \u0026ldquo;how leveraged your credit cards are\u0026rdquo;. revol_util should provide a relative measure of leverage, whereas revol_bal should provide an absolute measurement. Before we proceed, we need to convert \u0026lsquo;%\u0026rsquo; data to fraction.\n   In\u0026nbsp;[23]: df.revol_util = pd.Series(df.revol_util).str.replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;).astype(float) df.revol_util = df.revol_util * 0.01 \n  \n In\u0026nbsp;[24]: g = sns.pairplot(df, vars=[\u0026quot;revol_bal\u0026quot;, \u0026quot;revol_util\u0026quot;, \u0026quot;total_acc\u0026quot;], hue=\u0026quot;loan_status\u0026quot;, diag_kind=\u0026quot;kde\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n  None of these variables seem to make any direct correlation with the risk levels of the loan. Given their direct use in the FICO score calculation, we will keep these in our analysis.\n    Let us take a look at the initial listing status of the loan. Then, we can find a correlation between these and the risk level.\n   In\u0026nbsp;[25]: sns.countplot(x=\u0026quot;initial_list_status\u0026quot;, hue=\u0026quot;loan_status\u0026quot;, data=df) \n  \nOut[25]: \u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2e4fe23898\u0026gt;  \n  \n \n  For high risk loans as well low risk ones, there does not seem to be any significant difference among two types of initial listing of the loan and hence we can drop it.\n   In\u0026nbsp;[26]: df.drop([\u0026#39;initial_list_status\u0026#39;],1, inplace=True) \n  \n  Following variables remaining in the list refer to the current state of the loan and hence will not be playing any effect on the general state or risk level of the loan, therefore should be dropped from our analysis. We will also not consider any joint data for this analysis.\n   In\u0026nbsp;[27]: df.drop([\u0026#39;out_prncp\u0026#39;],1, inplace=True) df.drop([\u0026#39;out_prncp_inv\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_pymnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_pymnt_inv\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_prncp\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_int\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rec_late_fee\u0026#39;],1, inplace=True) df.drop([\u0026#39;recoveries\u0026#39;],1, inplace=True) df.drop([\u0026#39;collection_recovery_fee\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_pymnt_d\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_pymnt_amnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;next_pymnt_d\u0026#39;],1, inplace=True) df.drop([\u0026#39;policy_code\u0026#39;],1, inplace=True) df.drop([\u0026#39;application_type\u0026#39;],1, inplace=True) df.drop([\u0026#39;annual_inc_joint\u0026#39;],1, inplace=True) df.drop([\u0026#39;dti_joint\u0026#39;],1, inplace=True) df.drop([\u0026#39;verification_status_joint\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[28]: df.ix[:4,18:24] \n  \nOut[28]:    last_credit_pull_d last_fico_range_high last_fico_range_low collections_12_mths_ex_med mths_since_last_major_derog acc_now_delinq     0 Feb-2016 684 680 0 NaN 0   1 Feb-2016 679 675 0 59.0 0   2 Dec-2015 539 535 0 NaN 0   3 Feb-2016 704 700 0 36.0 0   4 Feb-2016 684 680 0 NaN 0      \n \n  First we need to convert, last credit pull day to a numeric value as days since lst credit pull. Let us find if there are any NA values.\n   In\u0026nbsp;[29]: print(\u0026quot;No. of Data with NA values = {}\u0026quot;.format(len(df.last_credit_pull_d) - df.last_credit_pull_d.count())) \n  \n No. of Data with NA values = 27     \n  We will replace these NA values with, Day corresponding with the oldest date of their account, i.e. now - credit history date.\n   In\u0026nbsp;[30]: df.last_credit_pull_d.fillna(\u0026quot;Jan-1980\u0026quot;, inplace=True) \n  \n In\u0026nbsp;[31]: df[\u0026quot;last_credit_pull_d\u0026quot;] = pd.to_datetime(df.last_credit_pull_d, format=\u0026quot;%b-%Y\u0026quot;) df[\u0026#39;last_credit_pull_d\u0026#39;] = (now - df[\u0026#39;last_credit_pull_d\u0026#39;]).dt.days.divide(30).astype(\u0026quot;int64\u0026quot;) df[df[\u0026#39;last_credit_pull_d\u0026#39;] \u0026gt;= 7000].last_credit_pull_d = df[df[\u0026#39;last_credit_pull_d\u0026#39;] \u0026gt;= 7000].credit_age \n  \n  Let us compare last fico score to the overall fico score.\n   In\u0026nbsp;[32]: sns.pairplot(df, vars=[\u0026quot;last_fico_range_high\u0026quot;, \u0026quot;last_fico_range_low\u0026quot;, \u0026quot;fico\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[32]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e5004d828\u0026gt;  \n  \n \n  As before, last fico high and low scores are correlated, and also with overall fico score, and hence we can get rid of these.\n   In\u0026nbsp;[33]: df.drop([\u0026#39;last_fico_range_high\u0026#39;],1, inplace=True) df.drop([\u0026#39;last_fico_range_low\u0026#39;],1, inplace=True) \n  \n  We can also get of \u0026ldquo;collections_12_mths_ex_med\u0026rdquo; column as this corresponds only to the current state of loan. Other two variables, \u0026ldquo;mths_since_last_major_derog\u0026rdquo; and \u0026ldquo;acc_now_delinq\u0026rdquo; should have no additional impact than ones already considered.\n   In\u0026nbsp;[34]: df.drop([\u0026#39;collections_12_mths_ex_med\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_last_major_derog\u0026#39;],1, inplace=True) df.drop([\u0026#39;acc_now_delinq\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[35]: df.ix[:4,19:29] \n  \nOut[35]:    tot_coll_amt tot_cur_bal open_acc_6m open_il_6m open_il_12m open_il_24m mths_since_rcnt_il total_bal_il il_util open_rv_12m     0 0 149140 NaN NaN NaN NaN NaN NaN NaN NaN   1 0 162110 NaN NaN NaN NaN NaN NaN NaN NaN   2 0 64426 NaN NaN NaN NaN NaN NaN NaN NaN   3 0 261815 NaN NaN NaN NaN NaN NaN NaN NaN   4 0 38566 NaN NaN NaN NaN NaN NaN NaN NaN      \n \n  Again, we can go on and delete all the columns that are related to only the current states of loans, including the ones with large amount of missing data.\n   In\u0026nbsp;[36]: df.drop([\u0026#39;tot_coll_amt\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_acc_6m\u0026#39;],1, inplace=True) df.drop([\u0026#39;tot_cur_bal\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_6m\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_12m\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_il_24m\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_rcnt_il\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_bal_il\u0026#39;],1, inplace=True) df.drop([\u0026#39;il_util\u0026#39;],1, inplace=True) df.drop([\u0026#39;open_rv_12m\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[37]: df.ix[:4,19:29] \n  \nOut[37]:    open_rv_24m max_bal_bc all_util total_rev_hi_lim inq_fi total_cu_tl inq_last_12m acc_open_past_24mths avg_cur_bal bc_open_to_buy     0 NaN NaN NaN 184500 NaN NaN NaN 5 29828.0 9525.0   1 NaN NaN NaN 19400 NaN NaN NaN 7 9536.0 7599.0   2 NaN NaN NaN 18300 NaN NaN NaN 6 5857.0 332.0   3 NaN NaN NaN 5700 NaN NaN NaN 2 32727.0 0.0   4 NaN NaN NaN 27600 NaN NaN NaN 8 3214.0 6494.0      \n \n  Out of these variables, only \u0026ldquo;avg_cur_bal\u0026rdquo; is viable additional feature for our model. We will also look at the distribution of average current balance. However, in order to use it correctly, we need to if there are any NA values in to and replace them correctly.\n   In\u0026nbsp;[38]: df.drop([\u0026#39;open_rv_24m\u0026#39;],1, inplace=True) df.drop([\u0026#39;max_bal_bc\u0026#39;],1, inplace=True) df.drop([\u0026#39;all_util\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_fi\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_cu_tl\u0026#39;],1, inplace=True) df.drop([\u0026#39;inq_last_12m\u0026#39;],1, inplace=True) df.drop([\u0026#39;acc_open_past_24mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;bc_open_to_buy\u0026#39;],1, inplace=True) df.drop([\u0026#39;total_rev_hi_lim\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[39]: print(\u0026quot;No. of Data with NA values = {}\u0026quot;.format(len(df.avg_cur_bal) - df.avg_cur_bal.count())) \n  \n No. of Data with NA values = 6     \n In\u0026nbsp;[40]: df.avg_cur_bal.fillna(df.avg_cur_bal.min(), inplace=True) \n  \n In\u0026nbsp;[41]: g = sns.pairplot(df, vars=[\u0026quot;avg_cur_bal\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n In\u0026nbsp;[42]: df.ix[:4,20:28] \n  \nOut[42]:    bc_util chargeoff_within_12_mths delinq_amnt mo_sin_old_il_acct mo_sin_old_rev_tl_op mo_sin_rcnt_rev_tl_op mo_sin_rcnt_tl mort_acc     0 4.7 0 0 103.0 244 1 1 0   1 41.5 0 0 76.0 290 1 1 1   2 93.2 0 0 137.0 148 8 8 0   3 103.2 0 0 16.0 170 21 16 5   4 69.2 0 0 183.0 265 23 3 0      \n \n  Similar to before, we can again get rid of variables that will not make significant impact on our model. Then look at the pair-wise effect of rest of them. We will also replace NAs with the mean values.\n   In\u0026nbsp;[43]: df.drop([\u0026#39;chargeoff_within_12_mths\u0026#39;],1, inplace=True) df.drop([\u0026#39;delinq_amnt\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_old_il_acct\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_old_rev_tl_op\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_rcnt_rev_tl_op\u0026#39;],1, inplace=True) df.drop([\u0026#39;mo_sin_rcnt_tl\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[44]: df.bc_util.fillna(df.bc_util.min(), inplace=True) \n  \n In\u0026nbsp;[45]: sns.pairplot(df, vars=[\u0026quot;bc_util\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[45]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4e1ab550\u0026gt;  \n  \n \n In\u0026nbsp;[46]: df.ix[:4,21:27] \n  \nOut[46]:    mort_acc mths_since_recent_bc mths_since_recent_bc_dlq mths_since_recent_inq mths_since_recent_revol_delinq num_accts_ever_120_pd     0 0 47.0 NaN NaN NaN 0   1 1 5.0 42.0 1.0 42.0 4   2 0 17.0 NaN 3.0 NaN 0   3 5 21.0 17.0 1.0 17.0 1   4 0 24.0 NaN 17.0 NaN 0      \n \n  In this list only variable of our interest is number of mortgage accounts.\n   In\u0026nbsp;[47]: sns.pairplot(df, vars=[\u0026quot;mort_acc\u0026quot;, \u0026quot;int_rate\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[47]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4dd0ed30\u0026gt;  \n  \n \n In\u0026nbsp;[48]: df.drop([\u0026#39;mths_since_recent_bc\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_bc_dlq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_inq\u0026#39;],1, inplace=True) df.drop([\u0026#39;mths_since_recent_revol_delinq\u0026#39;],1, inplace=True) df.drop([\u0026#39;num_accts_ever_120_pd\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[49]: df.ix[:4,22:31] \n  \nOut[49]:    num_actv_bc_tl num_actv_rev_tl num_bc_sats num_bc_tl num_il_tl num_op_rev_tl num_rev_accts num_rev_tl_bal_gt_0 num_sats     0 1 4 1 2 8 5 9 4 6   1 6 9 7 18 2 14 32 9 17   2 1 4 1 4 12 4 8 4 11   3 3 5 3 5 1 5 7 5 8   4 4 7 5 16 17 8 26 7 12      \n \n  All of these variables are related to some kind of number of accounts. Lets take a look at their inter-dependence.\n   In\u0026nbsp;[50]: sns.pairplot(df, vars=[\u0026quot;num_actv_bc_tl\u0026quot;, \u0026quot;num_actv_rev_tl\u0026quot;, \u0026quot;num_bc_sats\u0026quot;, \u0026quot;num_bc_tl\u0026quot;, \u0026quot;num_op_rev_tl\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[50]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4de95208\u0026gt;  \n  \n \n In\u0026nbsp;[51]: sns.pairplot(df, vars=[\u0026quot;num_op_rev_tl\u0026quot;, \u0026quot;num_il_tl\u0026quot;, \u0026quot;num_rev_accts\u0026quot;, \u0026quot;num_rev_tl_bal_gt_0\u0026quot;, \u0026quot;num_sats\u0026quot;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[51]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4caaedd8\u0026gt;  \n  \n \n  Lets also look at the remaining \u0026ldquo;num\u0026rdquo; of account variables.\n   In\u0026nbsp;[52]: sns.pairplot(df, vars=[\u0026quot;num_sats\u0026quot;, \u0026#39;num_tl_30dpd\u0026#39;, \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[52]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4bf1b2b0\u0026gt;  \n  \n \n  We see that first 9 sets of variables are all quite strongly correlated. The best set to consider for our model could be a sum of a subset of these. Lets consider a new variable, i.e. sum of all types of opened loan accounts, consisting of num_actv_bc_tl, num_actv_rev_tl, num_rev_tl_bal_gt_0, and num_tl_90g_dpd_24m. Additionally, we should also keep no. of accounts open in last year as a variable, i.e. num_tl_op_past_12m.\n   In\u0026nbsp;[53]: df[\u0026#39;num_accounts\u0026#39;] = df[\u0026#39;num_actv_bc_tl\u0026#39;] + df[\u0026#39;num_actv_rev_tl\u0026#39;] + df[\u0026#39;num_rev_tl_bal_gt_0\u0026#39;] + df[\u0026#39;num_tl_90g_dpd_24m\u0026#39;] \n  \n In\u0026nbsp;[54]: dropped_vars=[\u0026quot;num_actv_bc_tl\u0026quot;, \u0026quot;num_actv_rev_tl\u0026quot;, \u0026quot;num_bc_sats\u0026quot;, \u0026quot;num_bc_tl\u0026quot;, \u0026quot;num_op_rev_tl\u0026quot;, \u0026quot;num_il_tl\u0026quot;, \u0026quot;num_rev_accts\u0026quot;, \u0026quot;num_rev_tl_bal_gt_0\u0026quot;, \u0026quot;num_sats\u0026quot;, \u0026#39;num_tl_30dpd\u0026#39;, \u0026#39;num_tl_90g_dpd_24m\u0026#39;, \u0026#39;num_tl_120dpd_2m\u0026#39;] df.drop(dropped_vars,1, inplace=True) \n  \n In\u0026nbsp;[55]: df.ix[:4,24:32] \n  \nOut[55]:    percent_bc_gt_75 pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort total_bc_limit total_il_high_credit_limit credit_age     0 0.0 0 0 196500 149140 10000 12000 264   1 14.3 0 0 179407 15030 13000 11325 324   2 100.0 0 0 82331 64426 4900 64031 167   3 100.0 0 0 368700 18007 4400 18000 189   4 60.0 0 0 52490 38566 21100 24890 286      \n \n  Out of these 8 remaining features, lets first focus on first two percentages. We will first examining their role of loan status. percent_bc_gt_75 has very high amount of missing data and hence we will remove it from consideration.\n   In\u0026nbsp;[56]: sns.pairplot(df, vars=[\u0026quot;pct_tl_nvr_dlq\u0026quot;, \u0026#39;int_rate\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[56]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4b63e860\u0026gt;  \n  \n \n  We find even pct_tl_nvr_dlq to be not of much help!\n   In\u0026nbsp;[57]: df.drop([\u0026#39;pct_tl_nvr_dlq\u0026#39;, \u0026#39;percent_bc_gt_75\u0026#39;],1, inplace=True) \n  \n In\u0026nbsp;[58]: sns.pairplot(df, vars=[\u0026quot;pub_rec_bankruptcies\u0026quot;, \u0026#39;tax_liens\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) \n  \nOut[58]: \u0026lt;seaborn.axisgrid.PairGrid at 0x7f2e4b357e10\u0026gt;  \n  \n \n  This clearly shows a strong relationship between loan status and these features and hence need to be an integral part of the model.\nFinally let us take a look at the remaining total credit/balance related features.\n   In\u0026nbsp;[59]: g = sns.pairplot(df, vars=[\u0026quot;tot_hi_cred_lim\u0026quot;, \u0026#39;total_bal_ex_mort\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;total_il_high_credit_limit\u0026#39;], hue=\u0026quot;loan_status\u0026quot;) for ax in g.axes.flat:\nplt.setp(ax.get_xticklabels(), rotation=90) \n  \n  \n \n  We can see a clear effect of outliers on low status of loan on all these variables. However, we can also see a strong correlation between total_il_high_credit_limit, tot_hi_cred_lim and total_bal_ex_mort. Let us choose only tot_hi_cred_lim to be in out feature set.\n   In\u0026nbsp;[60]: df.drop([\u0026#39;total_il_high_credit_limit\u0026#39;, \u0026#39;total_bal_ex_mort\u0026#39;],1, inplace=True) \n  \n  Let us take a final look at all the remaining variables in our data.\n   In\u0026nbsp;[61]: print(df.columns) print(df.columns.shape) \n  \n Index([\u0026#39;loan_amnt\u0026#39;, \u0026#39;funded_amnt_inv\u0026#39;, \u0026#39;term\u0026#39;, \u0026#39;int_rate\u0026#39;, \u0026#39;grade\u0026#39;, \u0026#39;sub_grade\u0026#39;, \u0026#39;emp_length\u0026#39;, \u0026#39;home_ownership\u0026#39;, \u0026#39;annual_inc\u0026#39;, \u0026#39;verification_status\u0026#39;, \u0026#39;loan_status\u0026#39;, \u0026#39;purpose\u0026#39;, \u0026#39;zip_code\u0026#39;, \u0026#39;dti\u0026#39;, \u0026#39;delinq_2yrs\u0026#39;, \u0026#39;revol_bal\u0026#39;, \u0026#39;revol_util\u0026#39;, \u0026#39;total_acc\u0026#39;, \u0026#39;last_credit_pull_d\u0026#39;, \u0026#39;avg_cur_bal\u0026#39;, \u0026#39;bc_util\u0026#39;, \u0026#39;mort_acc\u0026#39;, \u0026#39;num_tl_op_past_12m\u0026#39;, \u0026#39;pub_rec_bankruptcies\u0026#39;, \u0026#39;tax_liens\u0026#39;, \u0026#39;tot_hi_cred_lim\u0026#39;, \u0026#39;total_bc_limit\u0026#39;, \u0026#39;credit_age\u0026#39;, \u0026#39;fico\u0026#39;, \u0026#39;num_accounts\u0026#39;], dtype=\u0026#39;object\u0026#39;) (30,)     \n  Let us stop here for this post. We will continue our model creation. We will save our pandas object as pickle and then catch up from there.\n   In\u0026nbsp;[62]: df.to_pickle(\u0026quot;/home/ssingh/LendingClubData/Part2.pickle\u0026quot;) \n  \n\n","title":"EDA of Lending Club Data - II","url":"https://sadanand-singh.github.io/posts/lceda2/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"We will first look at various aspects of the LendingClub data using techniques of Exploratory Data Analysis (EDA). Please look at my past post for finding further details on EDA techniques. Different data files for this analysis have already been downloaded in the current folder. In\u0026nbsp;[4]: import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) import pandas as pd import numpy as np from matplotlib import pyplot as plt %matplotlib inline    \n  Let\u0026rsquo;s also take a quick look at the data via shell scripts (file size, head, line count, column count).\n   In\u0026nbsp;[2]: !du -h /home/ssingh/LendingClubData/LoanStats3c_securev1.csv #!tail -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv #!head -3 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv \n  \n 181M /home/ssingh/LendingClubData/LoanStats3c_securev1.csv     \n  Examining the data we see that most of feature names are intuitive. We can get the specifics from the provided data dictionary.\n   In\u0026nbsp;[3]: !wc -l \u0026lt; /home/ssingh/LendingClubData/LoanStats3c_securev1.csv !head -2 /home/ssingh/LendingClubData/LoanStats3c_securev1.csv | sed \u0026#39;s/[^,]//g\u0026#39; | wc -c \n  \n 235633 116     \n  Based on the above analysis, we find that we have a total of 235633-2-2 = 235659 rows and 116 - 1 = 115 columns of data! Let us first look at the detailed description of columns from the dictionary of the data.\n   In\u0026nbsp;[5]: df = pd.read_csv(\u0026quot;/home/ssingh/LendingClubData/LoanStatsDict.csv\u0026quot;, sep=\u0026quot;,\u0026quot;, engine=\u0026#39;c\u0026#39;, encoding = \u0026quot;ISO-8859-1\u0026quot;, na_filter=False) df = df.ix[1:,0:2] from IPython.display import HTML HTML(df.to_html()) \n  \nOut[5]: --   LoanStatNew Description     1 acc_open_past_24mths Number of trades opened in past 24 months.   2 addr_state The state provided by the borrower in the loan\u0026hellip;   3 all_util Balance to credit limit on all trades   4 annual_inc The self-reported annual income provided by th\u0026hellip;   5 annual_inc_joint The combined self-reported annual income provi\u0026hellip;   6 application_type Indicates whether the loan is an individual ap\u0026hellip;   7 avg_cur_bal Average current balance of all accounts   8 bc_open_to_buy Total open to buy on revolving bankcards.   9 bc_util Ratio of total current balance to high credit/\u0026hellip;   10 chargeoff_within_12_mths Number of charge-offs within 12 months   11 collection_recovery_fee post charge off collection fee   12 collections_12_mths_ex_med Number of collections in 12 months excluding m\u0026hellip;   13 delinq_2yrs The number of 30+ days past-due incidences of \u0026hellip;   14 delinq_amnt The past-due amount owed for the accounts on w\u0026hellip;   15 desc Loan description provided by the borrower   16 dti A ratio calculated using the borrowerÕs total \u0026hellip;   17 dti_joint A ratio calculated using the co-borrowers\u0026rsquo; tot\u0026hellip;   18 earliest_cr_line The month the borrower\u0026rsquo;s earliest reported cre\u0026hellip;   19 emp_length Employment length in years. Possible values ar\u0026hellip;   20 emp_title The job title supplied by the Borrower when ap\u0026hellip;   21 fico_range_high The upper boundary range the borrowerÕs FICO a\u0026hellip;   22 fico_range_low The lower boundary range the borrowerÕs FICO a\u0026hellip;   23 funded_amnt The total amount committed to that loan at tha\u0026hellip;   24 funded_amnt_inv The total amount committed by investors for th\u0026hellip;   25 grade LC assigned loan grade   26 home_ownership The home ownership status provided by the borr\u0026hellip;   27 id A unique LC assigned ID for the loan listing.   28 il_util Ratio of total current balance to high credit/\u0026hellip;   29 initial_list_status The initial listing status of the loan. Possib\u0026hellip;   30 inq_fi Number of personal finance inquiries   31 inq_last_12m Number of credit inquiries in past 12 months   32 inq_last_6mths The number of inquiries in past 6 months (excl\u0026hellip;   33 installment The monthly payment owed by the borrower if th\u0026hellip;   34 int_rate Interest Rate on the loan   35 issue_d The month which the loan was funded   36 last_credit_pull_d The most recent month LC pulled credit for thi\u0026hellip;   37 last_fico_range_high The upper boundary range the borrowerÕs last F\u0026hellip;   38 last_fico_range_low The lower boundary range the borrowerÕs last F\u0026hellip;   39 last_pymnt_amnt Last total payment amount received   40 last_pymnt_d Last month payment was received   41 loan_amnt The listed amount of the loan applied for by t\u0026hellip;   42 loan_status Current status of the loan   43 max_bal_bc Maximum current balance owed on all revolving \u0026hellip;   44 member_id A unique LC assigned Id for the borrower member.   45 mo_sin_old_il_acct Months since oldest bank installment account o\u0026hellip;   46 mo_sin_old_rev_tl_op Months since oldest revolving account opened   47 mo_sin_rcnt_rev_tl_op Months since most recent revolving account opened   48 mo_sin_rcnt_tl Months since most recent account opened   49 mort_acc Number of mortgage accounts.   50 mths_since_last_delinq The number of months since the borrower\u0026rsquo;s last\u0026hellip;   51 mths_since_last_major_derog Months since most recent 90-day or worse rating   52 mths_since_last_record The number of months since the last public rec\u0026hellip;   53 mths_since_rcnt_il Months since most recent installment accounts \u0026hellip;   54 mths_since_recent_bc Months since most recent bankcard account opened.   55 mths_since_recent_bc_dlq Months since most recent bankcard delinquency   56 mths_since_recent_inq Months since most recent inquiry.   57 mths_since_recent_revol_delinq Months since most recent revolving delinquency.   58 next_pymnt_d Next scheduled payment date   59 num_accts_ever_120_pd Number of accounts ever 120 or more days past due   60 num_actv_bc_tl Number of currently active bankcard accounts   61 num_actv_rev_tl Number of currently active revolving trades   62 num_bc_sats Number of satisfactory bankcard accounts   63 num_bc_tl Number of bankcard accounts   64 num_il_tl Number of installment accounts   65 num_op_rev_tl Number of open revolving accounts   66 num_rev_accts Number of revolving accounts   67 num_rev_tl_bal_gt_0 Number of revolving trades with balance \u0026gt;0   68 num_sats Number of satisfactory accounts   69 num_tl_120dpd_2m Number of accounts currently 120 days past due\u0026hellip;   70 num_tl_30dpd Number of accounts currently 30 days past due \u0026hellip;   71 num_tl_90g_dpd_24m Number of accounts 90 or more days past due in\u0026hellip;   72 num_tl_op_past_12m Number of accounts opened in past 12 months   73 open_acc The number of open credit lines in the borrowe\u0026hellip;   74 open_acc_6m Number of open trades in last 6 months   75 open_il_12m Number of installment accounts opened in past \u0026hellip;   76 open_il_24m Number of installment accounts opened in past \u0026hellip;   77 open_il_6m Number of currently active installment trades   78 open_rv_12m Number of revolving trades opened in past 12 m\u0026hellip;   79 open_rv_24m Number of revolving trades opened in past 24 m\u0026hellip;   80 out_prncp Remaining outstanding principal for total amou\u0026hellip;   81 out_prncp_inv Remaining outstanding principal for portion of\u0026hellip;   82 pct_tl_nvr_dlq Percent of trades never delinquent   83 percent_bc_gt_75 Percentage of all bankcard accounts \u0026gt; 75% of l\u0026hellip;   84 policy_code publicly available policy_code=1 new products \u0026hellip;   85 pub_rec Number of derogatory public records   86 pub_rec_bankruptcies Number of public record bankruptcies   87 purpose A category provided by the borrower for the lo\u0026hellip;   88 pymnt_plan Indicates if a payment plan has been put in pl\u0026hellip;   89 recoveries post charge off gross recovery   90 revol_bal Total credit revolving balance   91 revol_util Revolving line utilization rate, or the amount\u0026hellip;   92 sub_grade LC assigned loan subgrade   93 tax_liens Number of tax liens   94 term The number of payments on the loan. Values are\u0026hellip;   95 title The loan title provided by the borrower   96 tot_coll_amt Total collection amounts ever owed   97 tot_cur_bal Total current balance of all accounts   98 tot_hi_cred_lim Total high credit/credit limit   99 total_acc The total number of credit lines currently in \u0026hellip;   100 total_bal_ex_mort Total credit balance excluding mortgage   101 total_bal_il Total current balance of all installment accounts   102 total_bc_limit Total bankcard high credit/credit limit   103 total_cu_tl Number of finance trades   104 total_il_high_credit_limit Total installment high credit/credit limit   105 total_pymnt Payments received to date for total amount funded   106 total_pymnt_inv Payments received to date for portion of total\u0026hellip;   107 total_rec_int Interest received to date   108 total_rec_late_fee Late fees received to date   109 total_rec_prncp Principal received to date   110 total_rev_hi_lim Ê Total revolving high credit/credit limit   111 url URL for the LC page with listing data.   112 verification_status Indicates if income was verified by LC, not ve\u0026hellip;   113 verified_status_joint Indicates if the co-borrowers\u0026rsquo; joint income wa\u0026hellip;   114 zip_code The first 3 numbers of the zip code provided b\u0026hellip;    \n\n \n  Lets choose some of the most important variables from these.\nThe Response Variable:\n Interest Rate (int_rate)  And some of possible important factors are:\n Annual Income (annual_inc) State (addr_state) Purpose (purpose) Description for Loan (desc) Amount Requested (loan_amount) Amount Funded (funded_amnt) Loan Length (term) Debt Income Ratio (dti) Home Ownership status (home_ownership) FICO high (fico_range_high) FICO low (fico_range_low) Last FICO low (last_fico_range_low) Last FICO high (last_fico_range_high) Average current balance (avg_cur_bal) Charge Offs in last Year (chargeoff_within_12_mths) Number of 30+ days past-due incidences (delinq_2yrs) Employment Length (emp_length) No. of Credit Inquiries (inq_last_6mths) Maximum current balance owed on all revolving (max_bal_bc) Total credit revolving balance (revol_bal) LC Verification status (verification_status) Revolving line utilization rate (revol_util) Percentage of account never delinquent (pct_tl_nvr_dlq) Months since most recent 90-day or worse rating (mths_since_last_major_derog) Total Credit Balance (total_bal_ex_mort)  We will first look at effects of some of these variables using EDA.\nLater, if we find any need to use some additional variables, we will revisit this list.\nFirst, lets load our data as a Pandas data frame:\n   In\u0026nbsp;[5]: df = pd.read_csv(\u0026quot;/home/ssingh/LendingClubData/LoanStats3c_securev1.csv\u0026quot;, skiprows=1, skipfooter=2) df.info(verbose = False) \n  \n \u0026lt;class \u0026#39;pandas.core.frame.DataFrame\u0026#39;\u0026gt; RangeIndex: 235629 entries, 0 to 235628 Columns: 115 entries, id to total_il_high_credit_limit dtypes: float64(44), int64(47), object(24) memory usage: 206.7+ MB     \n In\u0026nbsp;[6]: df.head(3) \n  \nOut[6]:    id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate installment grade sub_grade ... num_tl_90g_dpd_24m num_tl_op_past_12m pct_tl_nvr_dlq percent_bc_gt_75 pub_rec_bankruptcies tax_liens tot_hi_cred_lim total_bal_ex_mort total_bc_limit total_il_high_credit_limit     0 38098114 40860827 15000 15000 15000 60 months 12.39% 336.64 C C1 ... 0 4 100.0 0.0 0 0 196500 149140 10000 12000   1 36805548 39558264 10400 10400 10400 36 months 6.99% 321.08 A A3 ... 0 4 83.3 14.3 0 0 179407 15030 13000 11325   2 37662224 40425321 7650 7650 7650 36 months 13.66% 260.20 C C3 ... 0 2 100.0 100.0 0 0 82331 64426 4900 64031    3 rows × 115 columns\n  \n \n  Lets us remove columns/data that is redundant for prediction of interest rates. For example, id, member_id for sure are of no importance to us. Let\u0026rsquo;s work through the columns in batches to keep the cognitive burden low:\n   In\u0026nbsp;[7]: # .ix[row slice, column slice]  df.ix[:4,:7] \n  \nOut[7]:    id member_id loan_amnt funded_amnt funded_amnt_inv term int_rate     0 38098114 40860827 15000 15000 15000 60 months 12.39%   1 36805548 39558264 10400 10400 10400 36 months 6.99%   2 37662224 40425321 7650 7650 7650 36 months 13.66%   3 37612354 40375473 12800 12800 12800 60 months 17.14%   4 37822187 40585251 9600 9600 9600 36 months 13.66%      \n \n  We won\u0026rsquo;t need id or member_id as it has no real predictive power so we can drop them from this table int_rate was loaded as an object data type instead of float due to the \u0026lsquo;%\u0026rsquo; character. Let\u0026rsquo;s strip that out and convert the column type. And Also do similar transformation for term variable to get rid of months.\n   In\u0026nbsp;[8]: df.drop([\u0026#39;id\u0026#39;,\u0026#39;member_id\u0026#39;], 1, inplace=True) df.int_rate = pd.Series(df.int_rate).str.replace(\u0026#39;%\u0026#39;, \u0026#39;\u0026#39;).astype(float) df[\u0026#39;term\u0026#39;].replace(to_replace=\u0026#39;[^0-9]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) df[\u0026#39;term\u0026#39;] = df[\u0026#39;term\u0026#39;].convert_objects(convert_numeric=True) \n  \n  Moving on to next columns:\n   In\u0026nbsp;[9]: df.ix[:4,5:15] \n  \nOut[9]:    installment grade sub_grade emp_title emp_length home_ownership annual_inc verification_status issue_d loan_status     0 336.64 C C1 MANAGEMENT 10+ years RENT 78000.0 Source Verified Dec-2014 Current   1 321.08 A A3 Truck Driver Delivery Personel 8 years MORTGAGE 58000.0 Not Verified Dec-2014 Current   2 260.20 C C3 Technical Specialist \u0026lt; 1 year RENT 50000.0 Source Verified Dec-2014 Charged Off   3 319.08 D D4 Senior Sales Professional 10+ years MORTGAGE 125000.0 Verified Dec-2014 Current   4 326.53 C C3 Admin Specialist 10+ years RENT 69000.0 Source Verified Dec-2014 Fully Paid      \n \n  At first, it seems we employment title should be important. Let us first have a look at how many unique values we have for these. We would like to convert emp_length into an integer variable.\n   In\u0026nbsp;[10]: print(df.emp_title.value_counts().head()) print(df.emp_title.value_counts().tail()) df.emp_title.unique().shape \n  \n Teacher 4569 Manager 3772 Registered Nurse 1960 RN 1816 Supervisor 1663 Name: emp_title, dtype: int64 Care Aid 1 Deputy Probation 1 Front office staff 1 factor 1 Independent Contractor/Driver 1 Name: emp_title, dtype: int64    Out[10]: (75353,)  \n \n  This is just too many. Unless, we do some semantics based grouping of these titles, we would not be able to get any meaningful data out of this. If you think harder, this should be highly correlated with income. Just for the purpose of loan, it is highly unlikely (not impossible though!) that a high paying job title would be different than another!\n   In\u0026nbsp;[11]: df.drop([\u0026#39;emp_title\u0026#39;], 1, inplace=True) \n  \n  Let first look at unique value of emp_legth variable.\n   In\u0026nbsp;[12]: df.emp_length.value_counts() \n  \nOut[12]: 10+ years 79505 2 years 20487 3 years 18267 \u0026lt; 1 year 17982 1 year 14593 4 years 13528 7 years 13099 5 years 13051 n/a 12019 8 years 11853 6 years 11821 9 years 9424 Name: emp_length, dtype: int64  \n \n  Let us make this variable simple integers. We will replace na entries with 0, and all non numeric entries.\n   In\u0026nbsp;[13]: df.replace(\u0026#39;n/a\u0026#39;, np.nan, inplace=True) df.emp_length.fillna(value=0,inplace=True) df[\u0026#39;emp_length\u0026#39;].replace(to_replace=\u0026#39;^\u0026lt;\u0026#39;, value=0.0, inplace=True, regex=True) df[\u0026#39;emp_length\u0026#39;].replace(to_replace=\u0026#39;[^0-9]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) df[\u0026#39;emp_length\u0026#39;] = df[\u0026#39;emp_length\u0026#39;].astype(int) \n  \n  Lets us look at unique emp_length entries again:\n   In\u0026nbsp;[14]: df.emp_length.value_counts() \n  \nOut[14]: 10 79505 0 30001 2 20487 3 18267 1 14593 4 13528 7 13099 5 13051 8 11853 6 11821 9 9424 Name: emp_length, dtype: int64  \n \n  We should convert verification status into three ordinal values. Lets see what are different possible values of the verification status. We will convert these to ordinals accordingly.\n   In\u0026nbsp;[15]: df.verification_status.value_counts() \n  \nOut[15]: Source Verified 97741 Not Verified 70659 Verified 67229 Name: verification_status, dtype: int64  \n \n  We will assign the lowest rating to Not Verified and the highest rating to Verified.\n   In\u0026nbsp;[16]: df[\u0026quot;verification_status\u0026quot;] = df[\u0026quot;verification_status\u0026quot;].astype(\u0026#39;category\u0026#39;) df[\u0026quot;verification_status\u0026quot;] = df[\u0026quot;verification_status\u0026quot;].cat.set_categories([\u0026quot;Not Verified\u0026quot;, \u0026quot;Source Verified\u0026quot;, \u0026quot;Verified\u0026quot;], ordered = True) \n  \n  Loan status and issue dates are not of any interest to us, as we only plan to use this data for making models that can predict interest rate. However, let us do some digging into this data to see some statistics of loans on Lending Club! Let us look at a histogram of different states of loans.\n   In\u0026nbsp;[23]: import seaborn as sns sns.set() sns.set_context(\u0026quot;notebook\u0026quot;, font_scale=1.5, rc={\u0026quot;lines.linewidth\u0026quot;: 2.5}) total = float(len(df.index)) ax = sns.countplot(x=\u0026quot;loan_status\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); for p in ax.patches: height = p.get_height() ax.text(p.get_x(), height+18, \u0026#39;%2.2f\u0026#39;%(height*100/total)+\u0026quot;%\u0026quot;) plt.xticks(rotation=60) plt.show() \n  \n  \n \n  We can also look at a histogram of number of loans issued based on the month of the year. Just for fun, we will also look for any correlation between the issue month of loans and their states.\n   In\u0026nbsp;[18]: df[\u0026#39;issue_d\u0026#39;].replace(to_replace=\u0026#39;[^A-Z,a-z]+\u0026#39;, value=\u0026#39;\u0026#39;, inplace=True, regex=True) ax = sns.countplot(x=\u0026quot;issue_d\u0026quot;, data=df, palette=\u0026quot;Set2\u0026quot;); for p in ax.patches: height = p.get_height() ax.text(p.get_x(), height+18, \u0026#39;%2.1f\u0026#39;%(height*100/total)) plt.xticks(rotation=60) plt.show() \n  \n  \n \n  First, we find that more loans are issued during the holidays seasons: Oct and July! Nothing surprising there.\n   In\u0026nbsp;[19]: g = sns.factorplot(\u0026quot;loan_status\u0026quot;, col=\u0026quot;issue_d\u0026quot;, col_wrap=4, data=df,kind=\u0026quot;count\u0026quot;, aspect=1.25) (g.set_axis_labels(\u0026quot;\u0026quot;, \u0026quot;\u0026quot;) .set_titles(\u0026quot;{col_name}\u0026quot;) .set_xticklabels(rotation=60) .despine(left=True)) \n  \nOut[19]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fe6351a72b0\u0026gt;  \n  \n \n  We do not find any obvious correlation between the issue date and their states at this time. Not interesting. Lets drop both issue date from our further analysis. Moving on to next set of columns.\nWe will also divide loan status into two categories: Low, and High risks.\n   In\u0026nbsp;[20]: defaultList = [\u0026quot;Default\u0026quot;, \u0026quot;Charged Off\u0026quot;, \u0026quot;Late (31-120 days)\u0026quot;, \u0026quot;Late (16-30 days)\u0026quot;, \u0026quot;In Grace Period\u0026quot;] df.loc[df.loan_status.isin(defaultList), \u0026quot;loan_status\u0026quot;] = \u0026quot;High\u0026quot; goodList = [\u0026quot;Current\u0026quot;, \u0026quot;Fully Paid\u0026quot;] df.loc[df.loan_status.isin(goodList), \u0026quot;loan_status\u0026quot;] = \u0026quot;Low\u0026quot; \n  \n In\u0026nbsp;[21]: df.drop([\u0026#39;issue_d\u0026#39;],1, inplace=True) \n  \n  Let us stop here for this post. We will continue our EDA analysis in the next. We will save our pandas object as pickle and then catch up from there.\n   In\u0026nbsp;[22]: df.to_pickle(\u0026quot;/home/ssingh/LendingClubData/Part1.pickle\u0026quot;) \n  \n\n","title":"EDA of Lending Club Data","url":"https://sadanand-singh.github.io/posts/lceda1/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this section, we will continue re-using the data from the previous post based on Pseudo Facebook data from udacity.\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited tsv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[1]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning)    Out[1]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  Continuing with our analysis from the last post, finding a relationship between age and friends counts, let us add gender to the equation.\nIn order to do this, we will first use groupby() and agg() to get group by data.\n   In\u0026nbsp;[2]: def groupByStats(df, groupCol, statsCol): \u0026#39;\u0026#39;\u0026#39; return a dataframe with groupByCo groupbyCol: a string or a list of strings for col names in df statsCol: a string for col in df of which we need stats for \u0026#39;\u0026#39;\u0026#39;\n # Define the aggregation calculations aggregations = { statsCol: { (statsCol+\u0026#39;_mean\u0026#39;): \u0026#39;mean\u0026#39;, (statsCol+\u0026#39;_median\u0026#39;): \u0026#39;median\u0026#39;, (statsCol+\u0026#39;_q25\u0026#39;): lambda x: np.percentile(x,25), (statsCol+\u0026#39;_q75\u0026#39;): lambda x: np.percentile(x,75), \u0026#39;n\u0026#39;: \u0026#39;count\u0026#39; } } df_group_by_groupCol = df.groupby(groupCol, as_index=False, group_keys=False).agg(aggregations) df_group_by_groupCol.columns = df_group_by_groupCol.columns.droplevel() if isinstance(groupCol, list): cols = groupCol + list(df_group_by_groupCol.columns)[len(groupCol):] else: cols = [groupCol] + list(df_group_by_groupCol.columns)[1:] df_group_by_groupCol.columns = cols return df_group_by_groupCol    \n In\u0026nbsp;[3]: pf_group_by_age_gender = groupByStats(pf[pf.gender.notnull()], [\u0026#39;gender\u0026#39;, \u0026#39;age\u0026#39;], \u0026#39;friend_count\u0026#39;) pf_group_by_age_gender.head().T \n  \nOut[3]:    gender age friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 female 13 316.00 193 259.160622 148.0 39.0   1 female 14 410.50 847 362.428571 224.0 79.0   2 female 15 592.50 1139 538.681299 276.0 104.0   3 female 16 581.25 1238 519.514540 258.5 102.0   4 female 17 586.75 1236 538.994337 245.5 81.0      \n \n  Now, we can use plots to compare friend counts across age and gender.\n   In\u0026nbsp;[4]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\npf_group_by_age_gender[\u0026#39;unit\u0026#39;] = \u0026quot;U\u0026quot; ax = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_median\u0026#39;, condition=\u0026#39;gender\u0026#39;, unit=\u0026#39;unit\u0026#39;, data=pf_group_by_age_gender) plt.ylabel(\u0026quot;Friend Count Median\u0026quot;) plt.xlim(10,113) \n  \nOut[4]: (10, 113)  \n  \n \n  It would be helpful to analyze these differences in relative terms. So lets answer a different question - how many times more friends does the average female users have than average male users.\nTo answer the above question we will need to transform our data. Right now, our data is in “long format” - a row of data different values of different variables. We will need to convert this to a “wide format” - where we will create columns called \u0026lsquo;male\u0026rsquo; and \u0026lsquo;female\u0026rsquo;, that will have median counts in them.\nThis can be done using the “pivot()” method of pandas.\n   In\u0026nbsp;[5]: pf_group_by_age_gender_wide = pf_group_by_age_gender.pivot(index=\u0026#39;age\u0026#39;, columns=\u0026#39;gender\u0026#39;, values=\u0026#39;friend_count_median\u0026#39;) pf_group_by_age_gender_wide.columns.name = \u0026#39;\u0026#39; pf_group_by_age_gender_wide.reset_index(level=0, inplace=True) pf_group_by_age_gender_wide.head() \n  \nOut[5]:    age female male     0 13 148.0 55.0   1 14 224.0 92.5   2 15 276.0 106.5   3 16 258.5 136.0   4 17 245.5 125.0      \n \n  Similarly, we can use the melt() function to convert a wide format data back to a long format data. Now lets plot ratio of female to male median friend counts.\n   In\u0026nbsp;[6]: pf_group_by_age_gender_wide[\u0026#39;female_male_ratio\u0026#39;] = pf_group_by_age_gender_wide.female/pf_group_by_age_gender_wide.male pf_group_by_age_gender_wide[\u0026#39;unit\u0026#39;] = \u0026quot;u\u0026quot; ax = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;female_male_ratio\u0026#39;, unit=\u0026#39;unit\u0026#39;, data=pf_group_by_age_gender_wide) plt.plot([13, 113], [1, 1], linewidth=2, color=\u0026#39;r\u0026#39;, linestyle=\u0026#39;\u0026ndash;\u0026#39;) plt.ylabel(\u0026quot;Female-Male Ratio\u0026quot;) \n  \nOut[6]: \u0026lt;matplotlib.text.Text at 0x7fe86804b4a8\u0026gt;  \n  \n \n  In this particular case of looking at multiple variables, it would make more sense to look at count of friends as function of tenure of Facebook as well. People with a longer tenure of Facebook account are likely to accumulate a larger number of friends.\nIn the following section we will look at friend count of females, males at different ages along with their Facebook tenure.\n   In\u0026nbsp;[7]: pf[\u0026#39;year_joined\u0026#39;] = np.floor(2014 - pf[\u0026#39;tenure\u0026#39;]/365) \n  \n In\u0026nbsp;[8]: pf.year_joined.value_counts(dropna=False) \n  \nOut[8]:  2013.0 43588 2012.0 33366 2011.0 9860 2010.0 5448 2009.0 4557 2008.0 1507 2007.0 581 2014.0 70 2006.0 15 2005.0 9 NaN 2 Name: year_joined, dtype: int64  \n \n  The tabular view of this shows the distribution. We will next use the cut() method to bin this variable.\n   In\u0026nbsp;[9]: pf[\u0026#39;year_joined\u0026#39;] = pd.cut(pf.year_joined, bins=[2004,2009,2011,2012,2014]) pf[\u0026#39;year_joined\u0026#39;] = pf[\u0026#39;year_joined\u0026#39;].astype(\u0026#39;category\u0026#39;) \n  \n In\u0026nbsp;[10]: pf[\u0026#39;year_joined\u0026#39;].value_counts(dropna=False) \n  \nOut[10]: (2012, 2014] 43658 (2011, 2012] 33366 (2009, 2011] 15308 (2004, 2009] 6669 NaN 2 Name: year_joined, dtype: int64  \n \n  Let us now plot all these variables together. In particular, we want to create a plot of friend counts vs age where a different line is shown for each bin of year joined.\n   In\u0026nbsp;[11]: pf_group_by_age = groupByStats(pf, [\u0026#39;age\u0026#39;, \u0026#39;year_joined\u0026#39;], \u0026#39;friend_count\u0026#39;) pf_group_by_age.head().T \n  \nOut[11]:    age year_joined friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 13 (2009, 2011] 691.50 11 469.818182 362.0 124.50   1 13 (2011, 2012] 406.75 48 352.333333 248.5 145.25   2 13 (2012, 2014] 167.00 425 135.668235 63.0 18.00   3 14 (2009, 2011] 1033.75 28 860.928571 517.0 279.25   4 14 (2011, 2012] 383.00 449 350.812918 224.0 109.00      \n \n In\u0026nbsp;[12]: pf_group_by_age[\u0026#39;unit\u0026#39;] = \u0026#39;U\u0026#39; fig, ax = plt.subplots(figsize=(8,6)) g = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_median\u0026#39;, unit=\u0026#39;unit\u0026#39;,\ncondition=\u0026#39;year_joined\u0026#39;, data=pf_group_by_age, ax=ax) \n  \n  \n \n  Our initial hypothesis seems to be correct here - People with larger Facebook tenure tend to have higher friend counts. Lets us plot the mean of these bins and also the grand means of data.\n   In\u0026nbsp;[13]: fig, ax = plt.subplots(figsize=(8,6)) g = sns.tsplot(time=\u0026#39;age\u0026#39;, value=\u0026#39;friend_count_mean\u0026#39;, unit=\u0026#39;unit\u0026#39;,\ncondition=\u0026#39;year_joined\u0026#39;, data=pf_group_by_age, ax=ax) g = pf_group_by_age.groupby(\u0026#39;age\u0026#39;, as_index=False).mean() g = g.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;, ax=ax) \n  \n  \n \n  Since the trend holds up after conditioning on the each of the buckets of year joined, we can increase our confidence that this observation isn’t just an artifact. Let us look at this from a different angle.\nWe can define a variable called “friend rate”, i.e. number of friends each person had on a per day basis.\n   In\u0026nbsp;[14]: df = pf.loc[pf[\u0026#39;tenure\u0026#39;] \u0026gt;= 1] (df[\u0026#39;friend_count\u0026#39;]/df[\u0026#39;tenure\u0026#39;]).describe() \n  \nOut[14]: count 98931.000000 mean 0.609609 std 2.557356 min 0.000000 25% 0.077486 50% 0.220486 75% 0.565802 max 417.000000 dtype: float64  \n \n  We can now look at the effect of this variable in more detail using the following plot:\n   In\u0026nbsp;[15]: df.is_copy = False s = df[\u0026#39;friendships_initiated\u0026#39;]/df[\u0026#39;tenure\u0026#39;] df[\u0026#39;friendships_initiated_per_tenure\u0026#39;] = s.astype(\u0026#39;int64\u0026#39;)\ng = sns.lmplot(x=\u0026quot;tenure\u0026quot;, y=\u0026quot;friendships_initiated_per_tenure\u0026quot;, hue=\u0026quot;year_joined\u0026quot;, legend_out=False, data=df, size=5, fit_reg=False, scatter_kws={\u0026#39;alpha\u0026#39;: 0.5}) plt.xlim(1,3400) \n  \nOut[15]: (1, 3400)  \n  \n \n  This shows that people initiate less number of friends as they have longer tenure at Facebook.\nTill now, we have looked at different aspects of the Facebook data set. We will now use a new data set for some more detailed multivariate analysis, and then finally come back to the Facebook data set for comparison.\nWe are going to work with a data set describing household purchases of five flavors of Dannon Yogurt in 8 oz sizes. Their price is recorded with each purchase occasion. This yogurt data set has a quite different structure than our pseudo-Facebook data set. The synthetic Facebook data has one row per individual with that row giving their characteristics and counts of behaviors over a single period of time. On the other hand, the yogurt data has many rows per household, one for each purchase occasion. This kind of micro-data is often useful for answering different types of questions than we’ve looked at so far.\nWe will start by loading the yogurt data set and then looking at its summary.\n   In\u0026nbsp;[16]: yo = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/yogurt.csv\u0026quot;) #summarize data yo.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[16]:    count mean std min 50% max     obs 2380.0 1.367797e+03 790.076032 1.0 1369.50 2743.00   id 2380.0 2.128592e+06 17799.723643 2100081.0 2126532.00 2170639.00   time 2380.0 1.004967e+04 227.079811 9662.0 10045.00 10459.00   strawberry 2380.0 6.491597e-01 1.058612 0.0 0.00 11.00   blueberry 2380.0 3.571429e-01 0.819690 0.0 0.00 12.00   pina.colada 2380.0 3.584034e-01 0.803858 0.0 0.00 10.00   plain 2380.0 2.176471e-01 0.606556 0.0 0.00 6.00   mixed.berry 2380.0 3.886555e-01 0.904311 0.0 0.00 8.00   price 2380.0 5.925089e+01 10.913256 20.0 65.04 68.96      \n \n  We notice that most of the variables are integers here. We should convert the id variable to factor. This will come handy later since the same household data is available in multiple rows.\n   In\u0026nbsp;[17]: yo[\u0026#39;id\u0026#39;] = yo[\u0026#39;id\u0026#39;].astype(\u0026#39;category\u0026#39;) #summarize data yo.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[17]:    count unique top freq mean std min 50% max     obs 2380.0    1367.8 790.076 1 1369.5 2743   id 2380.0 332 2.13229e+06 74        time 2380.0    10049.7 227.08 9662 10045 10459   strawberry 2380.0    0.64916 1.05861 0 0 11   blueberry 2380.0    0.357143 0.81969 0 0 12   pina.colada 2380.0    0.358403 0.803858 0 0 10   plain 2380.0    0.217647 0.606556 0 0 6   mixed.berry 2380.0    0.388655 0.904311 0 0 8   price 2380.0    59.2509 10.9133 20 65.04 68.96      \n \n  Let us look at the histogram of yogurt prices first.\n   In\u0026nbsp;[18]: ax = sns.distplot(yo[\u0026quot;price\u0026quot;], kde=False, bins=36) plt.xlabel(\u0026#39;Price\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Count\u0026#39;, fontsize=12) \n  \nOut[18]: \u0026lt;matplotlib.text.Text at 0x7fe8665c31d0\u0026gt;  \n  \n \n  Now that we have some idea about distribution of prices, let’s figure out on a given purchase occasion how many eight ounce yogurts does a household purchase. To answer this we need to combine counts of the different yogurt flavors into one variable. Then we can look at the histogram.\n   In\u0026nbsp;[19]: yo[\u0026#39;all_purchase\u0026#39;] = yo[\u0026#39;strawberry\u0026#39;]+yo[\u0026#39;blueberry\u0026#39;]+yo[\u0026#39;plain\u0026#39;]+yo[\u0026#39;pina.colada\u0026#39;]+yo[\u0026#39;mixed.berry\u0026#39;] ax = sns.distplot(yo[\u0026#39;all_purchase\u0026#39;], kde=False, bins=36) plt.xlim(1,22) plt.xlabel(\u0026#39;All Purchases\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Count\u0026#39;, fontsize=12) \n  \nOut[19]: \u0026lt;matplotlib.text.Text at 0x7fe86657a198\u0026gt;  \n  \n \n  It seems most household purchase one 8 oz yogurt at any given purchase. Now we will look at the scatter plot of prime vs time data.\n   In\u0026nbsp;[20]: fig, ax = plt.subplots(figsize=(8,6)) sd = {\u0026#39;alpha\u0026#39;: 0.25, \u0026#39;edgecolors\u0026#39;: \u0026#39;black\u0026#39;} g = sns.regplot(x=\u0026#39;time\u0026#39;, y=\u0026#39;price\u0026#39;, data=yo, x_jitter=2, y_jitter=0.5, fit_reg=False, ax=ax, color=\u0026#39;red\u0026#39;, scatter_kws=sd) \n  \n  \n \n  We see the baseline price of yogurt has been increasing over time. We also see some scattered prices around the baseline price, which could be simply due to usage of coupons, sales etc. by customers.\nWhen familiarizing yourself with a new data set that contains multiple observations of the same units, it’s often useful to work with a sample of those units so that it’s easy to display the raw data for that sample. In the case of the yogurt data set, we might want to look at a small sample of households in more detail so that we know what kind of within and between household variation we are working with. This analysis of a sub-sample might come before trying to use within household variation as part of a model. For example, this data set was originally used to model consumer preferences for variety. But, before doing that, we’d want to look at how often we observe households buying yogurt, how often they buy multiple items, and what prices they’re buying yogurt at. One way to do this is to look at some sub-sample in more detail. Let’s pick 16 households at random and take a closer look.\n   In\u0026nbsp;[21]: sample_ids = yo.id.unique() sample_ids = pd.Series(sample_ids).sample(16, random_state=200) df = yo.ix[yo[\u0026#39;id\u0026#39;].isin(list(sample_ids)),[\u0026#39;id\u0026#39;, \u0026#39;price\u0026#39;, \u0026#39;time\u0026#39;, \u0026#39;all_purchase\u0026#39;]] df = df.reset_index(drop=True) df[\u0026#39;id\u0026#39;]=pd.Series(list(df.id)).astype(\u0026#39;category\u0026#39;) df.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \nOut[21]:    count unique top freq mean std min 50% max     id 171.0 16 2.13229e+06 74        price 171.0    59.8746 9.82084 33.04 65.04 68.96   time 171.0    10002.6 233.708 9664 9981 10457   all_purchase 171.0    1.84795 1.38489 1 1 10      \n \n In\u0026nbsp;[22]: g = sns.lmplot(x=\u0026#39;time\u0026#39;,y=\u0026#39;price\u0026#39;, hue=\u0026#39;all_purchase\u0026#39;, data=df, scatter_kws={\u0026quot;s\u0026quot;: 20*df[\u0026#39;all_purchase\u0026#39;]},\ncol=\u0026#39;id\u0026#39;, col_wrap=4, size=2, aspect=1, fit_reg=False, palette=\u0026quot;Set1\u0026quot;) g.set_xticklabels(rotation=90) \n  \nOut[22]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fe866497a90\u0026gt;  \n  \n \n  From these plots, we can see the variation and how often each household buys yogurt. And it seems that some household purchases more quantities than others with the larger circles. For most of the households, the price of yogurt holds steady, or tends to increase over time.\nNow, there are, of course, some exceptions, like in household 2124545 and in 2118927 household, we might think that the household is using coupons to drive the price down. Now, we don’t have the coupon data to associate with this buying data, but we can see how that information could be paired to this data to better understand the consumer behavior.\nThe general idea is that if we have observations over time, we can facet by the primary unit, case, or individual in the data set. For our yogurt data it was the households we were faceting over.\nThis faceted time series plot is something we can’t generate with our pseudo Facebook data set. Since we don’t have data on our sample of users over time.\nThe Facebook data isn’t great for examining the process of friending over time. The data set is just a cross section, it’s just one snapshot at a fixed point that tells us the characteristics of individuals. Not the individuals over, say, a year.\nBut if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure. This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases.\nMuch of the analysis we’ve done so far focused on some pre-chosen variable, relationship or question of interest. We then used EDA to let those chosen variables speak and surprise us. Most recently, when analyzing the relationship between two variables we look to incorporate more variables in the analysis to improve it. For example, by seeing whether a particular relationship is consistent across values of those other variables. In choosing a third or fourth variable to plot we relied on our domain knowledge. But often, we might want visualizations or summaries to help us identify such auxiliary variables. In some analyses, we may plan to make use of a large number of variables. Perhaps, we are planning on predicting one variable with ten, 20, or hundreds of others. Or maybe we want to summarize a large set of variables into a smaller set of dimensions. Or perhaps, we’re looking for interesting relationships among a large set of variables. In such cases, we can help speed up our exploratory data analysis by producing many plots or comparisons at once. This could be one way to let the data set as a whole speak in part by drawing our attention to variables we didn’t have a preexisting interest in.\nWe should let the data speak to determine variables of interest. There’s a tool that we can use to create a number of scatter plots automatically.\nIt’s called a scatter plot matrix. In a scatter plot matrix. There’s a grid of scatter plots between every pair of variables. As we’ve seen, scatter plots are great, but not necessarily suited for all types of variables. For example, categorical ones. So there are other types of visualizations that can be created instead of scatter plots. Like box plots or histograms when the variables are categorical.\nLet’s produce the scatter plot matrix for our pseudo Facebook data set. We’re going to use the pairplot() method to do so.\n   In\u0026nbsp;[23]: pf_subset = pf.iloc[:,1:4] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[24]: pf_subset = pf.iloc[:,4:8] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[25]: pf_subset = pf.iloc[:,8:11] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n In\u0026nbsp;[26]: pf_subset = pf.iloc[:,11:-1] pf_subset[\u0026#39;gender\u0026#39;] = pf.gender g = sns.pairplot(data=pf_subset.sample(1000), diag_kind=\u0026#39;kde\u0026#39;, hue=\u0026#39;gender\u0026#39;) g = g.map_diag(sns.distplot)\nfor ax in g.axes.flat: plt.setp(ax.get_xticklabels(), rotation=90) \n  \n /usr/lib/python3.5/site-packages/statsmodels/nonparametric/kdetools.py:20: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future y = X[:m/2+1] + np.r_[0,X[m/2+1:],0]*1j      \n \n  At the very least, a scatter plot matrix can be a useful starting point in many analyses.\nA matrix such as this one will be extremely helpful when we have even more variables than those in the pseudo-Facebook data set.\nExamples arise in many areas, but one that has attracted the attention of statisticians is genomic data. In these data sets, they’re often thousands of genetic measurements for each of a small number of samples. In some cases, some of these samples have a disease, and so we’d like to identify genes that are associated with the disease.\nWe will use an example data set of gene expression in tumors. The data contains the expression of 6,830 genes, compared with a larger baseline reference sample.\n   In\u0026nbsp;[27]: nci = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/nci.tsv\u0026quot;, sep = \u0026#39;\\s+\u0026#39;, header=None) nci.columns = list(range(1,65)) \n  \n In\u0026nbsp;[56]: fig, ax = plt.subplots(figsize=(8,6)) sns.heatmap(data=nci.loc[1:200,:], vmin=0, vmax=1.0, xticklabels=False, yticklabels=False, ax=ax, cmap=\u0026quot;YlGnBu\u0026quot;) plt.xlabel(\u0026quot;Case\u0026quot;, fontsize=14) _ = plt.ylabel(\u0026quot;Gene\u0026quot;, fontsize=14) \n  \n  \n \n  Heat maps can also be a good way to look at distributions of large dimensional data.\nIn summary in this post, we started with simple extensions to the scatter plot, and plots of conditional summaries that you worked with in lesson four, such as adding summaries for multiple groups. Then, we tried some techniques for examining a large number of variables at once, such as scatter-plot matrices and heat maps. We also learned how to reshape data, moving from broad data with one row per case, to aggregate data with one row per combination of variables, and we moved back and forth between long and wide formats for our data.\nIn the next and the final post in this series, We will do an in-depth analysis of the US department of Education dataset on college education, highlighting the role of EDA.\n  \n","title":"Exploring Multiple Variables","url":"https://sadanand-singh.github.io/posts/pyplotsmultivariables/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this section, we will be re-using the data from the previous post based on Pseudo Facebook data from udacity.\nThe data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a TAB delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[24]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /usr/lib/python3.5/site-packages/numpy/lib/function_base.py:3834: RuntimeWarning: Invalid value encountered in percentile RuntimeWarning)    Out[24]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  Usually, it is best to use a scatter plot to analyze two variables:\n   In\u0026nbsp;[47]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False) plt.xlim(13, 90) plt.ylim(0,5000) \n  \nOut[47]: (0, 5000)  \n  \n \n  We can notice some really interesting behavior in the ugly scatter plot above:\n The age data is binned, as expected (only integers allowed) Young people around the age of 20 have maximum friend count. There is unusual spike in friends count of people aged more than 100. This could mostly be some flaw in the data, probably based on incorrect entry by the users. People around the age of 70 too have quite a large amount of friends. This is pretty interesting and could point to the use of the social media site by an unexpected group of people.  We can use summary command to find the bounds on age and then use that to limit age axis.\n   In\u0026nbsp;[26]: pf.age.describe() \n  \nOut[26]: count 99003.000000 mean 37.280224 std 22.589748 min 13.000000 25% 20.000000 50% 28.000000 75% 50.000000 max 113.000000 Name: age, dtype: float64  \n \n  Furthermore, we notice at some areas of the plot being too dense, where as some to be really sparse. The areas where points are too dense is called “over plotting” - It is impossible to extract any meaningful statistics from this region. In order to overcome this, we can set the transparency of the plots using the alpha parameter in the plt.scatter() method. Using a value of 1\u0026frasl;20 means, one point that will plotted will be equal to 20 original points.\n   In\u0026nbsp;[27]: ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;green\u0026#39;, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}) plt.xlim(13, 90) plt.ylim(0,5000) plt.plot([13, 90], [600, 600], linewidth=2, color=\u0026#39;r\u0026#39;) \n  \nOut[27]: [\u0026lt;matplotlib.lines.Line2D at 0x7f07d45a22e8\u0026gt;]  \n  \n \n  Based on these new plots, we can find bulk of higher friend count for younger people is still less than 600. We still find higher count for age group of 70.\nFurthermore, we can do a better representation of data using the coord_trans() method. We will be using a square root function.\nIn order to that, we will first create an \u0026ldquo;sqrt\u0026rdquo; scale.\n   In\u0026nbsp;[28]: import numpy as np from numpy import ma from matplotlib import scale as mscale from matplotlib import transforms as mtransforms from matplotlib.ticker import AutoLocator\nclass SqrtScale(mscale.ScaleBase): \u0026quot;\u0026quot;\u0026quot; Scales data using np.sqrt method.\nThe scale function: np.sqrt(x)\nThe inverse scale function: x**2 \u0026quot;\u0026quot;\u0026quot;\n # The scale class must have a member ``name`` that defines the # string used to select the scale. For example, # ``gca().set_yscale(\u0026quot;mercator\u0026quot;)`` would be used to select this # scale. name = \u0026#39;sqrt\u0026#39; def __init__(self, axis): \u0026quot;\u0026quot;\u0026quot; Any keyword arguments passed to ``set_xscale`` and ``set_yscale`` will be passed along to the scale\u0026#39;s constructor. \u0026quot;\u0026quot;\u0026quot; mscale.ScaleBase.__init__(self) def get_transform(self): \u0026quot;\u0026quot;\u0026quot; Override this method to return a new instance that does the actual transformation of the data. The SqrtTransform class is defined below as a nested class of this one. \u0026quot;\u0026quot;\u0026quot; return self.SqrtTransform() def set_default_locators_and_formatters(self, axis): \u0026quot;\u0026quot;\u0026quot; Override to set up the locators and formatters to use with the scale. \u0026quot;\u0026quot;\u0026quot; axis.set_major_locator(AutoLocator()) class SqrtTransform(mtransforms.Transform): # There are two value members that must be defined. # ``input_dims`` and ``output_dims`` specify number of input # dimensions and output dimensions to the transformation. # These are used by the transformation framework to do some # error checking and prevent incompatible transformations from # being connected together. When defining transforms for a # scale, which are, by definition, separable and have only one # dimension, these members should always be set to 1. input_dims = 1 output_dims = 1 is_separable = True def __init__(self): mtransforms.Transform.__init__(self) def transform_non_affine(self, a): \u0026quot;\u0026quot;\u0026quot; This transform takes an Nx1 ``numpy`` array and returns a transformed copy. \u0026quot;\u0026quot;\u0026quot; return np.sqrt(a) def inverted(self): \u0026quot;\u0026quot;\u0026quot; Override this method so matplotlib knows how to get the inverse transform for this transform. \u0026quot;\u0026quot;\u0026quot; return SqrtScale.InvertedSqrtTransform()  class InvertedSqrtTransform(mtransforms.Transform): input_dims = 1 output_dims = 1 is_separable = True  def __init__(self): mtransforms.Transform.__init__(self)  def transform_non_affine(self, a): return a**2  def inverted(self): return SqrtScale.SqrtTransform() # Now that the Scale class has been defined, it must be registered so # that matplotlib can find it. mscale.register_scale(SqrtScale) \n  \n In\u0026nbsp;[29]: fig, ax = plt.subplots() fig.set_size_inches(8.6, 6.4) ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;cyan\u0026#39;, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}, ax=ax) plt.xlim(13, 90) plt.ylim(0,4599) plt.yscale(\u0026#39;sqrt\u0026#39;) plt.plot([13, 90], [600, 600], linewidth=2, color=\u0026#39;r\u0026#39;) \n  \nOut[29]: [\u0026lt;matplotlib.lines.Line2D at 0x7f07d24f5588\u0026gt;]  \n  \n \n  On a similar way, we can look at relationship between friends initiated and age.\n   In\u0026nbsp;[30]: fig, ax = plt.subplots() fig.set_size_inches(8.6, 6.4) kws = {\u0026#39;alpha\u0026#39;: 0.05} ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friendships_initiated\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;purple\u0026#39;, scatter_kws=kws, ax=ax) plt.xlim(13, 90) plt.ylim(0,4599) plt.yscale(\u0026#39;sqrt\u0026#39;) \n  \n  \n \n  Interestingly, we find this distribution to be very similar to the one for friend count.\nScatter plots try to keep us very close to the data. It represents each and every data point. However, in order to judge the quality of a data, it is important to know its important statistics like mean, median etc. How does average of a variable vary wrt to the some other variable.\nWe want to say, study how does average friend count vary with age. In order to study this we will use the grouping properties of pandas module.\nFirst we want to group our data frame by age. Then, we can create a new data frame that lists friend count mean, median and frequency (n) by using first the groupby() and then using the agg() method. We can look at first few data points of this new data frame using the head() method.\n   In\u0026nbsp;[121]: def groupByStats(pf, groupCol, statsCol): \u0026#39;\u0026#39;\u0026#39; return a dataframe with groupByCol\u0026#39;\u0026#39;\u0026#39;\n # Define the aggregation calculations aggregations = {  statsCol: { (statsCol+\u0026#39;_mean\u0026#39;): \u0026#39;mean\u0026#39;, (statsCol+\u0026#39;_median\u0026#39;): \u0026#39;median\u0026#39;, (statsCol+\u0026#39;_q25\u0026#39;): lambda x: np.percentile(x,25), (statsCol+\u0026#39;_q75\u0026#39;): lambda x: np.percentile(x,75), \u0026#39;n\u0026#39;: \u0026#39;count\u0026#39;/div }  } pf_group_by_age = pf.groupby(groupCol, as_index=False).agg(aggregations).rename(columns = {\u0026#39;\u0026#39;:groupCol}) pf_group_by_age.columns = pf_group_by_age.columns.droplevel() return pf_group_by_age  pf_group_by_age = groupByStats(pf, \u0026#39;age\u0026#39;, \u0026#39;friend_count\u0026#39;) pf_group_by_age.head(20)    \nOut[121]:    age friend_count_q75 n friend_count_mean friend_count_median friend_count_q25     0 13 230.00 484 164.750000 74.0 23.75   1 14 293.00 1925 251.390130 132.0 44.00   2 15 377.50 2618 347.692131 161.0 55.00   3 16 385.75 3086 351.937135 171.5 63.00   4 17 360.00 3283 350.300640 156.0 56.00   5 18 368.00 5196 331.166282 162.0 55.00   6 19 350.00 4391 333.692097 157.0 59.00   7 20 304.00 3769 283.499071 135.0 49.00   8 21 265.00 3671 235.941160 121.0 42.00   9 22 239.00 3032 211.394789 106.0 40.00   10 23 216.00 4404 202.842643 93.0 32.00   11 24 209.50 2827 185.712062 92.0 33.00   12 25 156.00 3641 131.021148 62.0 19.00   13 26 169.00 2815 144.008171 75.0 28.00   14 27 159.00 2240 134.147321 72.0 28.00   15 28 150.00 2364 125.835448 66.0 23.00   16 29 142.25 1936 120.818182 66.0 25.00   17 30 146.00 1716 115.208042 67.5 24.00   18 31 143.00 1694 118.459858 63.0 25.00   19 32 140.00 1443 114.279972 63.0 21.00      \n \n  Now, let us look at this new data frame visually. We can first look at the relationship between average friend count and age.\n   In\u0026nbsp;[97]: pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False) plt.ylabel(\u0026quot;Friend Count Mean\u0026quot;) \n  \nOut[97]: \u0026lt;matplotlib.text.Text at 0x7f07d16f9dd8\u0026gt;  \n  \n \n  We can use this plot as good summary of the original scatter plot and put the two on top of each other.\n   In\u0026nbsp;[109]: ax = sns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count\u0026#39;, data=pf, fit_reg=False, color=\u0026#39;cyan\u0026#39;, x_jitter=0.5, y_jitter=1.0, scatter_kws={\u0026#39;alpha\u0026#39;: 0.05}) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_q25\u0026#39;, ax=ax, color=\u0026#39;red\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_median\u0026#39;, ax=ax, color=\u0026#39;blue\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, ax=ax, color=\u0026#39;green\u0026#39;, style=\u0026#39;\u0026ndash;\u0026#39;) pf_group_by_age.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_q75\u0026#39;, ax=ax, color=\u0026#39;red\u0026#39;) plt.xlim(13, 70) plt.ylim(0,1000) plt.ylabel(\u0026quot;Friend Count\u0026quot;) \n  \nOut[109]: \u0026lt;matplotlib.text.Text at 0x7f07d091cbe0\u0026gt;  \n  \n \n  In the above plot, we can see that between the age group 30-69, 75% of population has less than 200 friends.\nIn stead of using 4 different summary measures to analyze the above data, we can use a single number!\nOften analysts will use correlation coefficients to summarize this. We will use the Pearson product moment correlation \u0026reg;. You can look at the pandas corr() method for details. This measures a linear correlation between two variables.\n   In\u0026nbsp;[112]: df = pf[(pf[\u0026#39;age\u0026#39;] \u0026lt; 70) \u0026amp; (pf[\u0026#39;age\u0026#39;] \u0026gt;= 13)] df[\u0026#39;age\u0026#39;].corr(df[\u0026#39;friend_count\u0026#39;], method=\u0026#39;pearson\u0026#39;) \n  \nOut[112]: -0.17121437273281295  \n \n  We can also have other measures of relationship. For example, a measure of monotonic relationship would be done using Spearman coefficient. Similarly, a measure of strength of dependence between two variables can be done using the “Kendall Rank Coefficient”. A more detailed description about these can be found at http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/.\nWe will now look at variables that are strictly correlated using scatter plots.\nOne such example in our dataset would be a relationship between likes_received (y) vs. www_likes_received (x).\n   In\u0026nbsp;[119]: ax = sns.regplot(x=\u0026#39;www_likes_received\u0026#39;, y=\u0026#39;likes_received\u0026#39;, data=pf, color=\u0026#39;cyan\u0026#39;, ci=None, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;}) plt.xlim(0, np.percentile(pf.www_likes_received, 95)) plt.ylim(0, np.percentile(pf.likes_received, 95)) \n  \nOut[119]: (0, 561.0)  \n  \n \n  We have used numpy percentile() method to find upper limits of x and y data. Additionally, we added a correlation line using the regplot().\nWe can find the numerical value of the correlation between these two variables.\n   In\u0026nbsp;[120]: pf[\u0026#39;www_likes_received\u0026#39;].corr(pf[\u0026#39;likes_received\u0026#39;], method=\u0026#39;pearson\u0026#39;) \n  \nOut[120]: 0.9479901803455516  \n \n  Correlation between two variables might not be a good thing always. For example in the above case, it was simply due to the artifact of the two data sets were highly coupled, one was a super set of the other.\nLet us take a look again at the modified data frame created using the groupby methods. In particular, we want to remove any noise in the average values.\n   In\u0026nbsp;[150]: pf[\u0026#39;age_with_months\u0026#39;] = pf.age + (12-pf.dob_month)/12 pf_group_by_age_with_months = groupByStats(pf, \u0026#39;age_with_months\u0026#39;, \u0026#39;friend_count\u0026#39;) pf1 = pf_group_by_age[pf_group_by_age[\u0026#39;age\u0026#39;] \u0026lt; 71] pf2 = pf_group_by_age_with_months[pf_group_by_age_with_months[\u0026#39;age_with_months\u0026#39;] \u0026lt; 71] \n  \n In\u0026nbsp;[157]: f, (ax1, ax2, ax3) = plt.subplots(3) f.set_size_inches(9, 9) sns.regplot(x=\u0026#39;age_with_months\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf2, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;red\u0026#39;}, ax=ax1) pf2.plot(x=\u0026#39;age_with_months\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax1) ax1.set_xlim([13, 71])\nsns.regplot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf1, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;green\u0026#39;}, ax=ax2) pf1.plot(x=\u0026#39;age\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax2) ax2.set_xlim([13, 71])\npf11 = pf1.copy() pf11[\u0026#39;ageRounded\u0026#39;] = np.round(pf1[\u0026#39;age\u0026#39;]/5.0)*5.0 sns.regplot(x=\u0026#39;ageRounded\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, data=pf11, scatter=False, lowess=True, ci=95, line_kws={\u0026#39;color\u0026#39;: \u0026#39;cyan\u0026#39;}, ax=ax3) pf11.plot(x=\u0026#39;ageRounded\u0026#39;, y=\u0026#39;friend_count_mean\u0026#39;, legend=False, ax=ax3) ax3.set_xlim([13, 71]) \n  \nOut[157]: (13, 71)  \n  \n \n  This is an example of bias variance trade off, and is similar to the trade off we make when choosing the bin width in histograms. One way, we can do this quite easily in seaborn is using the lowess option, however, in the current implementation it fails to provide any error estimate on the fitted regression!\nLowess option in the seaborn library uses Loess and Lowess method for smoothing. Here, the model is based on the idea that data is continuous and smooth.\nSo, through this post, we have noticed several ways to plot the same data. The obvious that arises is which plot to choose? In EDA, however, answer to this is, simply you should choose. The idea in EDA is that the same data when plotted differently, glean different incites.\n  \n","title":"Pseudo Facebook Data - Exploring Two Variables","url":"https://sadanand-singh.github.io/posts/pyplotstwovariables/"},{"tags":"Machine Learning, EDA, Python, Data Science","text":"In this post, we will learn about EDA of single variables using simple plots like histograms, frequency plots and box plots.\nData sets used below are part of a project from the UD651 course on udacity by Facebook. The data from the project corresponds to a typical data set at Facebook. You can load the data through the following command. Notice that this is a \u0026lt;TAB\u0026gt; delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below.\n\nIn\u0026nbsp;[1]: import pandas as pd import numpy as np #Read csv file pf = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\u0026quot;, sep = \u0026#39;\\t\u0026#39;) #summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \n /p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median RuntimeWarning)    Out[1]:    count unique top freq mean std min 50% max     userid 99003.0    1.59705e+06 344059 1.00001e+06 1.59615e+06 2.19354e+06   age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0    14.5304 9.01561 1 14 31   dob_year 99003.0    1975.72 22.5897 1900 1985 2000   dob_month 99003.0    6.28337 3.52967 1 6 12   gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  We need convert some of the variables from numeric to category.\n   In\u0026nbsp;[2]: cats = [\u0026#39;userid\u0026#39;, \u0026#39;dob_day\u0026#39;, \u0026#39;dob_year\u0026#39;, \u0026#39;dob_month\u0026#39;] for col in pf.columns: if col in cats: pf[col] = pf[col].astype(\u0026#39;category\u0026#39;)\n#summarize data pf.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True) \n  \n /p/ret/rettools/AnacondaPython/Python35/lib/python3.5/site-packages/numpy/lib/function_base.py:3403: RuntimeWarning: Invalid value encountered in median RuntimeWarning)    Out[2]:    count unique top freq mean std min 50% max     userid 99003.0 99003 2.19354e+06 1        age 99003.0    37.2802 22.5897 13 28 113   dob_day 99003.0 31 1 7900        dob_year 99003.0 101 1995 5196        dob_month 99003.0 12 1 11772        gender 98828.0 2 male 58574        tenure 99001.0    537.887 457.65 0  3139   friend_count 99003.0    196.351 387.304 0 82 4923   friendships_initiated 99003.0    107.452 188.787 0 46 4144   likes 99003.0    156.079 572.281 0 11 25111   likes_received 99003.0    142.689 1387.92 0 8 261197   mobile_likes 99003.0    106.116 445.253 0 4 25111   mobile_likes_received 99003.0    84.1205 839.889 0 4 138561   www_likes 99003.0    49.9624 285.56 0 0 14865   www_likes_received 99003.0    58.5688 601.416 0 2 129953      \n \n  The goal of this analysis is to understand user behavior and their demographics. We want to understand what they are doing on the Facebook and what they use. Please note this is not a real Facebook dataset.\nOur goal is to do some basic EDA (Exploratory Data Analysis) to understand any underlying patterns in the data. We will first look at a histogram of User\u0026rsquo;s Birthdays.\n   In\u0026nbsp;[3]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nax = sns.countplot(x=\u0026quot;dob_day\u0026quot;, data=pf) \n  \n  \n \n  We see some peculiar behavior of the data on the 1st of the month. Let us plot this data in more detail, in per month basis.\n   In\u0026nbsp;[4]: g = sns.factorplot(\u0026quot;dob_day\u0026quot;, col=\u0026quot;dob_month\u0026quot;, col_wrap=4, data=pf, kind=\u0026#39;count\u0026#39;, size=2.5, aspect=.8) g.set(xticklabels=[]) \n  \nOut[4]: \u0026lt;seaborn.axisgrid.FacetGrid at 0x7fffcd441208\u0026gt;  \n  \n \n  This explains the above plot. Because of the default settings, or users privacy concerns, numerous people have 1\u0026frasl;1 as their birthdays!\nNow, let us explore the distribution of friend counts in this data.\n   In\u0026nbsp;[5]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, bins=100) plt.xlim(0,1000) \n  \nOut[5]: (0, 1000)  \n  \n \n  We see the data has some outliers near 5000. This is an example of a long tail data. We want our analysis to be focused on the bunch of Facebook users, so we need to limit the axes of these plots. Additionally, we also want to look at these data as a function of gender. However, We also want to remove any data where gender is NA.\n   In\u0026nbsp;[6]: df = pf[pf.gender.notnull()] g = sns.FacetGrid(df, col=\u0026quot;gender\u0026quot;) g = g.map(plt.hist, \u0026quot;friend_count\u0026quot;, bins=100, color=\u0026quot;b\u0026quot;) plt.xlim(0,1000) \n  \nOut[6]: (0, 1000)  \n  \n \n  If we want to know, mean statistics of our data, we can use the \u0026lsquo;value_counts\u0026rsquo; command.\n   In\u0026nbsp;[164]: pf.groupby(\u0026#39;gender\u0026#39;).friend_count.describe() \n  \nOut[164]: gender female count 40254.000000 mean 241.969941 std 476.039706 min 0.000000 25% 37.000000 50% 96.000000 75% 244.000000 max 4923.000000 male count 58574.000000 mean 165.035459 std 308.466702 min 0.000000 25% 27.000000 50% 74.000000 75% 182.000000 max 4917.000000 Name: friend_count, dtype: float64  \n \n  Let us know look at the tenure of usage (measured in Years) of Facebook.\n   In\u0026nbsp;[8]: df = pf[pf.tenure.notnull()] ax = sns.distplot(df[\u0026quot;tenure\u0026quot;]/365, kde=False, bins=36) plt.xlim(0,7) plt.xlabel(\u0026#39;Number of years using Facebook\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Number of users in sample\u0026#39;, fontsize=12) \n  \nOut[8]: \u0026lt;matplotlib.text.Text at 0x7fffc9590ef0\u0026gt;  \n  \n \n  We will now look at any pattern in the ages of Facebook users in this dataset.\n   In\u0026nbsp;[9]: ax = sns.distplot(pf[\u0026quot;age\u0026quot;], kde=False, bins=100) plt.xlim(13,113) plt.xlabel(\u0026#39;Age of Users in Years\u0026#39;, fontsize=12) plt.ylabel(\u0026#39;Number of users in sample\u0026#39;, fontsize=12) \n  \nOut[9]: \u0026lt;matplotlib.text.Text at 0x7fffc94f7780\u0026gt;  \n  \n \n  One general theme of observation here is that most of the data have a long tail. In these circumstances, it is better to look at such data after certain types of transformation. Let us do such an analysis of “friend_count”.\n   In\u0026nbsp;[35]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, hist_kws={\u0026quot;alpha\u0026quot;: 0.9}) \n  \n  \n \n In\u0026nbsp;[49]: ax = sns.distplot(pf[\u0026quot;friend_count\u0026quot;], kde=False, bins=np.logspace(0,4), hist_kws={\u0026quot;alpha\u0026quot;: 0.9}) plt.xscale(\u0026#39;log\u0026#39;) \n  \n  \n \n  Let us try to compare distribution of male vs female friend counts.\n   In\u0026nbsp;[175]: def plotDensity(x, color=None, label=None, bins=np.linspace(0,1000,200), **kws):\n w = 100*np.ones_like(x)/x.size plt.hist(x, bins=bins, alpha=0.4, histtype=\u0026#39;step\u0026#39;, linewidth=2, label=label, color=color, weights=w, **kws) return  g = sns.FacetGrid(df, col=None, hue=\u0026#39;gender\u0026#39;, size=6.0, xlim=(6,600), ylim=(0,5), legend_out=True) g = (g.map(plotDensity, \u0026#39;friend_count\u0026#39;)).add_legend() g = g.set_axis_labels(\u0026#39;Friend Count\u0026#39;, \u0026#39;% of users\u0026#39;) \n  \n  \n \n  Similarly, we can compare distributions of www likes.\n   In\u0026nbsp;[179]: g = sns.FacetGrid(df, col=None, hue=\u0026#39;gender\u0026#39;, size=6.0, xlim=(1,15000)) g = (g.map(plotDensity, \u0026#39;www_likes\u0026#39;, bins=np.logspace(0,5,50))).add_legend() g = g.set_axis_labels(\u0026#39;www Likes Count\u0026#39;, \u0026#39;% of users\u0026#39;) plt.xscale(\u0026#39;log\u0026#39;) \n  \n  \n \n  We cal also look at the total number of likes numerically per gender, as follows:\n   In\u0026nbsp;[173]: pf.groupby(\u0026#39;gender\u0026#39;).www_likes.sum() \n  \nOut[173]: gender female 3507665 male 1430175 Name: www_likes, dtype: int64  \n \n  We can also compare two distributions graphically using “box plots”. We can also look at the actual value using the by command. Here, we are trying to understand which gender initiated more friendships.\n   In\u0026nbsp;[185]: ax = sns.boxplot(x=\u0026#39;gender\u0026#39;, y=\u0026#39;friendships_initiated\u0026#39;, data=df) plt.ylim(0,200) \n  \nOut[185]: (0, 200)  \n  \n \n In\u0026nbsp;[186]: pf.groupby(\u0026#39;gender\u0026#39;).friendships_initiated.describe() \n  \nOut[186]: gender female count 40254.000000 mean 113.899091 std 195.139308 min 0.000000 25% 19.000000 50% 49.000000 75% 124.750000 max 3654.000000 male count 58574.000000 mean 103.066600 std 184.292570 min 0.000000 25% 15.000000 50% 44.000000 75% 111.000000 max 4144.000000 Name: friendships_initiated, dtype: float64  \n \n  Next, we want to understand if users have used certain features of Facebook or not. If we look at the summary of mobile_likes variable, median is close to 0, indicating a lot many users with 0 values for this variable. We can look also look at the logical value if value of this quantity is non-zero. We can additionally create a new variable called mobile_check_in that takes a value 1 if mobile_likes is non-zero.\n   In\u0026nbsp;[187]: pf.mobile_likes.describe() \n  \nOut[187]: count 99003.000000 mean 106.116300 std 445.252985 min 0.000000 25% 0.000000 50% 4.000000 75% 46.000000 max 25111.000000 Name: mobile_likes, dtype: float64  \n \n In\u0026nbsp;[201]: (pf.mobile_likes \u0026gt; 0).value_counts() \n  \nOut[201]: True 63947 False 35056 Name: mobile_likes, dtype: int64  \n \n In\u0026nbsp;[200]: pf[\u0026#39;mobile_check_in\u0026#39;] = pd.Series(np.where(pf[\u0026#39;mobile_likes\u0026#39;] \u0026gt; 0, 1, 0)).astype(\u0026#39;category\u0026#39;) pf.mobile_check_in.value_counts() \n  \nOut[200]: 1 63947 0 35056 Name: mobile_check_in, dtype: int64  \n \n  We can find percentage of people who have done mobile check in.\n   In\u0026nbsp;[203]: frac = (pf.mobile_check_in == 1).sum()/pf.mobile_check_in.size print(\u0026quot;Fraction of Mobile Check-ins = \u0026quot;, frac) \n  \n Fraction of Mobile Check-ins = 0.645909719907     \n  We find that about 65% of people have used mobile devices for check in and hence it would be a good decision to continue development of such products.\nIn summary, here we have learned to make inferences about single variable data using a combination of plots - histograms, box plots and frequency plots; along with various numerical data.\n  \n","title":"Pseudo Facebook Data - Plots in Python","url":"https://sadanand-singh.github.io/posts/onevariableeda/"},{"tags":"EDA, Python, Data Science","text":"The data set used here is part of a project from UD651 course on udacity by Facebook.\nThe data from the project corresponds to a survey from reddit.com. You can load the data through the following command. We will first look at the different attributes of this data using the summary() and describe() pandas methods.\n\nIn\u0026nbsp;[45]: import pandas as pd import numpy as np #Read csv file reddit = pd.read_csv(\u0026quot;https://s3.amazonaws.com/udacity-hosted-downloads/ud651/reddit.csv\u0026quot;).astype(object) #summarize data reddit.describe(include=\u0026#39;all\u0026#39;, percentiles=[]).T.replace(np.nan,\u0026#39; \u0026#39;, regex=True)    \nOut[45]:    count unique top freq     id 32754.0 32754.0 32756 1.0   gender 32553.0 2.0 0 26418.0   age.range 32666.0 7.0 18-24 15802.0   marital.status 32749.0 6.0 Single 10428.0   employment.status 32603.0 6.0 Employed full time 14814.0   military.service 32749.0 2.0 No 30526.0   children 32535.0 2.0 No 27488.0   education 32610.0 7.0 Bachelor's degree 11046.0   country 32577.0 439.0 United States 20967.0   state 20846.0 52.0 California 3401.0   income.range 31139.0 8.0 Under $20,000 7892.0   fav.reddit 28393.0 1833.0 askreddit 2123.0   dog.cat 32749.0 3.0 I like dogs. 17151.0   cheese 32749.0 11.0 Other 6563.0      \n \n  The describe() method helped us get an overview of all the data available to us. We also ensured that all the data read was a categorical data.\nLet us look at the age.range variable in more detail. We can look at the different levels of this variables using the cat.categories property of a Pandas Series.\n   In\u0026nbsp;[46]: reddit[\u0026quot;age.range\u0026quot;].astype(\u0026#39;category\u0026#39;).cat.categories \n  \nOut[46]: Index([\u0026#39;18-24\u0026#39;, \u0026#39;25-34\u0026#39;, \u0026#39;35-44\u0026#39;, \u0026#39;45-54\u0026#39;, \u0026#39;55-64\u0026#39;, \u0026#39;65 or Above\u0026#39;, \u0026#39;Under 18\u0026#39;], dtype=\u0026#39;object\u0026#39;)  \n \n  This shows there are 7 possible values of this variable and some where no data is available (NA).\nA more pictorial view of this can be seen using a histogram plot of this.\n   In\u0026nbsp;[57]: import matplotlib.pyplot as plt import seaborn as sns sns.set(style=\u0026quot;darkgrid\u0026quot;) %matplotlib inline\nnewOrder = [\u0026quot;Under 18\u0026quot;, \u0026quot;18-24\u0026quot;, \u0026quot;25-34\u0026quot;, \u0026quot;35-44\u0026quot;, \u0026quot;45-54\u0026quot;, \u0026quot;55-64\u0026quot;, \u0026quot;65 or Above\u0026quot;] ax = sns.countplot(x=\u0026quot;age.range\u0026quot;, data=reddit, order=newOrder) \n  \n  \n \n  Similarly, we can also plot a distribution of income range.\n   In\u0026nbsp;[51]: ax = sns.countplot(x=\u0026quot;income.range\u0026quot;, data=reddit) locs, labels = plt.xticks() ax = plt.setp(labels, rotation=90) \n  \n  \n \n  One problem with the above plots is that the different levels are not ordered. This can be fixed using ordered Factors, instead of regular factor type variables. Additionally, We need to use a more reasonable x-label for plotting income.range.\n   In\u0026nbsp;[52]: newLevels = [\u0026quot;100K\u0026quot;, \u0026quot;\u0026gt;150K\u0026quot;, \u0026quot;20K\u0026quot;,\u0026quot;30K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;50K\u0026quot;, \u0026quot;70K\u0026quot;, \u0026quot;\u0026lt;20K\u0026quot;] reddit[\u0026quot;income.range\u0026quot;] = reddit[\u0026quot;income.range\u0026quot;].astype(\u0026#39;category\u0026#39;) reddit[\u0026quot;income.range\u0026quot;] = reddit[\u0026quot;income.range\u0026quot;].cat.rename_categories(newLevels) \n  \n In\u0026nbsp;[55]: newOrder = [\u0026quot;\u0026lt;20K\u0026quot;, \u0026quot;20K\u0026quot;,\u0026quot;30K\u0026quot;, \u0026quot;40K\u0026quot;, \u0026quot;50K\u0026quot;, \u0026quot;70K\u0026quot;, \u0026quot;100K\u0026quot;, \u0026quot;\u0026gt;150K\u0026quot;] ax = sns.countplot(x=\u0026quot;income.range\u0026quot;, data=reddit, order=newOrder) \n  \n  \n \n\n","title":"Reddit Survey: Introduction to Pandas","url":"https://sadanand-singh.github.io/posts/pandasintroreddit/"},{"tags":"Programming, Python","text":"Python is a widely used general-purpose, high-level programming language. Due to its focus on readability, wide-spread popularity and existence of a plethora of libraries (also called modules), it is one of the most preferred programming languages for scientists and engineers.\n\nPython for Non-Programmers In this series of python tutorials, I will provide a set of lectures on various basic topics of python. The prime target audience for this series are scientist in non-programming fields like microbiology, genetics, psychology etc. who have some to none programming experience.\nHere is a brief list of topics I will cover per week. I will also post exercises at the end of each session, along with the expected outputs. You should plan to complete these exercises within 5-6 days, before the new tutorial is posted. You will be judging your exercises on your own. The goal should be to match your program\u0026rsquo;s output to the expected output.\n Week 1 : Working with Python on Windows, Concept of Variables \u0026amp; Math Operations, Displaying Outputs Week 2 : User Inputs, Modules, Comments and Basics of Strings Week 3 : More on Strings, Lists and Other Containers Week 4 : Looping/iterating, if/else Conditions Week 5 : Advanced String Operations Week 6 : Regular Expressions and Strings Week 7 : Reading and Writing Files Week 8 : Functions and Writing Scripts Week 9 : Interacting with Operating System Week 10 : Handling and Plotting Data in Python Week 11 : Basic Statistics in Python using Pandas Week 12 : Introduction to BioPython  Week 1. Introduction to Python To start working with any programming language, first thing you need is a working installation of that language. Today, we will go through installation of python on Windows machines.\nTo keep things simple, We will be running our simple programs in google chrome browser, without any need of an installation.\nFor later exercises, from Week 7 onwards, I would highly recommend getting access to a Linux/Mac machine. However, I will also provide doing the same things on Windows machines and Google Chrome as well.\nIn this week\u0026rsquo;s session, I will be assuming you will be using online python in google chrome.\nTo follow these tutorials, please run any code and observe output that you see in code blocks.\nInstalling Python Go to the following website to get access to python.\nYou should get a window like this:\n You can work with this system in two ways: A. Write your code interactively (one command at a time) on the dark screen on the left. Pressing ENTER will show you the output of that particular command.\nLets\u0026rsquo; try our first program:\nIn\u0026nbsp;[1]: print(\u0026quot;My Name is so-and-so\u0026quot;)    \n My Name is so-and-so     \n  The above program will simply output whatever was put under \u0026ldquo;quotes\u0026rdquo;. We will learn more about the print() method (what is a method/function in python?) towards the end of this session.\nVariables\u0026#182;A variable is a symbol or name that stands for a value. For example, in the expression x = 23\nx is a name/symbol whose value is numerical and equal to 23. if we write something like this: y = x+12-5, then y is a new variable whose value should be equal to 23+12-5.\nLet\u0026rsquo;s try these in a program\n   In\u0026nbsp;[2]: x=23 print(\u0026quot;x = \u0026quot;,x) \n  \n x = 23     \n In\u0026nbsp;[3]: y=x+12-5 print(\u0026quot;y =\u0026quot;,y) \n  \n y = 30     \n In\u0026nbsp;[4]: print(23+12-5) \n  \n 30     \n  We ran 3 commands. First we created a variable \u0026lsquo;x\u0026rsquo; with a value of 23. We verified it value by using a print() method! Second, we created another variable called \u0026lsquo;y\u0026rsquo; whose values is equal to mathematical operation of \u0026lsquo;x+12-5\u0026rsquo;. If you remember basic algebra, this is exactly that. Finally, we confirmed that value of \u0026lsquo;y\u0026rsquo; is exactly equal to 23+12-5.\nHopefully, you got a feel of variables. Variables are not limited to just numbers though. We can also store text. For example:\n   In\u0026nbsp;[5]: name = \u0026quot;Sadanand Singh\u0026quot; print(\u0026quot;My Name is:\u0026quot;, name) \n  \n My Name is: Sadanand Singh     \n  Here, we saved my name Sadanand Singh in a variable called \u0026ldquo;name\u0026rdquo;.\nConcept of variable is very fundamental to any programming language. You can think of them as tokens that store some value.\nLets consider this example to understand variable in more detail.\nYou are doing your taxes, all of your calculations depend on your net income. If you do all your calculation in terms of actual value of net income, every time you write the number, your chances of making a mistake increases. Furthermore, suppose, you want to update the income due to some mistake in the beginning, now you have update every place where you used your income.\nLets consider a second case where you declare a variable called \u0026ldquo;income\u0026rdquo;, and store income = 30000. Now, any time you do any calculation with income, you will be using the variable \u0026ldquo;income\u0026rdquo;. In this framework, because you have to type your income just once, chances of making mistakes are least and changing it needs just one change!\n  \nMath Operations Now that we know how to declare and use variables, we will look at first what all we can do with numbers in python. Using simple math operations are extremely easy in Python. Here are all the basic math operations that one can do in Python.\n   Syntax Math Operation Name     a+b $a + b$ Addition   a-b $a - b$ Subtraction   a*b $a \\times b$ Multiplication   a/b $a \\div b$ Division   a**b $a^b$ Power/Exponent   abs(a) $\\lvert a \\rvert$ Absolute Value   -a $-1 \\times a$ Negation   a//b quotient of $a \\div b$ Quotient   a%b Remainder of $a \\div b$ Remainder    Here are some example operation. Please repeat these and observe the use of parenthesis in using the BODMAS principle.\nIn\u0026nbsp;[6]: 3 + 5    \nOut[6]: 8  \n \n In\u0026nbsp;[7]: 2 * 3 + 3.34 + 4 - 45.67 \n  \nOut[7]: -32.33  \n \n In\u0026nbsp;[8]: 12.7 - 10 * 23.5 / 0.5 \n  \nOut[8]: -457.3  \n \n In\u0026nbsp;[9]: 12 - (11 + 34) / 2 \n  \nOut[9]: -10.5  \n \n In\u0026nbsp;[10]: a,b=5,6 \n  \n  Above is a special case of assigning multiple variables together. In the above example we stored a=5 and b=6. Lets confirm these:\n   In\u0026nbsp;[11]: print(\u0026quot;a = \u0026quot;,a) print(\u0026quot;b = \u0026quot;,b) \n  \n a = 5 b = 6     \n In\u0026nbsp;[12]: c = a**b print(\u0026quot; a raised to the power b is:\u0026quot;,c) \n  \n  a raised to the power b is: 15625     \n In\u0026nbsp;[13]: b = -b \n  \n In\u0026nbsp;[14]: print(\u0026quot;b = \u0026quot;,b) \n  \n b = -6     \n  Can you guess what we did here? First we use negation operation to get \u0026ldquo;-b\u0026rdquo; i.e. -6. Then, we we redefined b to be equal to -6! Let consider the following example: a = a-b-3. What do you expect the value to be? Now lets check if you are correct:\n   In\u0026nbsp;[15]: a = a-b-3 print(\u0026quot;new value of a is: \u0026quot;,a) \n  \n new value of a is: 8     \n  What do you expect if we do b = -b again?\n   In\u0026nbsp;[16]: b = -b print(\u0026quot;New Value of b is:\u0026quot;,b) \n  \n New Value of b is: 6     \n In\u0026nbsp;[17]: a//b \n  \nOut[17]: 1  \n \n In\u0026nbsp;[18]: a%b \n  \nOut[18]: 2  \n \n In\u0026nbsp;[19]: b**-2 \n  \nOut[19]: 0.027777777777777776  \n \n  You can perform many other advanced math operations using the \u0026ldquo;math module\u0026rdquo;. To use them, first you will need to import the math module in your code like this:\n   In\u0026nbsp;[20]: import math \n  \n  Then, you use operations like the following:\n   In\u0026nbsp;[21]: c = math.sqrt(25) print(c) c = math.log(10) print(c) c = math.log10(10) print(c) c = math.cos(math.pi) print(c) \n  \n 5.0 2.302585092994046 1.0 -1.0     \n  You can all other available mathematical operation in this module on the python website.\n  \nprint() Function in Python Primary way to see and print information on your screen in python is using a method/function called print(). We will learn more about functions in python later. For today, you can think of functions as a program that \u0026ldquo;does something\u0026rdquo;. For example, the function print(), does the job of printing things on screen.\nNotice the use of () in functions. () separates a function from a variable. For example, \u0026ldquo;foo\u0026rdquo; is a variable, whereas foo() is a function, sometimes also called as method.\nFunctions also have a concept of arguments. Arguments can be thought as inputs to functions. For example, we have function that adds 2 numbers, then this function will need 2 arguments, the two numbers that we want to add. We can denote this function as, addition(a,b).\nSimilarly, functions also have a concept of return values. Return value can be thought as the output of that function. For example, in the above example of addition(a,b) function, sum of two numbers will be the \u0026ldquo;return value\u0026rdquo; of the function. We can write this as, c = addition(a,b). Here, a and b are arguments to function addition() and c is the return value of this function.\nA function can have any number of arguments, zero to any number; where it can have either zero or 1 return values.\nNow, coming back to the print() method, that we have been using throughout this tutorial.\nprint() method can take any number of arguments separated by commas. All it does is to \u0026ldquo;print\u0026rdquo; those on your screen. Lets look at some examples:\nIn\u0026nbsp;[22]: print(3,4) print(\u0026quot;My Name is Sadanand Singh\u0026quot;) print(\u0026quot;My Name is \u0026quot;,\u0026quot;Sadanand Singh\u0026quot;,\u0026quot;and My age is: \u0026quot;, 29)    \n 3 4 My Name is Sadanand Singh My Name is Sadanand Singh and My age is: 29     \n  Now, lets try something fun.\n   In\u0026nbsp;[23]: print(\u0026quot;My Name is\u0026quot;,\u0026quot;Sadanand\u0026quot;, sep=\u0026quot;***\u0026quot;) \n  \n My Name is***Sadanand     \n In\u0026nbsp;[24]: print(\u0026quot;My Name is\u0026quot;,\u0026quot;Sadanand\u0026quot;,\u0026quot;Singh\u0026quot;,\u0026quot;My Age is\u0026quot;,\u0026quot;12\u0026quot;,\u0026quot;\u0026quot;,sep=\u0026quot;***\u0026quot;) \n  \n My Name is***Sadanand***Singh***My Age is***12***     \n  Can you explain what is happening here!\n  \nExcercise We will all things we have learned today using the exercise below.\nWe will follow the tax preparation example:\n Create a variable to store \u0026ldquo;income\u0026rdquo; Create another variable called \u0026ldquo;taxRate\u0026rdquo; which is equal to 1/100000th of \u0026ldquo;income\u0026rdquo;. Net federal tax will be equal to 1.5 times income times taxRate Net state tax will be equal to square root on federal tax Net tax will be federal tax + state tax Total final tax will be Net tax + log to the base of 2 of Net Tax Print following values clearly using print(): income, taxRate, Federal Tax, State Tax, Net Tax and Final Tax. First run with an income of 60000 Repeat with an income of 134675  Your output should look like the following in two cases.\nCase 1: income = 60000\n Total Income is: 60000 Tax Rate is: 0.059999999999999998 Total Federal Tax is: 1800.0 Total State Tax is: 42.426406871192853 Net Tax is: 1842.4264068711927 Total Tax is: 1853.2737981499038  Case 2: income = 134675\n Total Income is: 134675 Tax Rate is: 0.13467499999999999 Total Federal Tax is: 9068.6778125000001 Total State Tax is: 95.229605756298284 Net Tax is: 9163.9074182562981 Total Tax is: 9177.0691654243201  Great! Next week we dive into Python further.\n\n","title":"Python Tutorial - Week 1","url":"https://sadanand-singh.github.io/posts/pythontutweek1/"},{"tags":"Food, Recipe","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule.\nBut, its never too late. Especially, when you can start with some exotic food!\n\nCarrot halwa is a common north Indian dessert, healthy carrots cooked in milk and mango puree.\n  Ingredients  8-10 organic carrots (for a more authentic taste). Condensed milk (medium size) or 1-2 cup Ricotta cheese 1-2 cup whole Milk. Dried nuts (Chopped Almonds, Cashews, Golden raisins and Dates) Sugar/Fresh Mango puree as per taste. 3-4 Tablespoon unsalted butter 3-4 whole cardamom or 1\u0026frasl;2 teaspoon cardamom powder  Preparation  Wash and Peel the carrots and grate them. Heat a non-stick pan on medium flame and add 3-4 tablespoon of butter. When the butter melts, add the grated carrots and start frying. Stir the carrots till all the water from the carrots is lost. Add Ricotta cheese with 1-2 cup whole milk or 1 can of Condensed milk and let the carrots cook for 5-7 minutes on low flame. Continuously stir the carrots so as not to burn them. When 3/4th of the milk is dried, add chopped nuts and Sugar. Turn off the flame after 3-4 minutes, after all the milk is dried. Let the halwa cool down. Garnish it with nuts and dried milk.   Interesting Facts  Nutritional Value of carrots. More about Gajar Halwa ","title":"Carrot Halwa Recipe","url":"https://sadanand-singh.github.io/posts/carrothalwarecipe/"},{"tags":"Algorithms, Python","text":"The world of computers is moving fast. While going through some materials on algorithms, I have come across an interesting discussion -enhancements in hardware (cpu) vis-a-vis algorithms.\nOne side of the coin is the hardware - the speed of computers. The famous Moore\u0026rsquo;s law states that:\n\nThe complexity for minimum component costs has increased at a rate of roughly a factor of two per year. Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years.\nG. Moore, 1965  In simple words, Moore\u0026rsquo;s law is the observation that, over the history of computing hardware, the number of transistors in a dense integrated circuit has doubled approximately every two years. More precisely, the number of transistors in a dense integrated circuit has increased by a factor of 1.6 every two years. More recently, keeping up with this has been challenging. In the context of this discussion, the inherent assumption is that number of transistors is directly proportional to the speed of computers.\nNow, looking at the other side of the coin - speed of algorithms. According to Excerpt from Report to the President and Congress: Designing a Digital Future, December 2010 (page 97):\nEveryone knows Moore’s Law – a prediction made in 1965 by Intel co-­founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years\u0026hellip;. in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed.\nThe gain in computing speed due to algorithms have been simply phenomenal, unprecedented, to say the least! Being actively involved with realization of Moore\u0026rsquo;s law, I have been naturally attracted in the study and design of algorithms.\nTo get a more practical perspective on this, lets look at the problem of finding large Fibonacci numbers. These numbers have been used in wide areas ranging from arts to economics to biology to computer science to the game of poker! The simple definition of these numbers are:\n$$F_{n} = \\begin{cases} F_{n-2} \u0026#43; F_{n-1} \u0026amp; \\text{if } n \u0026gt; 1 \\\\ 1 \u0026amp; \\text{if } n = 1 \\\\ 0 \u0026amp; \\text{if } n = 0 \\end{cases}$$     So, first few Fibonacci numbers are: $0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 \\ldots $ These numbers grow almost as fast as powers of 2: for example, \\(F_{30}\\)    is over a million, and \\(F_{100}\\)    is 21 digits long! In general, \\(F_n \\approx 2^{0.694n}\\)    Clearly, we need a computing device to calculate say \\(F_{200}\\)   .\nHere is a simple plot of first few Fibonacci numbers:\n #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016{-webkit-user-select:none;-webkit-font-smoothing:antialiased;font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text.major{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay text.value{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:16px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay text.label{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:10px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:14px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{font-family:Consolas,\"Liberation Mono\",Menlo,Courier,monospace;font-size:64px} #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016{background-color:#f0f0f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 path,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 rect,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 circle{-webkit-transition:250ms ease-in;-moz-transition:250ms ease-in;transition:250ms ease-in}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .graph \u0026gt; .background{fill:#f0f0f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .plot \u0026gt; .background{fill:#f8f8f8}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .graph{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend:hover text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guide.line{stroke:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .major.line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis text.major{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .line-graph .axis.x .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .stackedline-graph .axis.x .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .xy-graph .axis.x .guides:hover .guide.line{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guides:hover text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .reactive{fill-opacity:.5;stroke-opacity:.8}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .ci{stroke:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .reactive.active,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .active .reactive{fill-opacity:.9;stroke-opacity:.9;stroke-width:4}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .ci .reactive.active{stroke-width:1.5}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .series text{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip rect{fill:#f8f8f8;stroke:rgba(0,0,0,0.9);-webkit-transition:opacity 250ms ease-in;-moz-transition:opacity 250ms ease-in;transition:opacity 250ms ease-in}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .label{fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .legend{font-size:.8em;fill:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .x_label{font-size:.6em;fill:rgba(0,0,0,0.9)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .xlink{font-size:.5em;text-decoration:underline}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip .value{font-size:1.5em}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .bound{font-size:.5em}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .max-value{font-size:.75em;fill:rgba(0,0,0,0.6)}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .map-element{fill:#f8f8f8;stroke:rgba(0,0,0,0.6) !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .map-element .reactive{fill-opacity:inherit;stroke-opacity:inherit}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-0,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-0 a:visited{stroke:#00b2f0;fill:#00b2f0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-1,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-1 a:visited{stroke:#43d9be;fill:#43d9be}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-2,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .color-2 a:visited{stroke:#0662ab;fill:#0662ab}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-0 text{fill:black}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-1 text{fill:black}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .text-overlay .color-2 text{fill:black} #chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text.no_data{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .guide.line{fill:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .centered{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .title{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .legends .legend text{fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x text{text-anchor:middle}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x:not(.web) text[transform]{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x:not(.web) text[transform].backwards{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y text{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y text[transform].backwards{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 text{text-anchor:start}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 text[transform].backwards{text-anchor:end}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guide.line{stroke-dasharray:4,4}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .major.guide.line{stroke-dasharray:6,6}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.y .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.y2 .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .vertical .axis.x .guide.line{opacity:0}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .horizontal .axis.always_show .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .vertical .axis.always_show .guide.line{opacity:1 !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.y2 .guides:hover .guide.line,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis.x .guides:hover .guide.line{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .axis .guides:hover text{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .nofill{fill:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .subtle-fill{fill-opacity:.2}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot{stroke-width:1px;fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot.active{stroke-width:5px}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .dot.negative{fill:transparent}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 text,#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 tspan{stroke:none !important}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .series text.active{opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip rect{fill-opacity:.95;stroke-width:.5}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .tooltip text{fill-opacity:1}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .showable{visibility:hidden}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .showable.shown{visibility:visible}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .gauge-background{fill:rgba(229,229,229,1);stroke:none}#chart-89be0ddc-40d8-49ce-9109-b3cd2f229016 .bg-lines{stroke:#f0f0f0;stroke-width:2px} window.pygal = window.pygal || {};window.pygal.config = window.pygal.config || {};window.pygal.config['89be0ddc-40d8-49ce-9109-b3cd2f229016'] = {\"allow_interruptions\": false, \"box_mode\": \"extremes\", \"classes\": [\"pygal-chart\"], \"css\": [\"file://style.css\", \"file://graph.css\"], \"defs\": [], \"disable_xml_declaration\": false, \"dots_size\": 2.5, \"dynamic_print_values\": false, \"explicit_size\": false, \"fill\": false, \"force_uri_protocol\": \"https\", \"formatter\": null, \"half_pie\": false, \"height\": 600, \"include_x_axis\": false, \"inner_radius\": 0, \"interpolate\": null, \"interpolation_parameters\": {}, \"interpolation_precision\": 250, \"inverse_y_axis\": false, \"js\": [\"//kozea.github.io/pygal.js/2.0.x/pygal-tooltips.min.js\"], \"legend_at_bottom\": true, \"legend_at_bottom_columns\": 3, \"legend_box_size\": 12, \"logarithmic\": true, \"margin\": 20, \"margin_bottom\": null, \"margin_left\": null, \"margin_right\": null, \"margin_top\": null, \"max_scale\": 16, \"min_scale\": 4, \"missing_value_fill_truncation\": \"x\", \"no_data_text\": \"No data\", \"no_prefix\": false, \"order_min\": null, \"pretty_print\": false, \"print_labels\": false, \"print_values\": false, \"print_values_position\": \"center\", \"print_zeroes\": true, \"range\": null, \"rounded_bars\": null, \"secondary_range\": null, \"show_dots\": true, \"show_legend\": true, \"show_minor_x_labels\": true, \"show_minor_y_labels\": true, \"show_only_major_dots\": false, \"show_x_guides\": false, \"show_x_labels\": true, \"show_y_guides\": true, \"show_y_labels\": true, \"spacing\": 10, \"stack_from_top\": false, \"strict\": false, \"stroke\": true, \"stroke_style\": null, \"style\": {\"background\": \"#f0f0f0\", \"ci_colors\": [], \"colors\": [\"#00b2f0\", \"#43d9be\", \"#0662ab\", \"#00668a\", \"#98eadb\", \"#97d959\", \"#033861\", \"#ffd541\", \"#7dcf30\", \"#3ecdff\", \"#daaa00\"], \"font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"foreground\": \"rgba(0, 0, 0, 0.9)\", \"foreground_strong\": \"rgba(0, 0, 0, 0.9)\", \"foreground_subtle\": \"rgba(0, 0, 0, 0.6)\", \"guide_stroke_dasharray\": \"4,4\", \"label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"label_font_size\": 10, \"legend_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"legend_font_size\": 14, \"major_guide_stroke_dasharray\": \"6,6\", \"major_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"major_label_font_size\": 10, \"no_data_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"no_data_font_size\": 64, \"opacity\": \".5\", \"opacity_hover\": \".9\", \"plot_background\": \"#f8f8f8\", \"stroke_opacity\": \".8\", \"stroke_opacity_hover\": \".9\", \"title_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"title_font_size\": 16, \"tooltip_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"tooltip_font_size\": 14, \"transition\": \"250ms ease-in\", \"value_background\": \"rgba(229, 229, 229, 1)\", \"value_colors\": [], \"value_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_font_size\": 16, \"value_label_font_family\": \"Consolas, \\\"Liberation Mono\\\", Menlo, Courier, monospace\", \"value_label_font_size\": 10}, \"title\": \"Fibonacci Numbers\", \"tooltip_border_radius\": 0, \"tooltip_fancy_mode\": true, \"truncate_label\": null, \"truncate_legend\": null, \"width\": 800, \"x_label_rotation\": 0, \"x_labels\": [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"], \"x_labels_major\": null, \"x_labels_major_count\": null, \"x_labels_major_every\": null, \"x_title\": null, \"xrange\": null, \"y_label_rotation\": 0, \"y_labels\": null, \"y_labels_major\": null, \"y_labels_major_count\": null, \"y_labels_major_every\": null, \"y_title\": null, \"zero\": 1, \"legends\": [\"Fibonacci(n)\", \"2\\u207f\", \"n\\u00b2\"]}Fibonacci Numbers 112244668810102020404060608080100100200200400400600600800800100010002000200040004000600060008000800010000100002000020000400004000060000600008000080000100000100000200000200000400000400000600000600000800000800000100000010000001234567891011121314151617181920Fibonacci Numbers113.615384615384615490.01149.445344129554655490.02285.27530364372468465.533121.10526315789473451.168418732331745156.93522267206478433.112761675259658192.76518218623482416.5613228.59514170040487399.3392269055432721264.4251012145749382.3882231419204834300.2550607287449365.3571603893667955336.085020242915348.356687018645861089371.914979757085331.344530941323211144407.7449392712551314.336837464663312233443.5748987854251297.3274394559701413377479.40485829959516280.318692524917714610515.2348178137652263.309696904968915987551.0647773279353246.30079627581827161597586.8947368421052229.2918593634248172584622.7246963562753212.2829363099989184181658.5546558704452195.27400796291863196765694.3846153846154178.265081637834520213.615384615384615465.51449.445344129554655441.02885.27530364372468416.5316121.10526315789473392.0432156.93522267206478367.5564192.76518218623482343.06128228.59514170040487318.57256264.4251012145749294.08512300.2550607287449269.591024336.085020242915244.99999999999997102048371.914979757085220.5114096407.7449392712551196.00000000000006128192443.5748987854251171.500000000000061316384479.40485829959516147.01432768515.2348178137652122.51565536551.064777327935398.016131072586.894736842105273.517262144622.724696356275349.0000000000000618524288658.554655870445224.500000000000057191048576694.3846153846154-5.684341886080802e-1420113.615384615384615490.01449.445344129554655441.02985.27530364372468412.33683746466335316121.10526315789473392.0425156.93522267206478376.2255233505192536192.76518218623482363.33683746466335649228.59514170040487352.4396088191774764264.4251012145749343.0881300.2550607287449334.67367492932679100336.085020242915327.225523350519210121371.914979757085320.4878506867724511144407.7449392712551314.336837464663312169443.5748987854251308.6784538110864613196479.40485829959516303.4396088191774714225515.2348178137652298.562360815182615256551.0647773279353294.016289586.8947368421052289.714320778733417324622.7246963562753285.673674929326718361658.5546558704452281.851551841264419400694.3846153846154278.2255233505192720Fibonacci(n)2ⁿn² The most basic algorithm, that comes to mind is a recursive scheme that taps directly into the above definition of Fibonacci series.\ndef fibRecursive(n): if n == 0: return 0 if n == 1: return 1 return fibRecursive(n-2)\u0026#43;fibRecursive(n-1)   If you analyze this scheme, this is in fact an exponential algorithm, i.e. fibRecursive( n ) is proportional to \\(2^{0.694n} \\approx (1.6)^n\\)   , so it takes 1.6 times longer to compute \\(F_{n\u0026#43;1}\\)    than \\(F_n\\)   . This is interesting. Recall, under Moore\u0026rsquo;s law, computers get roughly 1.6 times faster every 2 years. So if we can reasonably compute \\(F_{100}\\)    with this year\u0026rsquo;s technology, then only after 2 years we will manage to get \\(F_{101}\\)   ! Only one more Fibonacci number every 2 years!\nLuckily, algorithms have grown at a much faster pace. Let\u0026rsquo;s consider improvements w.r.t to this current problem of finding $n^{th}$ Fibonacci number, $F_n$.\nFirst problem we should realize in the above recursive scheme is that we are recalculating lower $F_n$ at each recursion level. Lets solve this issue by storing each calculation and avoiding any re-calculation!\ndef fibN2(n): a = 0 b = 1 if n == 0: return 0 for i in range(1, n\u0026#43;1): c = a \u0026#43; b a = b b = c return b   On first glance this looks like an $\\mathcal{O}(n)$ scheme, as we consider each addition as one operation. However, we should realize that as $n$ increases, addition can not be assumed as a single operation, rather every step of addition is an $\\mathcal{O}(n)$ operation, recall first grade Math for adding numbers digit by digit!! Hence, this algorithm is an $\\mathcal{O}(n^2)$ scheme. Can we do better?\nYou bet, we can! Lets consider the following scheme:\n$$\\begin{pmatrix} 1\u0026amp;1 \\\\ 1\u0026amp;0 \\end{pmatrix}^n = \\begin{pmatrix} F_{n\u0026#43;1}\u0026amp;F_n \\\\ F_n\u0026amp;F_{n-1} \\end{pmatrix}$$     We can use a recursive scheme to calculate this matrix power using a divide and conquer scheme in $\\mathcal{O}(\\log{}n)$ time.\ndef mul(A, B): a, b, c = A d, e, f = B return a*d \u0026#43; b*e, a*e \u0026#43; b*f, b*e \u0026#43; c*f def pow(A, n): if n == 1: return A if n \u0026amp; 1 == 0: return pow(mul(A, A), n//2) else: return mul(A, pow(mul(A, A), (n-1)//2)) def fibLogN(n): if n \u0026lt; 2: return n return pow((1,1,0), n-1)[0]   Lets think a bit harder about this. Is it really an $\\mathcal{O}(\\log{}n)$ scheme? It involves multiplication of numbers, the method mul(A, B). What happens when $n$ is very large? Sure, this will blow up, as typical multiplication would be an $\\mathcal{O}(n^2)$ operation. So, in fact, our new scheme is $\\mathcal{O}(n^2 \\log{}n)$!\nLuckily, we can solve even large multiplications in $\\mathcal{O}(n^{log_2{3}} \\approx n^{1.585})$, using Karatsuba multiplication, which is again a divide and conquer scheme.\nHere is one simple implementation (Same as the above scheme, but with the following mul(A,B) method):\n_CUTOFF = 1536 def mul(A, B): a, b, c = A d, e, f = B return multiply(a,d) \u0026#43; multiply(b,e), multiply(a,e) \u0026#43; multiply(b,f), multiply(b,e) \u0026#43; multiply(c,f) def multiply(x, y): if x.bit_length() \u0026lt;= _CUTOFF or y.bit_length() \u0026lt;= _CUTOFF: return x * y else: n = max(x.bit_length(), y.bit_length()) half = (n \u0026#43; 32) // 64 * 32 mask = (1 \u0026lt;\u0026lt; half) - 1 xlow = x \u0026amp; mask ylow = y \u0026amp; mask xhigh = x \u0026gt;\u0026gt; half yhigh = y \u0026gt;\u0026gt; half a = multiply(xhigh, yhigh) b = multiply(xlow \u0026#43; xhigh, ylow \u0026#43; yhigh) c = multiply(xlow, ylow) d = b - a - c return (((a \u0026lt;\u0026lt; half) \u0026#43; d) \u0026lt;\u0026lt; half) \u0026#43; c   So, this final scheme is in $\\mathcal{O}(n^{1.585}\\log{}n)$ time.\nHere is one final way of solving this problem in the same $\\mathcal{O}(n^{1.585}\\log{}n)$ time, but using a somewhat simpler scheme!\nIf we know \\(F_K\\)    and \\(F_{K\u0026#43;1}\\)   , then we can find,\n$$F_{2K} = F_K \\left [ 2F_{K\u0026#43;1}-F_K \\right ]$$     $$F_{2K\u0026#43;1} = {F_{K\u0026#43;1}}^2\u0026#43;{F_K}^2$$     We can implement this using the Karatsuba multiplication as follows:\ndef fibFast(n): if n \u0026lt;= 2: return 1 k = n // 2 a = fibFast(k \u0026#43; 1) b = fibFast(k) if n % 2 == 1: return multiply(a,a) \u0026#43; multiply(b,b) else: return multiply(b,(2*a - b))   That\u0026rsquo;s it for today. We saw how far algorithms can go in speed for such simple problems. Let me know in the comments below, if you have any faster or alternate algorithms in mind. Have fun, May zero be with you!\n","title":"Moore's Law and Algorithms - Case of Fibonacci Numbers","url":"https://sadanand-singh.github.io/posts/fibonaccinumbers/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":"In my last post on Arch Installation Guide , We installed the base system and we can now login into our new system as root using the password that we set.\n\n  \nNow, we will proceed further to install the Plasma 5 desktop.\nAdd New User Choose $USERNAME per your liking. I chose ssingh, so in future commands whenever you see ssingh please replace it with your $USERNAME.\n$ useradd -m -G wheel -s /bin/bash $USERNAME $ chfn --full-name \u0026#34;$FULL_NAME\u0026#34; $USERNAME $ passwd $USERNAME   Plasma 5 Desktop Network should be setup at the start. Check the status of network using:\n$ ping google.com -c 2 $ $ PING google.com (10.38.24.84) 56(84) bytes of data. $ 64 bytes from google.com (10.38.24.84): icmp_seq=1 ttl=64 time=0.022 ms $ 64 bytes from google.com (10.38.24.84): icmp_seq=2 ttl=64 time=0.023 ms $ $ --- google.com ping statistics --- $ 2 packets transmitted, 2 received, 0% packet loss, time 999ms $ rtt min/avg/max/mdev = 0.022/0.022/0.023/0.004 ms $   If you do not get this output, please follow the troubleshooting links at arch wiki on setting up network.\nI will be assuming you have an NVIDIA card for graphics installation.\nTo setup a graphical desktop, first we need to install some basic X related packages, and some essential packages (including fonts):\n$ pacman -S xorg-server xorg-server-utils nvidia nvidia-libgl   To avoid the possibility of forgetting to update your initramfs after an nvidia upgrade, you have to use a pacman hook like this:\n$ vim /etc/pacman.d/hooks/nvidia.hook $ ... [Trigger] Operation=Install Operation=Upgrade Operation=Remove Type=Package Target=nvidia [Action] Depends=mkinitcpio When=PostTransaction Exec=/usr/bin/mkinitcpio -p linux ... $   Nvidia has a daemon that is to be run at boot. To start the persistence daemon at boot, enable the nvidia-persistenced.service.\n$ systemctl enable nvidia-persistenced.service $ systemctl start nvidia-persistenced.service   \nKWIN FLICKERING ISSUE To avoid screen tearing in KDE (KWin), add following:\n$ vim /etc/profile.d/kwin.sh $ ... export __GL_YIELD=\u0026#34;USLEEP\u0026#34; ...   If this does not help please try adding the following instead -\n$ vim /etc/profile.d/kwin.sh $ ... export KWIN_TRIPLE_BUFFER=1 ...   Do not have both of the above enabled at the same time. Please look at Arch Wiki for additional details.   Now continue installing remaining important packages for the GUI.\n$ pacman -S mesa ttf-hack ttf-anonymous-pro $ pacman -S tlp tlp-rdw acpi_call bash-completion git meld $ pacman -S ttf-dejavu ttf-freefont ttf-liberation   Now, we will install the packages related to Plasma 5:\n$ pacman -S plasma-meta kf5 kdebase kdeutils kde-applications $ pacman -S kdegraphics gwenview   Now we have to setup a display manager. I chose recommended SDDM for plasma 5.\n$ pacman -S sddm sddm-kcm $ vim /etc/sddm.conf ... [Theme] # Current theme name Current=breeze # Cursor theme CursorTheme=breeze_cursors ... $ systemctl enable sddm   Also make sure that network manager starts at boot:\n$ systemctl disable dhcpcd.service $ systemctl enable NetworkManager   Audio Setup This is pretty simple. Install following packages and you should be done:\n$ pacman -S alsa-utils pulseaudio pulseaudio-alsa libcanberra-pulse $ pacman -S libcanberra-gstreamer jack2-dbus kmix $ pacman -S mpv mplayer   Useful Tips This part is optional and you can choose as per your taste. Sync time using the systemd service:\n$ vim /etc/systemd/timesyncd.conf $ ... [Time] NTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org FallbackNTP=0.pool.ntp.org 1.pool.ntp.org 0.fr.pool.ntp.org ... $ $ timedatectl set-ntp true $ timedatectl status $ ... Local time: Tue 2016-09-20 16:40:44 PDT Universal time: Tue 2016-09-20 23:40:44 UTC RTC time: Tue 2016-09-20 23:40:44 Time zone: US/Pacific (PDT, -0700) Network time on: yes NTP synchronized: yes RTC in local TZ: no ... $   On Plasma 5, It is recommended to enable no-bitmaps to improve the font rendering:\n$ sudo ln -s /etc/fonts/conf.avail/70-no-bitmaps.conf /etc/fonts/conf.d   If you use vim as your primary editor, you may find this vimrc quite useful.\nThat\u0026rsquo;s It. You are done. Start playing your new beautiful desktop. Please leave your comments with suggestions or any word of appreciation if this has been of any help to you.\nFollow this page for any additional suggestions or improvements in this guide.\n","title":"Plasma 5 Installation on Arch Linux","url":"https://sadanand-singh.github.io/posts/plasmainstall/"},{"tags":"Linux, Arch Linux, Plasma 5, KDE","text":"You must be thinking - yet another installation guide! There is no dearth of Installation guides of Arch on web. So why another one?\nWith advancements like BTRFS file system, UEFI motherboards and modern in-development desktop environment like Plasma 5; traditional Arch Wiki guide and Arch Beginners\u0026rsquo; Guide can only be of a limited help. After I got my new desktop, my goal was to setup it with a modern setup. I decided to go with Arch Linux with btrfs file system and Plasma 5 desktop. Coming from OSX, I just love how far linux has come in terms of looks - quite close to OSX!\n\n For all of you who love installation videos-\n  \nI will cover this in two parts. First in this post, I will install the base system. Then, in a follow up post, I will discuss details of setting up final working Plasma 5 desktop.\nInitial Setup Download the latest iso from Arch website and create the uefi usb installation media. I used my mac to do this on terminal:\n$ diskutil list $ diskutil unmountDisk /dev/disk1 $ dd if=image.iso of=/dev/rdisk1 bs=1m 20480\u0026#43;0 records in 20480\u0026#43;0 records out 167772160 bytes transferred in 220.016918 secs (762542 bytes/sec) $ diskutil eject /dev/disk1   Use this media to boot into your machine. You should boot into UEFI mode if you have a UEFI motherboard and UEFI mode enabled.\nTo verify you have booted in UEFU mode, run:\n$ efivar -l   This should give you a list of set UEFI variables. Please look at the Begineers\u0026rsquo; Guide in case you do not get any list of UEFI variables.\nEthernet/Wifi Ethernet should have started by default on your machine. If you do not plan to use wifi during installation, you can skip to the next section. If desired later, wifi will still be configurable after you are done with all the installation.\nTo setup wifi simply run:\n$ wifi-menu   This is a pretty straight forward tool and will setup wifi for you for this installation session.\nThis will also create a file at /etc/netctl/. We will use this file later to enable wifi at the first session after installation.\nSystem Updates For editing different configurations, I tend to use vim. So we will update our package cache and install vim.\n$ pacman -Syy $ pacman -S vim   Hard Drives In my desktop, I have three hard drives, one 256 GB solid state drive (SDD), one 1 TB HDD and another 3TB HDD. I set up my drives as follows: -SDD for root(/), /boot, and /home partitions, 1st HDD for /data and the 2nd HDD for /media partitions.\nFor UEFI machines, we need to use a GPT partition table and /boot partition has to be a fat32 partition with a minimum size of 512 MB. We will format rest other partitions with BTRFS. See this link for benefits of using btrfs partitions.\nFirst list your hard drives with the following:\n$ lsblk $ cat /proc/partitions   Assuming, my setup above, now create gpt partitions and format them.\n$ dd if=/dev/zero of=/dev/sda bs=1M count=5000 $ gdisk /dev/sda Found invalid MBR and corrupt GPT. What do you want to do? (Using the GPT MAY permit recovery of GPT data.) 1 - Use current GPT 2 - Create blank GPT   Then press 2 to create a blank GPT and start fresh\nZAP: $ press x - to go to extended menu $ press z - to zap $ press Y - to confirm $ press Y - to delete MBR   It might now kick us out of gdisk, so get back into it:\n$ gdisk /dev/sda $ Command (? for help): m $ Command (? for help): n $ Partition number (1-128, default 1): $ First sector (34-500118158, default = 2048) or {\u0026#43;-}size{KMGTP}: $ Last sector (2048-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: 512M $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): ef00 $ Changed type of partition to \u0026#39;EFI System\u0026#39; $ Partition number (2-128, default 2): $ First sector (34-500118, default = 16779264) or {\u0026#43;-}size{KMGTP}: $ Last sector (16779264-500118, default = 500118) or {\u0026#43;-}size{KMGTP}: $ Current type is \u0026#39;Linux filesystem\u0026#39; $ Hex code or GUID (L to show codes, Enter = 8300): $ Changed type of partition to \u0026#39;Linux filesystem\u0026#39; $ Command (? for help): p $ Press w to write to disk $ Press Y to confirm   Repeat the above procedure for /dev/sdb and /dev/sdc, but create just one partition with all values as default. At the end we will have three partitions: /dev/sda1, /dev/sda2, /dev/sdb1 and /dev/sdc1\nNow we will format these partitions.\n$ mkfs.vfat -F32 /dev/sda1 $ mkfs.btrfs -L arch /dev/sda2 $ mkfs.btrfs -L data /dev/sdb1 $ mkfs.btrfs -L media /dev/sdc1   Now, we will create btrfs subvolumes and mount them properly for installation and final setup.\n$ mount /dev/sda2 /mnt $ btrfs subvolume create /mnt/ROOT $ btrfs subvolume create /mnt/home $ umount /mnt $ mount /dev/sdb1 /mnt $ btrfs subvolume create /mnt/data $ umount /mnt $ mount /dev/sdc1 /mnt $ btrfs subvolume create /mnt/media $ umount /mnt   Now, once the sub-volumes have been created, we will mount them in appropriate locations with optimal flags.\n$SSD_MOUNTS=\u0026#34;rw,noatime,nodev,compress=lzo,ssd,discard, space_cache,autodefrag,inode_cache\u0026#34; $ HDD_MOUNTS=\u0026#34;rw,nosuid,nodev,relatime,space_cache\u0026#34; $ EFI_MOUNTS=\u0026#34;rw,noatime,discard,nodev,nosuid,noexec\u0026#34; $ mount -o $SSD_MOUNTS,subvol=ROOT /dev/sda2 /mnt $ mkdir -p /mnt/home $ mkdir -p /mnt/data $ mkdir -p /mnt/media $ mount -o $SSD_MOUNTS,nosuid,subvol=home /dev/sda2 /mnt/home $ mount -o $HDD_MOUNTS,subvol=data /dev/sdb1 /mnt/data $ mount -o $HDD_MOUNTS,subvol=media /dev/sdc1 /mnt/media $ mkdir -p /mnt/boot $ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot   Base Installation Now, we will do the actually installation of base packages.\n$ pacstrap /mnt base base-devel btrfs-progs $ genfstab -U -p /mnt \u0026gt;\u0026gt; /mnt/etc/fstab   Edit the /mnt/ect/fstab file to add following /tmp mounts.\ntmpfs /tmp tmpfs rw,nodev,nosuid 0 0 tmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0   Wifi at First Boot Copy our current wifi setup file into the new system. This will enable wifi at first boot. Next, chroot into our newly installed system: $cp /etc/netctl/wl* /mnt/etc/netctl/  \n  Finally bind root for installation.\n$ arch-chroot /mnt /bin/bash   Basic Setup Here are some basic commands you need to run to get the installation started.\n$ pacman -Syy $ pacman -S sudo vim $ vim /etc/locale.gen ... # en_SG ISO-8859-1 en_US.UTF-8 UTF-8 # en_US ISO-8859-1 ... $ locale-gen $ echo LANG=en_US.UTF-8 \u0026gt; /etc/locale.conf $ export LANG=en_US.UTF-8 $ ls -l /usr/share/zoneinfo $ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime $ hwclock --systohc --utc $ sed -i \u0026#34;s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\u0026#34; /etc/sudoers $ HOSTNAME=euler $ echo $HOSTNAME \u0026gt; /etc/hostname $ pacman -S dosfstools efibootmgr $ sed -i \u0026#39;s/^\\(HOOKS=.*fsck\\)\\(.*$\\)/\\1 btrfs\\2/g\u0026#39; /etc/mkinitcpio.conf $ mkinitcpio -p linux $ passwd   Wifi Packages We also need to install following packages for wifi to work at first boot:\n$ pacman -S iw wpa_supplicant     We will also add hostname to our /etc/hosts file:\n$ vim /etc/hosts ... 127.0.0.1 localhost.localdomain localhost $HOSTNAME ::1 localhost.localdomain localhost $HOSTNAME ...   Bootloader Setup systemd-boot, previously called gummiboot, is a simple UEFI boot manager which executes configured EFI images. The default entry is selected by a configured pattern (glob) or an on-screen menu. It is included with the systemd, which is installed on an Arch systems by default.\nAssuming /boot is your boot drive, first run the following command to get started:\n$ bootctl --path=/boot install   It will copy the systemd-boot binary to your EFI System Partition ( /boot/EFI/systemd/systemd-bootx64.efi and /boot/EFI/Boot/BOOTX64.EFI - both of which are identical - on x64 systems ) and add systemd-boot itself as the default EFI application (default boot entry) loaded by the EFI Boot Manager.\nFinally to configure out boot loader, we will need the UUID of out root drive (/dev/sda2). You can find that by:\n$ lsblk -no NAME,UUID /dev/sda2   Now, make sure that the following two files look as follows, where $UUID is the value obtained from above command:\n$ vim /boot/loader/loader.conf ... timeout 3 default arch ... $ vim /boot/loader/entries/arch.conf ... title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options root=UUID=$UUID rw rootfstype=btrfs rootflags=subvol=ROOT ...   IMPORTANT Please note that you will to need manually run bootctl command every time systemd-boot gets updated.\n$ bootctl update     Network Setup First setup hostname using systemd:\n$ hostnamectl set-hostname $HOSTNAME   Check the \u0026ldquo;Ethernet controller\u0026rdquo; entry (or similar) from the lspci -v output. It should tell you which kernel module contains the driver for your network device. For example:\n$ lspci -v $ ... 04:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8111/8168/8411 PCI Express Gigabit Ethernet Controller (rev 11) Subsystem: ASUSTeK Computer Inc. Device 859e Flags: bus master, fast devsel, latency 0, IRQ 29 I/O ports at d000 [size=256] Memory at f7100000 (64-bit, non-prefetchable) [size=4K] Memory at f2100000 (64-bit, prefetchable) [size=16K] Capabilities: \u0026lt;access denied\u0026gt; Kernel driver in use: r8169 Kernel modules: r8169 ... $   Next, check that the driver was loaded via dmesg | grep module_name. For example:\n$ dmesg | grep r8169 $ ... [ 3.215178] r8169 Gigabit Ethernet driver 2.3LK-NAPI loaded [ 3.215185] r8169 0000:04:00.0: can\u0026#39;t disable ASPM; OS doesn\u0026#39;t have ASPM control [ 3.220477] r8169 0000:04:00.0 eth0: RTL8168g/8111g at 0xffffc90000c74000, 78:24:af:d7:1d:3d, XID 0c000800 IRQ 29 [ 3.220481] r8169 0000:04:00.0 eth0: jumbo features [frames: 9200 bytes, tx checksumming: ko] [ 3.226949] r8169 0000:04:00.0 enp4s0: renamed from eth0 [ 5.128713] r8169 0000:04:00.0 enp4s0: link down [ 5.128713] r8169 0000:04:00.0 enp4s0: link down [ 8.110869] r8169 0000:04:00.0 enp4s0: link up ... $   Proceed if the driver was loaded successfully. Otherwise, you will need to know which module is needed for your particular model. Please follow the Arch Wiki Networking guide for further assistance.\nGet current device names via /sys/class/net or ip link. For example:\n$ ls /sys/class/net $ ... enp4s0 lo wlp3s0 ... $ $ ip link $ ... 2: enp4s0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 link/ether 78:24:af:d7:1d:3d brd ff:ff:ff:ff:ff:ff ... $   Using this name of the device, we need to configure, enable following two systemd services: systemd-networkd.service and systemd-resolved.service.\nFor compatibility with resolv.conf, delete or rename the existing file and create the following symbolic link:\n$ ln -s /usr/lib/systemd/resolv.conf /etc/resolv.conf   Network configurations are stored as *.network in /etc/systemd/network. We need to create ours as follows.:\n$ vim /etc/systemd/network/wired.network $ ... [Match] Name=enp4s0 [Network] DHCP=ipv4 ... $   Now enable these services:\n$ systemctl enable systemd-resolved.service $ systemctl enable systemd-networkd.service   Your network should be ready for first use!\nFirst Boot Now we are ready for the first boot! Run the following command:\n$ exit $ umount -R /mnt $ reboot   Awesome! We are ready to play with our new system. Alas! what you have is just a basic installation without any GUI.\nPlease see my next post for where to go next!\n","title":"Arch Installation Guide","url":"https://sadanand-singh.github.io/posts/archinstall/"},{"tags":"Algorithms, Puzzles","text":"Here are two math puzzles, solve, comment and enjoy the discussion!\n\n Puzzle 1: Prime Numbers Prove that $p^2-1$ is divisible by 24, where $p$ is a prime number with $p\u0026gt;3$.\nThis is a simple one - do not go by the technicality of the problem statement.\nSolution $p^2-1 = (p-1)\\times (p+1)$ Given, $p$ is a prime number $\u0026gt;3$, $p-1$ and $p+1$ are even. We can also write,\n$$p-1=2K, \\text{and } p+1=2K+2=2(K+1)$$\nGiven, $K \\in \\mathbb{N}$, either $K$ or $K+1$ are also even. Hence, $(p-1)\\times (p+1)$ is divisible by $2\\times 4 = 8$.\nFurthermore, as $p$ is prime, we can write it as either $p = 3s+1$ or $p = 3s-1$, where $s \\in \\mathbb{N}$. In either case, one of $p-1$ or $p+1$ are divisible by 3 as well.\nHence, $p^2-1$ is divisible by $8\\times 3 = 24$.\nPuzzle 2: Hopping on a Chess Board On a chess board, basically a $8\\times 8$ grid, how many ways one can go from the left most bottom corner to the right most upper corner? In chess naming conventions, from $a1$ to $h8$.\nNOTE In the original post this problem was ill defined.\nPlease solve this problem with the constraints that only up and right moves are allowed.    Can you find the generic answer for the case of $N\\times M$ grid?\nSolution Correction For an $N\\times M$ grid, we need only $N-1$ right and $M-1$ up moves.\nThank you Devin for pointing this out.   Given only forward moves are allowed, for any arbitrary grid of $N\\times M$, a total of $(N-1) + (M-1)$ moves are needed.\nAny $N-1$ of these moves can be of type right and $M-1$ of type up. In short, this is a combinatorial problem of distributing $N+M-2$ objects into groups of $N-1$ and $M-1$. This is simply,\n$$\\dbinom{N+M-2}{N-1} = \\frac{(N+M-2)!}{(N-1)! (M-1)!}$$\nIn the particular case of the chess board, $N = M = 8$. Hence, total number of possible paths are:\n$$\\text{No. of Paths} = \\frac{14!}{7! 7!} =3432$$\nThank you Rohit and Amber for posting quick solutions!\nLet me know if you have any other interesting alternative solutions to these problems.\n","title":"Two Simple Math Puzzles","url":"https://sadanand-singh.github.io/posts/primenumberandpath/"},{"tags":"Food, Recipe","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule.\nYes, the silence is finally over, and what\u0026rsquo;s better than giving your taste buds some rejuvenation after some long busy weeks at work. Continuing with my healthy but tasty choices, today I tried to cook Palak (Spinach) with Paneer.\n\nPalak Paneer is a common north Indian cuisine, Indian cottage cheese cooked in spinach puree. Its a bit involved than my last few dishes. Nevertheless, I promise you - the \u0026ldquo;yummy-ness\u0026rdquo; of this one is worth all the trouble!\n  Ingredients  1 lb Paneer (Indian Cottage Cheese) 1 Bunch Fresh Spinach Leaves 1\u0026frasl;2 Medium Onion Finely Cut 1\u0026frasl;2 inch Ginger Root Grated 1 Medium Tomato Finely Chopped 1 Clove of Garlic Minced 1 Medium Bay Leaf 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;4 tbsp Coriander Powder 1 tbsp Cumin Seeds 1 tbsp Garam Masala 2 pinch Asafoetida Heeng 2 tbsp Olive Oil Salt to taste 1 cup Milk or Cream   Preparation Spinach Puree  Wash the leaves thoroughly in running water. In a deep pot, bring about 6 cup of water to boiling temperature. Add a pinch of salt and 2 pinch asafoetida (Heeng) into the boiling water. Add Spinach leaves in boiling water and let it cook for 3 minutes. Strain boiled spinach leaves under cold water. In a blender, make a puree of leaves along with ginger, garlic and milk or cream.   Curry for Paneer  Cut Paneer into small cubes. Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add cumin seeds, bay leaf and minced garlic. Once cumin seeds start to pop, add finely cut onions. Fry the onions until oil starts to separate (about 1-2 minutes). Add finely chopped tomatoes and all the spices except Garam Masala. Once oil start to separate out, add the spinach puree. Stir well while cooking. Add salt and Garam Masala after about 2 minutes. After about another 2 minutes add Paneer cubes. Cook for another 3-4 minutes on low heat.  Let it cool before serving. It can be served with Indian Breads or rice.\n Interesting Facts  Nutritional Value of Paneer. More about Paneer Nutritional Value of Spinach. ","title":"Palak Paneer Recipe","url":"https://sadanand-singh.github.io/posts/palakpaneerrecipe/"},{"tags":"Food, Recipe","text":"Typically, hunger and laziness come to me as inseparable couples. To make things worse, I have been trying to eat healthy.\nNevertheless, here is another recipe that solves all these at once. Its my five minute Tilapia Fish Recipe.\nTwo major ingredients - fish and veggies, of this recipe can be grabbed from the frozen section of any supermarket.\n\n  Ingredients  2 frozen Boneless Tilapia Fish Pieces 1 Packet Frozen Steamed Veggies 2 tbsp Cooking Oil 1\u0026frasl;2 tbsp Parsley 1 tbsp Gamram Masala 1 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1 tbsp Red Chili Powder 1\u0026frasl;2 Lime Salt to Taste  Preparation  Thaw and wash fish pieces carefully. In a skillet, put a tbsp of oil and add frozen veggies to it. Add 1\u0026frasl;2 of all the spices and salt to it. Fry for about 3-5 minutes and keep it aside on a plate. In the same skillet, add rest of the oil. After about a minute, add fish pieces on medium heat. Sprinkle salt and half of the remaining spices on the top of the fish. After about 3 minutes, turn the fish and sprinkle rest of the spices. Let the fish cook slowly. Sprinkle some lime juice on the fish. Once the fish is of mustard color, put fish pieces on veggies.  Serve and garnish it with lime juice and some cilantro for an exotic lunch/dinner.\n Interesting Facts  More about Tilapia Fish Nutritional Value of Tilapia Fish ","title":"Tilapia Fish Recipe","url":"https://sadanand-singh.github.io/posts/tilapiafish/"},{"tags":"Food, Recipe","text":"Today, I share one of my favorite dishes - Aloo Paratha. It is a dish of mashed potato stuffed bread from the Northern India. It is my favorite breakfast dish when I am in India.\nHere is my try in making some edible parathas. It might take few trials in making a perfectly round-shaped Paratha. Please share your views and inputs in comments below.\n\n  Ingredients For Stuffing  2 medium Potatoes 2 tbsp Cooking Oil 1\u0026frasl;2 tbsp Coriander Powder 1\u0026frasl;2 tbsp Gamram Masala 1\u0026frasl;2 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1 tbsp Red Chili Flakes 1\u0026frasl;2 Medium Onion Salt to Taste   For Bread  4 cups of Whole Wheat Flour 4-5 tbsp Butter or Ghee 1 tbsp Black Indian Onion Seeds (Kalaunji) 1 tbsp Ajwain (Carom Seeds) Salt to taste  Preparation Stuffing  Boil and peel Potatoes. Mash boiled Potatoes and mix salt and chili flakes. In a skillet heat oil and add Garam Masala. Once it start to sputter, add cut onions. Add rest of spices and fry until golden brown. Add mashed Potatoes mix and fry for another 2-3 minutes.  Bread (Paratha)  Knead flour with Ajwain, Kalaunji and salt. Make small balls Make craters in the balls and fill them with the stuffing. Roll these into flat circular breads, typical Indian Bread shape. Heat these slowly on a skillet on each side. Once almost cooked, apply some butter or Ghee.  Let it cool before serving. It can be served with Indian spicy pickle and yogurt (Dahi).\n Interesting Facts  More about Aloo Paratha More about Ajwain ","title":"Aloo Paratha Recipe","url":"https://sadanand-singh.github.io/posts/alooparatha/"},{"tags":"Food, Recipe","text":"I am quite found of Paneer. However, cooking it can be a hassle.\nYou can get paneer generally at any Indian grocery store. For enthusiasts, Here is a recipe for making Paneer from milk.\nHere is a version of recipe that I use quite often. It involves two parts - Baking Paneer and Cooking the veggies.\n\n  Ingredients For Baking Paneer  400 g Paneer 1 tbsp Coriander Powder 1 tbsp Gamram Masala 1\u0026frasl;2 tbsp Cumin Powder 1\u0026frasl;2 tbsp Turmeric Powder 1\u0026frasl;2 tbsp Garlic Powder 1\u0026frasl;4 cup Milk or Yogurt Salt to Taste  For Veggies  500 g Cut Cauliflowers 1 cup Peas 1\u0026frasl;2 Medium Onion 1 Medium Tomato 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste  Preparation Bake Paneer  Thaw Paneer and cut in cubes. Preheat the oven at 400 F Mix Paneer with all the spices and milk/yogurt. Place cubes in a baking sheet and bake it at 400F for about 30 minutes until Paneer is well cooked.  Prepare Veggies  Cut onion, tomatoes and cauliflowers. Heat oil in the pan and add cumin seeds. Once Seeds start to pop, add cut onions. Fry the onions until oil starts to separate. (about 1-2 minutes) Add spices, peas and tomatoes. (about 1-2 minutes) After about a minute, add cauliflowers. After another minute or so, add salt, and keep stirring occasionally. Once the cauliflowers are almost cooked, add baked Paneer pieces (about 3-5 minutes). Add a little amount of water and cook for about a minute. (optional) Garnish with cilantro leaves.  Let it cool before serving. It can be served either Indian breads or rice.\n Interesting Facts  Nutritional Value of Paneer. Baked Paneer in this recipe is also referred as Paneer Tikka More about Paneer ","title":"Mix-Veg Paneer Recipe","url":"https://sadanand-singh.github.io/posts/mixedvegpaneer/"},{"tags":"Linux","text":"It has been long due. Just built a new desktop. Here are different parts I used to build this beauty. (Updated Cost)\n\n i7 4790 3.6 GHz (Haswell) - Can\u0026rsquo;t Reveal - (Market Price $300) Asus Micro MATX B85-G R2.0 Intel LGA1150 - $50 ADATA XPG V1.0 DDR3 1866 2x4 GB RAM - $50 OCZ Vertex 460A Series 2.5\u0026rdquo; 240 GB - $60 WD Blue 1TB 3.5\u0026rdquo; 7200 RPM, 64MB Cache - $45 Ultra LSP V2 650 Watt PSU - $30 Ultra XBlaster Pro U12-42350 Case - $15 Asus BW-12B1ST/BLK/G/AS Blue Ray Burner - $20 Das Keyboard V4 Evoluent Ergo Right Mouse  Total Value - $580 (Excluding Keyboard/Mouse)\nIt is up and running Arch Linux with BTFRS file system, Plasma 5 Desktop and so on. Over the next few posts, I will be posting my installation ordeals.\nDo not forget to add your comments below. Please add your alternative solutions to anything I did.\n","title":"My New Desktop","url":"https://sadanand-singh.github.io/posts/mynewcompspecs/"},{"tags":"Food, Recipe","text":"This is for all the lazy souls like me - in a mood to eat something tasty, but in no mood to cook for long.\nVermicelli, or also known as seviyan in Hindi, is commonly cooked as a sweet dish in Indian subcontinent. As I try to be away from all things sweet, I came across this recipe which uses this in a quite spicy flavor.\n\nSo, here is my super quick and tasty recipe for these noodles. It takes less than 12 minutes ( 4 Minutes Preparation + 8 Minutes Cooking).\n  Ingredients  300 g Vermicelli Seviyan 1 cup Peas 1\u0026frasl;2 Medium Onion 1 Medium Tomato 1 Clove of Garlic 4 Curry Leaves 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste 1\u0026frasl;2 cup Peanuts optional  Preparation  Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add garlic and cut onions. Fry the onions until oil starts to separate (about 1-2 minutes) Add spices, curry leaves, peas and tomatoes. After about a minute, add noodles. After another minute or so, add salt, and keep stirring occasionally. Once the noodles have become light golden brown in color, add 1\u0026frasl;2 cup of water (about 2-3 minutes). Keep stirring, so that noodles do not stick with each other, for about another 2 minutes  Let it cool before serving. You can use some ketchup or sauce with it. I like adding peanuts to it while frying onions.\n Interesting Facts  The sweet dish seviyan made out of this is also known as shemai in Bengali, sev in Gujarati, shavige in Kannada, sevalu or semiya in Telugu, and semiya in Tamil and Malayalam. The recipe above is also commonly known as upma in various parts of India. More about Vermicelli ","title":"Indian Vermicelli Recipe","url":"https://sadanand-singh.github.io/posts/desinoodlesrecipe/"},{"tags":"Algorithms, Python","text":"Given an alphanumeric string, find the shortest substring that occurs exactly once as a (contiguous) substring in it. Overlapping occurrences are counted as distinct. If there are several candidates of the same length, you must output all of them in the order of occurrence. The space is NOT considered as a valid non-repeating substring.\n\nExample Consider the following cases:\nCase 1: If the given string is asdfsasa, the answer should be ['d', 'f']\nCase 2: If the given string is sadanands,\nthe answer should be ['sa', 'ad', 'da', 'na', 'nd', 'ds']\nCase 3: If the given string is wwwwwwww, the answer should be ['wwwwwwww']\n  My Solution Here is my solution in Python.\nIt is quite brute force. I am not sure about the order of find() and rfind() built-in methods in Python. Assuming these are $O(n)$, my algorithm is in $O(n^3)$. Please put your answers in comments below, if your answer has a better scaling.\nThe function definition that I use for finding non-empty non-repeating strings is recursive.\ndef findNsubString(s,n): subS = [] for index in range(len(s)\u0026#43;1) : x = s[index:index\u0026#43;n] if s.find(x)==s.rfind(x) : subS.append(x) if subS : return subS else : return findNsubString(s,n\u0026#43;1)   I call this method as follows to get the desired results:\n#! /usr/bin/python import argparse # Parse Command Line Arguments parser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-s\u0026#34;, \u0026#34;--string\u0026#34;, default = \u0026#34;asda\u0026#34;, help=\u0026#34;Input\u0026#34;) args = parser.parse_args() s = args.string # Call Method to find smallest non-repeating sub-string ans = findNsubString(s,1) print(ans)   A similar solution can also be written in JAVA or C++. The corresponding find() and rfind() methods in JAVA are called indexOf() and lastIndexOf(), respectively. In C++, these methods are called same as in Python.\nPlease feel free to put your method definition in any programming language.\n","title":"Shortest Non-repeating Substring","url":"https://sadanand-singh.github.io/posts/shortestsubstring/"},{"tags":"Food, Recipe","text":"I have started eating a lot of greens these days. As a kid, I always loved a vegetable made by my mom, which was made of red leaves, called \u0026ldquo;Laal Saag\u0026rdquo; in Hindi. I could never find what exactly was the English/American name for those leaves.\nRecently, at one of the Korean grocery stores, I came across red Amaranth leaves. Those looked strikingly similar to the red leaves that I used to have back home in India.\n\nSo, here is my super fast and healthy recipe for these red leaves. It took me just 10 minutes ( 5 Minutes Preparation + 5 Minutes Cooking).\n  Ingredients  1 Bunch Red Amaranth Leaves 1\u0026frasl;2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Garam Masala 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste  Preparation  Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add cut leaves into the pan. Add salt, close the lid and keep stirring occasionally. Once the leaves have lost water, add all the spices and keep frying until things are dry (about 2-3 minutes).  You can serve this with any Indian bread, or with rice and lentils (Daal).\n Interesting Facts  Amaranth is also known as \u0026ldquo;Chauli\u0026rdquo; or \u0026ldquo;Chavli\u0026rdquo; in Hindi. Nutritional Value of Amaranth History of Amaranth ","title":"Lal Saag Recipe","url":"https://sadanand-singh.github.io/posts/laalsaagrecipe/"},{"tags":"Puzzles, Algebra","text":"Here is another puzzle starring a monkey, transportation and money! Short summary - avoid dealing with fools!\n\nProblem Statement\nThe owner of an apple plantation has a monkey. He wants to transport his 10000 apples to the market, which is located after the forest. The distance between his apple plantation and the market is about 1000 kilometer. So he decided to take his monkey to carry the apples. The monkey can carry at the maximum of 2000 apples at a time, and it eats one apple and throws another one for every kilometer it travels.\nWhat is the largest number of apples that can be delivered to the market?\nPlease give your solutions in the comments below.\nSolution\nIf the owner lets the monkey carry apples all the way to the end of the forest, in the first trip itself, all of 2000 apples will be lost and the monkey will never return back, as the monkey will be out of any food and play.\nLets approach this in a different way. Lets break the monkey\u0026rsquo;s journey by per unit distance. To carry 10000 apples for 1 km, monkey has to make 2 x 5 - 1 = 9 trips! On each trip, the owner will loose 2 apples. Hence, for each km distance traveled until number of apples is greater than 8000, monkey requires 9 x 2 = 18 apples. So, distance traveled by monkey until number of apples is less than 8000 is int(2000 / 18) + 1 = 112 km, and by this time, 10,000 - 112 x 18 = 7984 apples are left. Similarly, until 6000 apples are left, the monkey will require 2 x 4 -1 = 7 trips for every km. Hence, the total distance traveled until no. of apples left is less than 6000 is 112 + int(2000 / 14) + 1 = 255 km, and by this time no. of apples left is 7984 - 143 x 14 = 5982. Now proceeding in a similar manner, until 4000 apples are left, the monkey will require 5 trips for each km. Total distance traveled until no. of apples left is less than 4000 is 255 + int(2000\u0026frasl;10) = 455 km. By this time, number of apples left is 5982 - 200 x 10 = 3982. Until 2000 apples are left, the monkey will require 3 trips per km traveled. Total distance traveled till this time is 455 + int(2000\u0026frasl;6) + 1 = 789 km. By this time, no. of apples left is 3982 - 334 x 6 = 1978. Below 2000 apples, the monkey will require only one trip to reach to market. Distance left is 1000 - 789 = 211 km. Number of apples required by the monkey to travel this distance is 211 x 2 = 422. Hence, total number of apples that can reach the market is just 1978 - 422 = 1556! That\u0026rsquo;s just 15.56% of all apples.\nThat\u0026rsquo;s quite bad monkey 🙈. Moral of the story is, avoid dealing with fools 😛\nThank you all who tried this.\nDisclaimer: This puzzle has been inspired by a problem at the Blog on Technical Interviews.\n","title":"Puzzle 2","url":"https://sadanand-singh.github.io/posts/consumetransportproblem/"},{"tags":"Puzzles, Algebra","text":"As promised in the intro post, here is the first puzzle!\n\nProblem Statement\nA man needs to go through a train tunnel to reach the other side. He starts running through the tunnel in an effort to reach his destination as soon as possible. When he is 1/4th of the way through the tunnel, he hears the train whistle behind him. Assuming the tunnel is not big enough for him and the train, he has to get out of the tunnel in order to survive. We know that the following conditions are true.\n If he runs back, he will make it out of the tunnel by a whisker. If he continues running forward, he will still make it out through the other end by a whisker.  What is the speed of the train compared to that of the man?\nPlease give your solutions in the comments below.\nSolution\nIf the man decided to go back, he would have met the train at the entrance of tunnel (i.e The man would have traveled 0.25 of the tunnel). Instead, if man went forward- by the time train would have reached at the entrance of tunnel, the man would have been at the 0.25+0.25 = 0.5 of tunnel. Hence, according to the second condition, the time taken by man to travel half of tunnel is same as the time taken by the train to travel all of the tunnel. Therefore, the train is traveling at twice the speed of the man.\nDisclaimer: This puzzle has been copied from a Blog on Technical Interviews.\n","title":"Puzzle 1","url":"https://sadanand-singh.github.io/posts/trainspeedproblem/"},{"tags":"Introduction","text":"This is Sadanand Singh. I am a process engineer, a physicist, a programmer, an Indian and a human being; with interests in world politics, economics, and society.\n  Tweets by sadanandsingh  \n  This space is for my personal notes on different subjects. I plan to share my thoughts on following topics from time to time.\n Technology Statistics Machine Learning News Economics Education Politics, Society \u0026amp; Education Indian Food Puzzles  If you have interests in any of these topics, you are welcome to have a peek into my world. Please drop me your views through comments or any of the social media links.\nHAPPY WEB SURFING!!!\n","title":"Welcome","url":"https://sadanand-singh.github.io/posts/firstpost/"}]}
