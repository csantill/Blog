{"pages":[{"url":"http://sadanand-singh.github.io/mptex","text":"/*! * * IPython notebook * */.ansibold{font-weight:700}.ansiblack{}.ansired{color:#8b0000}.ansigreen{6400}.ansiyellow{color:#c4a000}.ansiblue{8b}.ansipurple{color:#9400d3}.ansicyan{color:#4682b4}.ansigray{color:gray}.ansibgblack{background-}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:#ff0}.ansibgblue{background-f}.ansibgpurple{background-color:#ff00ff}.ansibgcyan{background-ff}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:0}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:700;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a,div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){.prompt{text-align:left}div.unrecognized_cell>div.prompt{display:none}}div.code_cell{}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:0 0}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base,.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#<span class=\"caps\">BA2121</span>}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88F}.highlight-keyword{color:green;font-weight:700}.highlight-builtin{color:green}.highlight-error{color:red}.highlight-operator{color:#<span class=\"caps\">A2F</span>;font-weight:700}.highlight-meta{color:#<span class=\"caps\">A2F</span>}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{f}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{color:green;font-weight:700}.cm-s-ipython span.cm-atom{color:#88F}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#<span class=\"caps\">A2F</span>;font-weight:700}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#<span class=\"caps\">BA2121</span>}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#<span class=\"caps\">A2F</span>}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{color:green}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{f}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:red}.cm-s-ipython span.cm-tab{background:url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=')right no-repeat}div.output_wrapper{display:-webkit-box;-webkit-box-align:stretch;display:-moz-box;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;z-index:1}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,.8);box-shadow:inset 0 2px 8px rgba(0,0,0,.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,.5)}div.output_prompt{color:#8b0000}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left!important}div.output_area div.output_area img,div.output_area svg{max-width:100%;height:auto}div.output_area img.unconfined,div.output_area svg.unconfined{max-width:none}.output{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;background-color:transparent;border-radius:0}div.output_subarea{overflow-x:auto;padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1;max-width:calc(100% - 14ex)}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:#8b0000}div.raw_input_container{font-family:monospace;padding-top:5px}span.raw_input_prompt{}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:700;color:red}div.output_unrecognized a,div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link,.rendered_html :visited,.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child,.rendered_html h5:first-child,.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ol,.rendered_html *+ul{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:0;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.rendered .text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:700;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5,.cm-header-6{font-size:100%;font-style:italic} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #<span class=\"caps\">FF0000</span> } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #<span class=\"caps\">BC7A00</span> } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #<span class=\"caps\">FF0000</span> } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #<span class=\"caps\">0044DD</span> } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #<span class=\"caps\">BA2121</span> } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #<span class=\"caps\">0000FF</span>; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #<span class=\"caps\">AA22FF</span> } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #<span class=\"caps\">D2413A</span>; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #<span class=\"caps\">0000FF</span> } /* Name.Function */ .highlight .nl { color: #<span class=\"caps\">A0A000</span> } /* Name.Label */ .highlight .nn { color: #<span class=\"caps\">0000FF</span>; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #<span class=\"caps\">AA22FF</span>; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Backtick */ .highlight .sc { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Char */ .highlight .sd { color: #<span class=\"caps\">BA2121</span>; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Double */ .highlight .se { color: #<span class=\"caps\">BB6622</span>; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Heredoc */ .highlight .si { color: #<span class=\"caps\">BB6688</span>; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #<span class=\"caps\">BB6688</span> } /* Literal.String.Regex */ .highlight .s1 { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ In [3]: % matplotlib inline import matplotlib import numpy as np import matplotlib.pyplot as plt In [2]: N = 5 menMeans = ( 20 , 35 , 30 , 35 , 27 ) menStd = ( 2 , 3 , 4 , 1 , 2 ) ind = np . arange ( N ) # the x locations for the groups width = 0.35 # the width of the bars fig , ax = plt . subplots () rects1 = ax . bar ( ind , menMeans , width , color = 'r' , yerr = menStd ) womenMeans = ( 25 , 32 , 34 , 20 , 25 ) womenStd = ( 3 , 5 , 2 , 3 , 3 ) rects2 = ax . bar ( ind + width , womenMeans , width , color = 'y' , yerr = womenStd ) # add some text for labels, title and axes ticks ax . set_ylabel ( 'Scores' ) ax . set_title ( 'Scores by group and gender' ) ax . set_xticks ( ind + width ) ax . set_xticklabels (( 'G1' , 'G2' , 'G3' , 'G4' , 'G5' )) ax . legend (( rects1 [ 0 ], rects2 [ 0 ]), ( 'Men' , 'Women' )) def autolabel ( rects ): # attach some text labels for rect in rects : height = rect . get_height () ax . text ( rect . get_x () + rect . get_width () / 2. , 1.05 * height , ' %d ' % int ( height ), ha = 'center' , va = 'bottom' ) autolabel ( rects1 ) autolabel ( rects2 ) plt . show () In [4]: from matplotlib.colors import BoundaryNorm from matplotlib.ticker import MaxNLocator # make these smaller to increase the resolution dx , dy = 0.05 , 0.05 # generate 2 2d grids for the x & y bounds y , x = np . mgrid [ slice ( 1 , 5 + dy , dy ), slice ( 1 , 5 + dx , dx )] z = np . sin ( x ) ** 10 + np . cos ( 10 + y * x ) * np . cos ( x ) # x and y are bounds, so z should be the value *inside* those bounds. # Therefore, remove the last value from the z array. z = z [: - 1 , : - 1 ] levels = MaxNLocator ( nbins = 15 ) . tick_values ( z . min (), z . max ()) # pick the desired colormap, sensible levels, and define a normalization # instance which takes data values and translates those into levels. cmap = plt . get_cmap ( 'PiYG' ) norm = BoundaryNorm ( levels , ncolors = cmap . N , clip = True ) fig , ( ax0 , ax1 ) = plt . subplots ( nrows = 2 ) im = ax0 . pcolormesh ( x , y , z , cmap = cmap , norm = norm ) fig . colorbar ( im , ax = ax0 ) ax0 . set_title ( 'pcolormesh with levels' ) # contours are *point* based plots, so convert our bound into point # centers cf = ax1 . contourf ( x [: - 1 , : - 1 ] + dx / 2. , y [: - 1 , : - 1 ] + dy / 2. , z , levels = levels , cmap = cmap ) fig . colorbar ( cf , ax = ax1 ) ax1 . set_title ( 'contourf with levels' ) # adjust spacing between subplots so `ax1` title and `ax0` tick labels # don't overlap fig . tight_layout () plt . show () In [5]: N = 150 r = 2 * np . random . rand ( N ) theta = 2 * np . pi * np . random . rand ( N ) area = 200 * r ** 2 * np . random . rand ( N ) colors = theta ax = plt . subplot ( 111 , projection = 'polar' ) c = plt . scatter ( theta , r , c = colors , s = area , cmap = plt . cm . hsv ) c . set_alpha ( 0.75 ) plt . show () In [6]: from mpl_toolkits.mplot3d import axes3d fig , [ ax1 , ax2 ] = plt . subplots ( 2 , 1 , figsize = ( 8 , 12 ), subplot_kw = { 'projection' : '3d' }) X , Y , Z = axes3d . get_test_data ( 0.05 ) ax1 . plot_wireframe ( X , Y , Z , rstride = 10 , cstride = 0 ) ax1 . set_title ( \"Column stride 0\" ) ax2 . plot_wireframe ( X , Y , Z , rstride = 0 , cstride = 10 ) ax2 . set_title ( \"Row stride 0\" ) plt . tight_layout () plt . show () In [ ]: if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"programming","title":"MatPlotLib Example"},{"url":"http://sadanand-singh.github.io/RplotsMultiVariables","text":"In this section, we will be re-using the data from the previous post based on Pseudo Facebook data from udacity . The data from the project corresponds to a typical data set at Facebook . You can load the data through the following command. Notice that this is a delimited tsv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below. pf <- read.csv ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\" , sep = '\\t' ) str ( pf ) ## 'data.frame': 99003 obs. of 15 variables: ## $ userid : int 2094382 1192601 2083884 1203168 1733186 1524765 1136133 1680361 1365174 1712567 ... ## $ age : int 14 14 14 14 14 14 13 13 13 13 ... ## $ dob_day : int 19 2 16 25 4 1 14 4 1 2 ... ## $ dob_year : int 1999 1999 1999 1999 1999 1999 2000 2000 2000 2000 ... ## $ dob_month : int 11 11 11 12 12 12 1 1 1 2 ... ## $ gender : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 1 2 2 ... ## $ tenure : int 266 6 13 93 82 15 12 0 81 171 ... ## $ friend_count : int 0 0 0 0 0 0 0 0 0 0 ... ## $ friendships_initiated: int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes_received : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes_received: int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes_received : int 0 0 0 0 0 0 0 0 0 0 ... library ( ggplot2 ) Restarting our analysis from the last blog, finding a relationship between age and friends counts, let us add gender to the equation. In order to do this, we will first use summarise() to get group by data. Notice the use of ungroup() to ungroup the data by the second level. The first call of summarize did the ungrouping by default on the first variable, i.e. age. We had to call ungroup() a second time to ungroup around the variable gender. library ( dplyr ) pf.fc_by_age_gender <- pf %>% filter ( ! is.na ( gender )) %>% group_by ( age , gender ) %>% summarise ( mean_friend_count = mean ( friend_count ), median_friend_count = median ( friend_count ), n = n ()) %>% ungroup () %>% arrange ( age ) Now, we can use ggplot to compare friend counts across age and gender. ggplot ( data = pf.fc_by_age_gender , aes ( x = age , y = median_friend_count )) + geom_line ( aes ( color = gender )) It would be helpful to analyze these differences in relative terms. So lets answer a different question - how many times more friends does the average female users have than average male users. To answer the above question we will need to transform our data. Right now, our data is in \"long format\" - a row of data different values of different variables. We will need to convert this to a \"wide format\" - where we will create columns called male and female, that will have median counts in them. This can be done using either \" reshape2 \" or \" tidyr \" package. Here we are using dcast() function (from the reshape2 package) as we need data-frame as output. If an array was required, we would have used acast() function. The second argument of this function is formulae that determines which variables to keep and which to compress. The ones on the left of the \"~\" are kept while ones on right are compressed. Multiple variables can be combined using a \"+\" symbol. library ( reshape2 ) pf.fc_by_age_gender_wide <- dcast ( pf.fc_by_age_gender , age ~ gender , value.var = 'median_friend_count' ) head ( pf.fc_by_age_gender_wide ) ## age female male ## 1 13 148.0 55.0 ## 2 14 224.0 92.5 ## 3 15 276.0 106.5 ## 4 16 258.5 136.0 ## 5 17 245.5 125.0 ## 6 18 243.0 122.0 Similarly, we can use the melt() function to convert a wide format data back to a long format data. The above conversion can also be done quite more elegantly using the tidyr package, as shown below. The tidyr package was created by the original authors of the reshape2 package. The main advantage of using the tidyr package that it plays well with the chaining syntax of the dplyr package. Furthermore, each function of the tidyr package does one and one thing only, hence it makes arranging the overall algorithm of cleaning data much robust. In the future version, we will be using tidyr package at most places. library ( tidyr ) pf.fc_by_age_gender_wide2 <- spread ( subset ( pf.fc_by_age_gender , select = c ( 'gender' , 'age' , 'median_friend_count' )), gender , median_friend_count ) head ( pf.fc_by_age_gender_wide2 ) ## Source: local data frame [6 x 3] ## ## age female male ## (int) (dbl) (dbl) ## 1 13 148.0 55.0 ## 2 14 224.0 92.5 ## 3 15 276.0 106.5 ## 4 16 258.5 136.0 ## 5 17 245.5 125.0 ## 6 18 243.0 122.0 rm ( pf.fc_by_age_gender_wide2 ) Now lets plot ratio of female to male median friend counts. ggplot ( data = pf.fc_by_age_gender_wide , aes ( x = age , y = female / male )) + geom_line () + geom_hline ( yintercept = 1 , alpha = 0.8 , linetype = 2 , colour = \"red\" , size = 1 ) In this particular case of looking at multiple variables, it would make more sense to look at count of friends as function of tenure of Facebook as well. People with a longer tenure of Facebook account are likely to accumulate a larger number of friends. In the following section we will look at friend count of females, males at different ages along with their Facebook tenure. pf <- within ( pf , year_joined <- floor ( 2014 - tenure / 365 )) Lets look at the summary of this new variable as well. summary ( pf $ year_joined ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 2005 2012 2012 2012 2013 2014 2 table ( pf $ year_joined ) ## ## 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 ## 9 15 581 1507 4557 5448 9860 33366 43588 70 The tabular view of this shows the distribution. We will next use the cut() method to bin this variable. pf $ year_joined.bucket <- cut ( pf $ year_joined , breaks = c ( 2004 , 2009 , 2011 , 2012 , 2014 )) table ( pf $ year_joined.bucket , useNA = \"ifany\" ) ## ## (2004,2009] (2009,2011] (2011,2012] (2012,2014] <NA> ## 6669 15308 33366 43658 2 Let us now plot all these variables together. In particular, we want to create a plot of friend counts vs age where a different line is shown for each bin of year joined. ggplot ( data = subset ( pf , ! is.na ( year_joined.bucket )), aes ( x = age , y = friend_count )) + geom_line ( aes ( color = year_joined.bucket ), stat = \"summary\" , fun.y = median ) Our initial hypothesis seems to be correct here - People with larger Facebook tenure tend to have higher friend counts. Lets us plot the mean of these bins and also the grand means of data. ggplot ( data = subset ( pf , ! is.na ( year_joined.bucket )), aes ( x = age , y = friend_count )) + geom_line ( aes ( color = year_joined.bucket ), stat = \"summary\" , fun.y = mean ) + geom_line ( stat = \"summary\" , fun.y = mean , linetype = 2 ) Since the trend holds up after conditioning on the each of the buckets of year joined, we can increase our confidence that this observation isn't just an artifact. Let us look at this from a different angle. We can define a variable called \"friend rate\", i.e. number of friends each person had on a per day basis. with ( subset ( pf , tenure >= 1 ), summary ( friend_count / tenure )) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.0775 0.2205 0.6096 0.5658 417.0000 We can now look at the effect of this variable in more detail using the following plot: ggplot ( data = subset ( pf , tenure >= 1 ), aes ( x = tenure , y = friendships_initiated / tenure )) + geom_smooth ( aes ( color = year_joined.bucket )) This shows that people initiate less number of friends as they have longer tenure at Facebook. Till now, we have looked at different aspects of the Facebook data set. We will now use a new data set for some more detailed multivariate analysis, and then finally come back to the Facebook data set for comparison. We are going to work with a data set describing household purchases of five flavors of Dannon Yogurt in 8 oz sizes. Their price is recorded with each purchase occasion. This yogurt data set has a quite different structure than our pseudo-Facebook data set. The synthetic Facebook data has one row per individual with that row giving their characteristics and counts of behaviors over a single period of time. On the other hand, the yogurt data has many rows per household, one for each purchase occasion. This kind of micro-data is often useful for answering different types of questions than we've looked at so far. We will start by loading the yogurt data set and then looking at its summary. yo <- read.csv ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/yogurt.csv\" ) str ( yo ) ## 'data.frame': 2380 obs. of 9 variables: ## $ obs : int 1 2 3 4 5 6 7 8 9 10 ... ## $ id : int 2100081 2100081 2100081 2100081 2100081 2100081 2100081 2100081 2100081 2100081 ... ## $ time : int 9678 9697 9825 9999 10015 10029 10036 10042 10083 10091 ... ## $ strawberry : int 0 0 0 0 1 1 0 0 0 0 ... ## $ blueberry : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pina.colada: int 0 0 0 0 1 2 0 0 0 0 ... ## $ plain : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mixed.berry: int 1 1 1 1 1 1 1 1 1 1 ... ## $ price : num 59 59 65 65 49 ... We notice that most of the variables are integers here. We should convert the id variable to factor. This will come handy later since the same household data is available in multiple rows. yo <- within ( yo , id <- as.factor ( id )) str ( yo ) ## 'data.frame': 2380 obs. of 9 variables: ## $ obs : int 1 2 3 4 5 6 7 8 9 10 ... ## $ id : Factor w/ 332 levels \"2100081\",\"2100370\",..: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : int 9678 9697 9825 9999 10015 10029 10036 10042 10083 10091 ... ## $ strawberry : int 0 0 0 0 1 1 0 0 0 0 ... ## $ blueberry : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pina.colada: int 0 0 0 0 1 2 0 0 0 0 ... ## $ plain : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mixed.berry: int 1 1 1 1 1 1 1 1 1 1 ... ## $ price : num 59 59 65 65 49 ... Let us look at the histogram of yogurt prices first. ggplot ( data = yo , aes ( x = price )) + geom_histogram () Now that we have some idea about distribution of prices, let's figure out on a given purchase occasion how many eight ounce yogurts does a household purchase. To answer this we need to combine counts of the different yogurt flavors into one variable. Then we can look at the histogram. yo <- within ( yo , all.purchases <- ( strawberry + blueberry + pina.colada + plain + mixed.berry )) ggplot ( data = yo , aes ( x = all.purchases )) + geom_histogram () It seems most household purchase one 8 oz yogurt at any given purchase. Now we will look at the scatter plot of prime vs time data. ggplot ( data = yo , aes ( x = time , y = price )) + geom_jitter ( alpha = 1 / 4 , shape = 21 , fill = I ( \"#F79420\" )) We see the baseline price of yogurt has been increasing over time. We also see some scattered prices around the baseline price, which could be simply due to usage of coupons, sales etc. by customers. When familiarizing yourself with a new data set that contains multiple observations of the same units, it's often useful to work with a sample of those units so that it's easy to display the raw data for that sample. In the case of the yogurt data set, we might want to look at a small sample of households in more detail so that we know what kind of within and between household variation we are working with. This analysis of a sub-sample might come before trying to use within household variation as part of a model. For example, this data set was originally used to model consumer preferences for variety. But, before doing that, we'd want to look at how often we observe households buying yogurt, how often they buy multiple items, and what prices they're buying yogurt at. One way to do this is to look at some sub-sample in more detail. Let's pick 16 households at random and take a closer look. set.seed ( 4230 ) sample.ids <- sample ( levels ( yo $ id ), 16 ) ggplot ( data = subset ( yo , id %in% sample.ids ), aes ( x = time , y = price )) + facet_wrap ( ~ id ) + geom_line () + geom_point ( aes ( size = all.purchases ), pch = 1 ) Notice the %in% syntax above. This helps us create a subset of all ids that belong to the set sample.ids chosen randomly. Also notice that I'm sampling from the levels because those are all of the different households that I have. The above ggplot command gives us a panel plot for each of the randomly selected household ids. We made this possible using the facet_wrap() command. From these plots, we can see the variation and how often each household buys yogurt. And it seems that some household purchases more quantities than others with the larger circles. For most of the households, the price of yogurt holds steady, or tends to increase over time. Now, there are, of course, some exceptions, like in household 2123463 and in 2141341 household, we might think that the household is using coupons to drive the price down. Now, we don't have the coupon data to associate with this buying data, but we can see how that information could be paired to this data to better understand the consumer behavior. The general idea is that if we have observations over time, we can facet by the primary unit, case, or individual in the data set. For our yogurt data it was the households we were faceting over. This faceted time series plot is something we can't generate with our pseudo Facebook data set. Since we don't have data on our sample of users over time. The Facebook data isn't great for examining the process of friending over time. The data set is just a cross section, it's just one snapshot at a fixed point that tells us the characteristics of individuals. Not the individuals over, say, a year. But if we had a dataset like the yogurt one, we would be able to track friendships initiated over time and compare that with tenure. This would give us better evidence to explain the difference or the drop in friendships initiated over time as tenure increases. Much of the analysis we've done so far focused on some pre-chosen variable, relationship or question of interest. We then used EDA to let those chosen variables speak and surprise us. Most recently, when analyzing the relationship between two variables we look to incorporate more variables in the analysis to improve it. For example, by seeing whether a particular relationship is consistent across values of those other variables. In choosing a third or fourth variable to plot we relied on our domain knowledge. But often, we might want visualizations or summaries to help us identify such auxiliary variables. In some analyses, we may plan to make use of a large number of variables. Perhaps, we are planning on predicting one variable with ten, 20, or hundreds of others. Or maybe we want to summarize a large set of variables into a smaller set of dimensions. Or perhaps, we're looking for interesting relationships among a large set of variables. In such cases, we can help speed up our exploratory data analysis by producing many plots or comparisons at once. This could be one way to let the data set as a whole speak in part by drawing our attention to variables we didn't have a preexisting interest in. We should let the data speak to determine variables of interest. There's a tool that we can use to create a number of scatter plots automatically. It's called a scatter plot matrix. In a scatter plot matrix. There's a grid of scatter plots between every pair of variables. As we've seen, scatter plots are great, but not necessarily suited for all types of variables. For example, categorical ones. So there are other types of visualizations that can be created instead of scatter plots. Like box plots or histograms when the variables are categorical. Let's produce the scatter plot matrix for our pseudo Facebook data set. We're going to use the GGally package to do so. library ( GGally ) # set the seed for reproducible results set.seed ( 1836 ) pf_subset <- pf [, c ( 2 : 8 )] names ( pf_subset ) ## [1] \"age\" \"dob_day\" \"dob_year\" \"dob_month\" ## [5] \"gender\" \"tenure\" \"friend_count\" ggpairs ( pf_subset [ sample.int ( nrow ( pf_subset ), 1000 ), ]) We will sample from the variable set from the pf data frame. However, some variables are not of interest to use, for example, \" useid \", \" year_joined \" or \" year_joined.bucket \". pf_subset <- pf [, c ( 9 : 15 )] names ( pf_subset ) ## [1] \"friendships_initiated\" \"likes\" \"likes_received\" ## [4] \"mobile_likes\" \"mobile_likes_received\" \"www_likes\" ## [7] \"www_likes_received\" ggpairs ( pf_subset [ sample.int ( nrow ( pf_subset ), 1000 ), ]) The GGpairs() function uses a different plot type for different types of combinations of variables. Hence, we have histograms here and we have scatter plots here. Many of these plots aren't quite as nice as they would be if we fine-tuned them for the particular variables. For example, for all the counts of likes, we might want to work on a logarithmic scale. But, GG Pairs doesn't do this for us. At the very least, a scatter plot matrix can be a useful starting point in many analyses. A matrix such as this one will be extremely helpful when we have even more variables than those in the pseudo-Facebook data set. Examples arise in many areas, but one that has attracted the attention of statisticians is genomic data. In these data sets, they're often thousands of genetic measurements for each of a small number of samples. In some cases, some of these samples have a disease, and so we'd like to identify genes that are associated with the disease. We will use an example data set of gene expression in tumors. The data contains the expression of 6,830 genes, compared with a larger baseline reference sample. nci <- read.table ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/nci.tsv\" ) #Change col names for nicer plots colnames ( nci ) <- c ( 1 : 64 ) We renamed the column names just for the ease of plotting and x labels. We will now make heat map plot for this data showing distribution of different genes in different tumors. # We can use the tidyr package instead of the reshape2 package #library(tidyr) # nci$gene <- rownames(nci) # nci.long.samp <- gather(nci[1:200,], case, value, 1:64) library ( reshape2 ) nci.long.samp <- melt ( as.matrix ( nci [ 1 : 200 ,])) names ( nci.long.samp ) <- c ( \"gene\" , \"case\" , \"value\" ) ggplot ( aes ( y = gene , x = case , fill = value ), data = nci.long.samp ) + geom_tile () + scale_y_discrete ( breaks = seq ( 1 , 200 , 19 )) + scale_x_discrete ( breaks = seq ( 1 , 64 , 7 )) + scale_fill_gradientn ( colors = colorRampPalette ( c ( \"blue\" , \"red\" ))( 100 )) Heat maps can also be a good way to look at distributions of large dimensional data. In summary in this post, we started with simple extensions to the scatter plot, and plots of conditional summaries that you worked with in lesson four, such as adding summaries for multiple groups. Then, we tried some techniques for examining a large number of variables at once, such as scatter-plot matrices and heat maps. We also learned how to reshape data, moving from broad data with one row per case, to aggregate data with one row per combination of variables, and we moved back and forth between long and wide formats for our data. In the next and the final post in this series, We will do an in-depth analysis of the US department of Education dataset on college education, highlighting the role of EDA .","tags":"R, Data Science, Statistics","title":"Pseudo Facebook Data - Exploring Many Variables"},{"url":"http://sadanand-singh.github.io/RplotsTwoVariables","text":"In this section, we will be re-using the data from the previous post based on Pseudo Facebook data from udacity . The data from the project corresponds to a typical data set at Facebook . You can load the data through the following command. Notice that this is a delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below. pf <- read.csv ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\" , sep = '\\t' ) str ( pf ) ## 'data.frame': 99003 obs. of 15 variables: ## $ userid : int 2094382 1192601 2083884 1203168 1733186 1524765 1136133 1680361 1365174 1712567 ... ## $ age : int 14 14 14 14 14 14 13 13 13 13 ... ## $ dob_day : int 19 2 16 25 4 1 14 4 1 2 ... ## $ dob_year : int 1999 1999 1999 1999 1999 1999 2000 2000 2000 2000 ... ## $ dob_month : int 11 11 11 12 12 12 1 1 1 2 ... ## $ gender : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 1 2 2 ... ## $ tenure : int 266 6 13 93 82 15 12 0 81 171 ... ## $ friend_count : int 0 0 0 0 0 0 0 0 0 0 ... ## $ friendships_initiated: int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes_received : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes_received: int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes_received : int 0 0 0 0 0 0 0 0 0 0 ... library ( ggplot2 ) Usually, it is best to use a scatter plot to analyze two variables: summary ( pf $ age ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 13.00 20.00 28.00 37.28 50.00 113.00 ggplot ( data = pf , aes ( x = age , y = friend_count )) + geom_point () + xlim ( 13 , 90 ) We can notice some really interesting behavior in the ugly scatter plot above: 1. The age data is binned, as expected (only integers allowed) 2. Young people around the age of 20 have maximum friend count. 3. There is unusual spike in friends count of people aged more than 100. This could mostly be some flaw in the data, probably based on incorrect entry by the users. 4. People around the age of 70 too have quite a large amount of friends. This is pretty interesting and could point to the use of the social media site by an unexpected group of people. We can use summary command to find the bounds on age and then use that to limit age axis. Furthermore, we notice at some areas of the plot being too dense, where as some to be really sparse. The areas where points are too dense is called \"over plotting\" - It is impossible to extract any meaningful statistics from this region. In order to overcome this, we can set the transparency of the plots using the alpha parameter in the geo_point(). Using a value of 1/20 means, one point that will plotted will be equal to 20 original points. Given age is a discrete variable we can add some noise to this using geom_jitter to get a better feel about its distribution. ggplot ( data = pf , aes ( x = age , y = friend_count )) + geom_jitter ( alpha = 1 / 20 , colour = \"red\" ) + xlim ( 13 , 90 ) Based on these new plots, we can find bulk to higher friend count for younger people is still less than 600. We still find higher count for age group of 70. We do a better representation of data using the coord_trans() method. We will be using a square root function. ggplot ( data = pf , aes ( x = age , y = friend_count )) + geom_jitter ( alpha = 1 / 20 , colour = \"cyan\" , position = position_jitter ( h = 0 )) + xlim ( 13 , 90 ) + coord_trans ( y = \"sqrt\" ) On a similar way, we can look at relationship between friends initiated and age. ggplot ( data = pf , aes ( x = age , y = friendships_initiated )) + geom_jitter ( alpha = 1 / 10 , colour = \"#ff33cc\" , position = position_jitter ( h = 0 )) + xlim ( 13 , 90 ) + coord_trans ( y = \"sqrt\" ) Interestingly, we find this distribution to be very similar to the one for friend count. Scatter plots try to keep us very close to the data. It represents each and evry data point. However, in order to judge the quality of a data, it is important to know its important statisics like mean, median etc. How does average of a variable wrt to the other variable. We want to say, study how does average friend count vary with age. In order to study this we will use a package called ‘dplyr'. First we want to group our data frame by age. Then, we can create a new data frame that lists friend count mean, median and frequency (n) by using the summarise() method. We can look a first few data of this new data frame using the head() method. We notice that by default, data may not be arranged by order of age. We can do that by explicitly calling the arrange() method. library ( dplyr ) age_groups <- group_by ( pf , age ) pf.fc_by_age <- summarise ( age_groups , friend_count_mean = mean ( friend_count ), friend_count_median = median ( friend_count ), n = n ()) pf.fc_by_age <- arrange ( pf.fc_by_age , age ) head ( pf.fc_by_age ) ## Source: local data frame [6 x 4] ## ## age friend_count_mean friend_count_median n ## (int) (dbl) (dbl) (int) ## 1 13 164.7500 74.0 484 ## 2 14 251.3901 132.0 1925 ## 3 15 347.6921 161.0 2618 ## 4 16 351.9371 171.5 3086 ## 5 17 350.3006 156.0 3283 ## 6 18 331.1663 162.0 5196 If you notice one detail about all dplyr method, every method needs one data frame and output needs to be saved to be used by next set of methods. All this can be done as a chain of commands using the following syntax: pf.fc_by_age <- pf %>% group_by ( age ) %>% summarise ( friend_count_mean = mean ( friend_count ), friend_count_median = median ( friend_count ), n = n ()) %>% arrange ( age ) head ( pf.fc_by_age , 20 ) ## Source: local data frame [20 x 4] ## ## age friend_count_mean friend_count_median n ## (int) (dbl) (dbl) (int) ## 1 13 164.7500 74.0 484 ## 2 14 251.3901 132.0 1925 ## 3 15 347.6921 161.0 2618 ## 4 16 351.9371 171.5 3086 ## 5 17 350.3006 156.0 3283 ## 6 18 331.1663 162.0 5196 ## 7 19 333.6921 157.0 4391 ## 8 20 283.4991 135.0 3769 ## 9 21 235.9412 121.0 3671 ## 10 22 211.3948 106.0 3032 ## 11 23 202.8426 93.0 4404 ## 12 24 185.7121 92.0 2827 ## 13 25 131.0211 62.0 3641 ## 14 26 144.0082 75.0 2815 ## 15 27 134.1473 72.0 2240 ## 16 28 125.8354 66.0 2364 ## 17 29 120.8182 66.0 1936 ## 18 30 115.2080 67.5 1716 ## 19 31 118.4599 63.0 1694 ## 20 32 114.2800 63.0 1443 Now, let us look at this new data frame visually. We can first look at the relationship between average friend count and age. ggplot ( data = pf.fc_by_age , aes ( x = age , y = friend_count_mean )) + geom_line () We can use this plot as good summary of the original scatter plot and put the two on top of each other. We can also transform the y axis using the coors_cartesian() method. ggplot ( data = pf , aes ( x = age , y = friend_count )) + geom_point ( alpha = 1 / 20 , colour = \"#00bfff\" , position = position_jitter ( h = 0 )) + coord_cartesian ( xlim = c ( 13 , 70 ), ylim = c ( 0 , 1000 )) + geom_line ( stat = 'summary' , fun.y = mean ) + geom_line ( stat = 'summary' , fun.y = quantile , fun.args = list ( probs = 0.1 ), linetype = 2 , color = \"blue\" ) + geom_line ( stat = 'summary' , fun.y = quantile , fun.args = list ( probs = 0.5 ), linetype = 1 , color = \"blue\" ) + geom_line ( stat = 'summary' , fun.y = quantile , fun.args = list ( probs = 0.9 ), linetype = 2 , color = \"#ff9900\" ) In the above plot, we can see that between the age group 30-69, 90% of population has less than 250 freinds. In stead of using 4 different summary measures to analyze the above data, we can use a single number! Often analysts will use correlation coefficients to summarize this. We will use the Pearson product moment correlation (r). You can look at the cor.test() method for details. This measures a linear correlation between two variables. with ( subset ( pf , age < 70 & age >= 13 ), cor.test ( age , friend_count , method = \"pearson\" )) ## ## Pearson's product-moment correlation ## ## data: age and friend_count ## t = -52.326, df = 90664, p-value < 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.1775257 -0.1648889 ## sample estimates: ## cor ## -0.1712144 We can also have other measures of relationship. For example, a measure of monotonic relationship would be done using spearman coefficient. Similarly, a measure of strength of dependence between two variables can be done using the \"Kendall Rank Coefficient\". A more detailed description about these can be found at http://www.statisticssolutions.com/correlation-pearson-kendall-spearman/ . We will now look at variables that are strictly correlated using scatterplots. One such example in our datset would be a relationship between likes_received (y) vs. www_likes_received (x). ggplot ( data = pf , aes ( x = www_likes_received , y = likes_received )) + geom_point ( alpha = 1 , colour = \"#996633\" ) + xlim ( 0 , quantile ( pf $ www_likes_received , 0.95 )) + ylim ( 0 , quantile ( pf $ likes_received , 0.95 )) + geom_smooth ( method = \"lm\" , color = \"green\" ) We have used quantile() method to find upper limits of x and y data. Additionally, we added a correlation line using the geom_smooth() function. We can find the numerical value of the correlation between these two variables. with ( pf , cor.test ( www_likes_received , likes_received )) ## ## Pearson's product-moment correlation ## ## data: www_likes_received and likes_received ## t = 937.1, df = 99001, p-value < 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9473553 0.9486176 ## sample estimates: ## cor ## 0.9479902 Correlation between two variables might not be a good thing always. For example in the above case, it was simply due to the artifact of the two data sets were highly coupled, one was a super set of the other. We can look at the caution with which correlations should be used in more detail using the \"Mitchell\" data set from the alr3 package. We can look at more details of Mitchell data set using its help method. This data set lists Data collected by Kenneth G. Hubbard on soil temperature at 20 cm depth in Mitchell, Nebraska for 17 years (1976-1992) . library ( alr3 ) data ( Mitchell ) str ( Mitchell ) ## 'data.frame': 204 obs. of 2 variables: ## $ Month: int 0 1 2 3 4 5 6 7 8 9 ... ## $ Temp : num -5.18 -1.65 2.49 10.4 14.99 ... Let use first look at a scatter plot between temperature and month. ggplot ( data = Mitchell , aes ( x = Month , y = Temp )) + geom_point () + scale_x_discrete ( breaks = seq ( 0 , 203 , 12 )) One direct observation we can make is in terms of cyclic nature of the data, Temp. is fluctuating every 12 months. This tells us the importance scales when we are looking at Data set. Clearly, a direct use of cor.test() would give us a un-physical correlation coeffieicnt! Now, we will return back to out pseudo Facebook data. Let us take a look again at the modified data frame created using the dplyr methods. We want to remove any noise in the average values. Firse, we want to create a new data frame where age is measured in fractions of months. Then we can plot two means together. By splitting age into finer bins of months, we have made the age means data noisier. In a similar way, we could smooth the data further, if we measure age in multiples of 5! pf $ age_with_month <- pf $ age + ( 12 - pf $ dob_month ) / 12 pf.fc_by_age_month <- pf %>% group_by ( age_with_month ) %>% summarise ( friend_count_mean = mean ( friend_count ), friend_count_median = median ( friend_count ), n = n ()) %>% arrange ( age_with_month ) p1 <- ggplot ( data = subset ( pf.fc_by_age , age < 71 ), aes ( x = age , y = friend_count_mean )) + geom_line () p2 <- ggplot ( data = subset ( pf.fc_by_age_month , age_with_month < 71 ), aes ( x = age_with_month , y = friend_count_mean )) + geom_line () p3 <- ggplot ( data = subset ( pf.fc_by_age , age < 71 ), aes ( x = round ( age / 5 ) * 5 , y = friend_count_mean )) + geom_line () library ( gridExtra ) grid.arrange ( p2 , p1 , p3 , ncol = 1 ) This is an example of bias variance tradeoff, and is similar to the tradeoff we make when choosing the bin width in histograms. One way, we can do this quite easily in ggplot is using the geom_smooth() function to fit a flexible statistical model. p1 <- ggplot ( data = subset ( pf.fc_by_age , age < 71 ), aes ( x = age , y = friend_count_mean )) + geom_line () + geom_smooth () p2 <- ggplot ( data = subset ( pf.fc_by_age_month , age_with_month < 71 ), aes ( x = age_with_month , y = friend_count_mean )) + geom_line () + geom_smooth () grid.arrange ( p2 , p1 , ncol = 1 ) By default, geom_smooth() uses Loess and Lowess method for smoothing. Here, the model is based on the idea that data is continuous and smooth. So, through this blog, we have noticed several ways to plot the same data. The obvious that arises is which plot to choose? In EDA , however, answer to this is, simply you should choose. The idea in EDA is that the same data when plotted differently, glean different incites.","tags":"R, Data Science, Statistics","title":"Pseudo Facebook Data - Exploring Two Variables"},{"url":"http://sadanand-singh.github.io/RplotsFacebbok","text":"In this post, we will learn about EDA of single variables using simple plots like histograms, frequency plots and box plots. Data sets used below are part of a project from the UD651 course on udacity by Facebook. The data from the project corresponds to a typical data set at Facebook . You can load the data through the following command. Notice that this is a delimited csv file. This data set consists of 99000 rows of data. We will see the details of different columns using the command below. pf <- read.csv ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/pseudo_facebook.tsv\" , sep = '\\t' ) str ( pf ) ## 'data.frame': 99003 obs. of 15 variables: ## $ userid : int 2094382 1192601 2083884 1203168 1733186 1524765 1136133 1680361 1365174 1712567 ... ## $ age : int 14 14 14 14 14 14 13 13 13 13 ... ## $ dob_day : int 19 2 16 25 4 1 14 4 1 2 ... ## $ dob_year : int 1999 1999 1999 1999 1999 1999 2000 2000 2000 2000 ... ## $ dob_month : int 11 11 11 12 12 12 1 1 1 2 ... ## $ gender : Factor w/ 2 levels \"female\",\"male\": 2 1 2 1 2 2 2 1 2 2 ... ## $ tenure : int 266 6 13 93 82 15 12 0 81 171 ... ## $ friend_count : int 0 0 0 0 0 0 0 0 0 0 ... ## $ friendships_initiated: int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ likes_received : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ mobile_likes_received: int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes : int 0 0 0 0 0 0 0 0 0 0 ... ## $ www_likes_received : int 0 0 0 0 0 0 0 0 0 0 ... The goal of this analysis is to understand user behavior and their demographics. We want to understand what they are doing on the Facebook and what they use. Please note this is not a real Facebook dataset. Our goal is to do some basic EDA (Exploratory Data Analysis) to understand any underlying patterns in the data. We will first look at a histogram of User's Birthdays. library ( ggplot2 ) qplot ( data = pf , x = dob_day , geom = \"bar\" ) + scale_x_discrete ( breaks = 1 : 31 ) We see some peculiar behavior of the data on the 1st of the month. Let us plot this data in more detail, in per month basis. qplot ( data = pf , x = dob_day , geom = \"bar\" ) + scale_x_discrete ( breaks = seq ( 1 , 31 , 4 )) + facet_wrap ( ~ dob_month , ncol = 3 ) This explains the above plot. Because of the default settings, or users privacy concerns, numerous people have 1/1 as their birthdays! Now, let us explore the distribution of friend counts in this data. qplot ( data = pf , x = friend_count , binwidth = 25 , color = I ( 'black' ), fill = I ( '#099DD9' )) + theme ( axis.text.x = element_text ( color = \"black\" , size = 11 , angle = 30 , vjust = .8 , hjust = 0.8 )) + scale_x_continuous ( limits = c ( 0 , 1000 ), breaks = seq ( 0 , 1000 , 50 )) We see the data has some outlier near 5000. This is an example of a long tail data. We want our analysis to be focused on the bunch of Facebook users, so we need to limit the axes of these plots. Additionally, we also want to look at these data as a function of gender. However, We also want to remove any data where gender is NA . qplot ( data = subset ( pf , ! is.na ( gender )), x = friend_count , binwidth = 25 , color = I ( 'black' ), fill = I ( '#099DD9' )) + theme ( axis.text.x = element_text ( color = \"black\" , size = 11 , angle = 30 , vjust = .8 , hjust = 0.8 )) + scale_x_continuous ( limits = c ( 0 , 1000 ), breaks = seq ( 0 , 1000 , 100 )) + facet_wrap ( ~ gender ) If we want to know, mean statistics of our data, we can use the table command. table ( pf $ gender ) ## ## female male ## 40254 58574 by ( pf $ friend_count , pf $ gender , summary ) ## pf$gender: female ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 37 96 242 244 4923 ## -------------------------------------------------------- ## pf$gender: male ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 27 74 165 182 4917 Let us know look at the tenure of usage (measured in Years) of Facebook. qplot ( data = pf , x = tenure / 365 , binwidth = 0.25 , color = I ( 'black' ), fill = I ( '#F79420' )) + scale_x_continuous ( breaks = seq ( 1 , 7 , 1 ), lim = c ( 0 , 7 )) + xlab ( 'Number of years using Facebook' ) + ylab ( 'Number of users in sample' ) We will now look at any pattern in the ages of Facebook users in this dataset. qplot ( data = pf , x = age , binwidth = 1 , color = I ( 'black' ), fill = I ( '#5760AB' )) + scale_x_continuous ( breaks = seq ( 8 , 113 , 5 ), lim = c ( 8 , 113 )) + xlab ( 'Age of Users in Years' ) + ylab ( 'Number of users in sample' ) One general theme of observation here is that most of the data have a long tail. In these circumstances, it is better to look at such data after certain types of transformation. Let us do such an analysis of \"friend_count\". library ( gridExtra ) plot1 <- ggplot ( data = pf , aes ( x = friend_count )) + geom_histogram () plot2 <- plot1 + scale_x_log10 () plot3 <- plot1 + scale_x_sqrt () grid.arrange ( plot1 , plot2 , plot3 , ncol = 1 ) Let us try to compare distribution of male vs female friend counts. ggplot ( aes ( x = friend_count , y = .. density.. / sum ( .. density.. )), data = subset ( pf , ! is.na ( gender ))) + geom_freqpoly ( aes ( color = gender ), binwidth = 10 ) + scale_x_continuous ( limits = c ( 0 , 1000 ), breaks = seq ( 0 , 1000 , 100 )) + theme ( axis.text.x = element_text ( color = \"black\" , size = 11 , angle = 30 , vjust = .8 , hjust = 0.8 )) + xlab ( 'Friend Count' ) + ylab ( 'Percentage of users with that friend count' ) Similarly, we can compare distributions of www likes. ggplot ( aes ( x = www_likes , y = .. count.. / sum ( .. count.. )), data = subset ( pf , ! is.na ( gender ))) + geom_freqpoly ( aes ( color = gender ), binwidth = 1 ) + scale_x_log10 () + theme ( axis.text.x = element_text ( color = \"black\" , size = 11 , angle = 30 , vjust = .8 , hjust = 0.8 )) + xlab ( 'www Likes Count' ) + ylab ( 'Percentage of users with that www likes count' ) Numerically, the two curves can quantified as: by ( pf $ www_likes , pf $ gender , sum ) ## pf$gender: female ## [1] 3507665 ## -------------------------------------------------------- ## pf$gender: male ## [1] 1430175 We can also compare two distributions graphically using \"box plots\". We can also look at the actual value using the by command. Here, we are trying to understand which gender initiated more friendships. qplot ( x = gender , y = friendships_initiated , data = subset ( pf , ! is.na ( gender )), geom = 'boxplot' ) + coord_cartesian ( ylim = c ( 0 , 200 )) by ( pf $ friendships_initiated , pf $ gender , summary ) ## pf$gender: female ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 19.0 49.0 113.9 124.8 3654.0 ## -------------------------------------------------------- ## pf$gender: male ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 15.0 44.0 103.1 111.0 4144.0 Note the use of coord_cartesian in the above plot. Use of this method ensures that numerical vale of median and shown value in the plot match, as this does not modify the actual data for the plotting purpose. Next, we want to understand if users have used certain features of Facebook or not. If we look at the summary of mobile_likes variable, median is close to 0, indicating a lot many users with 0 values for this variable. We can look also look at the logical value if value of this quantity is non-zero. We can additionally create a new variable called mobile_check_in that takes a value 1 if mobile_likes os non-zero. summary ( pf $ mobile_likes ) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0 0.0 4.0 106.1 46.0 25110.0 summary ( pf $ mobile_likes > 0 ) ## Mode FALSE TRUE NA's ## logical 35056 63947 0 mobile_check_in <- NA pf $ mobile_check_in <- factor ( ifelse ( pf $ mobile_likes > 0 , 1 , 0 )) summary ( pf $ mobile_check_in ) ## 0 1 ## 35056 63947 We can find percentage of people who have done mobile check in. sum ( pf $ mobile_check_in == 1 ) / length ( pf $ mobile_check_in ) ## [1] 0.6459097 We find that about 65% of people have used mobile devices for check in and hence it would be a good decision to continue development of such products. In summary, here we have learnt to make inferences about single variable data using a combination of plots - histograms, box plots and frequency plots; along with various numerical data.","tags":"R, Data Science, Statistics","title":"Pseudo Facebook Data - Plots in R"},{"url":"http://sadanand-singh.github.io/RIntroReddit","text":"The data set used here is part of a project from UD651 course on udacity by Facebook. The data from the project corresponds to a survey from http://reddit.com . You can load the data through the following command.We will first look at the different attributes of this data using the str() method. reddit <- read.csv ( \"https://s3.amazonaws.com/udacity-hosted-downloads/ud651/reddit.csv\" ) str (( reddit )) ## 'data.frame': 32754 obs. of 14 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ gender : int 0 0 1 0 1 0 0 0 0 0 ... ## $ age.range : Factor w/ 7 levels \"18-24\",\"25-34\",..: 2 2 1 2 2 2 2 1 3 2 ... ## $ marital.status : Factor w/ 6 levels \"Engaged\",\"Forever Alone\",..: NA NA NA NA NA 4 3 4 4 3 ... ## $ employment.status: Factor w/ 6 levels \"Employed full time\",..: 1 1 2 2 1 1 1 4 1 2 ... ## $ military.service : Factor w/ 2 levels \"No\",\"Yes\": NA NA NA NA NA 1 1 1 1 1 ... ## $ children : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ... ## $ education : Factor w/ 7 levels \"Associate degree\",..: 2 2 5 2 2 2 5 2 2 5 ... ## $ country : Factor w/ 439 levels \"/\",\"Afghanistan\",..: 391 391 391 391 391 391 124 391 391 124 ... ## $ state : Factor w/ 53 levels \"\",\"Alabama\",\"Alaska\",..: 33 33 48 33 6 33 1 6 33 1 ... ## $ income.range : Factor w/ 8 levels \"$100,000 - $149,999\",..: 2 2 8 2 7 2 NA 7 2 7 ... ## $ fav.reddit : Factor w/ 1834 levels \"\",\"___\",\"-\",\"?\",..: 717 688 1511 1528 185 688 1318 568 1629 1 ... ## $ dog.cat : Factor w/ 3 levels \"I like cats.\",..: NA NA NA NA NA 2 2 2 1 1 ... ## $ cheese : Factor w/ 11 levels \"American\",\"Brie\",..: NA NA NA NA NA 3 3 1 10 7 ... This gives us the details about different columns of data in this data set. More closely, we find two types of variables being used in this dataset: int and Factor . \"id\"\" and \"gender\" are integers, while rest of the variables are of type Factor . These Factor type variables have multiple levels. We can also look at a brief summary of the data set using the summary() method. summary ( reddit ) ## id gender age.range ## Min. : 1 Min. :0.0000 18-24 :15802 ## 1st Qu.: 8189 1st Qu.:0.0000 25-34 :11575 ## Median :16380 Median :0.0000 Under 18: 2330 ## Mean :16379 Mean :0.1885 35-44 : 2257 ## 3rd Qu.:24568 3rd Qu.:0.0000 45-54 : 502 ## Max. :32756 Max. :1.0000 (Other) : 200 ## NA's :201 NA's : 88 ## marital.status ## Engaged : 1109 ## Forever Alone : 5850 ## In a relationship : 9828 ## Married/civil union/domestic partnership: 5490 ## Single :10428 ## Widowed : 44 ## NA's : 5 ## employment.status military.service ## Employed full time :14814 No :30526 ## Freelance : 1948 Yes : 2223 ## Not employed and not looking for work: 682 NA's: 5 ## Not employed, but looking for work : 2087 ## Retired : 85 ## Student :12987 ## NA's : 151 ## children education ## No :27488 Bachelor's degree :11046 ## Yes : 5047 Some college : 9600 ## NA's: 219 Graduate or professional degree : 4722 ## High school graduate or equivalent: 3272 ## Some high school : 1924 ## (Other) : 2046 ## NA's : 144 ## country state income.range ## United States :20967 :11908 Under $20,000 :7892 ## Canada : 2888 California: 3401 $50,000 - $69,999 :4133 ## United Kingdom: 1782 Texas : 1541 $70,000 - $99,999 :4101 ## Australia : 1051 New York : 1418 $100,000 - $149,999:3522 ## Germany : 407 Illinois : 976 $20,000 - $29,999 :3206 ## (Other) : 5482 Washington: 910 (Other) :8285 ## NA's : 177 (Other) :12600 NA's :1615 ## fav.reddit dog.cat cheese ## : 4335 I like cats. :11156 Other :6563 ## askreddit : 2123 I like dogs. :17151 Cheddar :6102 ## fffffffuuuuuuuuuuuu: 1746 I like turtles.: 4442 Brie :3742 ## pics : 1651 NA's : 5 Provolone:3456 ## trees : 1311 Swiss :3214 ## (Other) :21562 (Other) :9672 ## NA's : 26 NA's : 5 The summary() method provides a more detailed analysis of the data, albeit only numerically. We can see that the int types are reported with important statistics like min,max and 4 Quartiles. The Factor types are listed with frequency of different levels. levels ( reddit $ age.range ) ## [1] \"18-24\" \"25-34\" \"35-44\" \"45-54\" \"55-64\" ## [6] \"65 or Above\" \"Under 18\" Let us look at the age.range variable in more detail. We can look at the different levels of this variables using the levels() method. This shows there are 7 possible values of this variable and some where no data is available ( NA ). A more pictorial view of this can be seen using a histogram plot of this. Similarly, we can also plot a distribution of income.range. One problem with the above plots is that the different levels are not ordered. This can be fixed using ordered Factors, instead of regular factor type variables. Additionally, We need to use a more reasonable x-label for plotting income.range. reddit $ age.range <- factor ( reddit $ age.range , levels = c ( \"Under 18\" , \"18-24\" , \"25-34\" , \"35-44\" , \"45-54\" , \"55-64\" , \"65 or Above\" ), ordered = TRUE ) levels ( reddit $ income.range ) ## [1] \"$100,000 - $149,999\" \"$150,000 or more\" \"$20,000 - $29,999\" ## [4] \"$30,000 - $39,999\" \"$40,000 - $49,999\" \"$50,000 - $69,999\" ## [7] \"$70,000 - $99,999\" \"Under $20,000\" incomes_ordered <- c ( \"Under $20,000\" , \"$20,000 - $29,999\" , \"$30,000 - $39,999\" , \"$40,000 - $49,999\" , \"$50,000 - $69,999\" , \"$70,000 - $99,999\" , \"$100,000 - $149,999\" , \"$150,000 or more\" ) incomes_ordered_lab <- c ( \"<20K\" , \"20K\" , \"30K\" , \"40K\" , \"50K\" , \"70K\" , \"100K\" , \">150K\" ) reddit $ income.range <- factor ( reddit $ income.range , levels = incomes_ordered , ordered = TRUE ) qplot ( data = reddit , x = age.range , geom = \"bar\" ) hplot <- ggplot ( data = reddit , aes ( x = income.range )) + geom_bar () hplot <- hplot + theme ( axis.text.x = element_text ( color = \"black\" , size = 11 , angle = 30 , vjust = .8 , hjust = 0.8 )) print ( hplot )","tags":"R, Data Science, Statistics","title":"Reddit Survey: Introduction to R"},{"url":"http://sadanand-singh.github.io/PythonTutWeek1","text":"/*! * * IPython notebook * */.ansibold{font-weight:700}.ansiblack{}.ansired{color:#8b0000}.ansigreen{6400}.ansiyellow{color:#c4a000}.ansiblue{8b}.ansipurple{color:#9400d3}.ansicyan{color:#4682b4}.ansigray{color:gray}.ansibgblack{background-}.ansibgred{background-color:red}.ansibggreen{background-color:green}.ansibgyellow{background-color:#ff0}.ansibgblue{background-f}.ansibgpurple{background-color:#ff00ff}.ansibgcyan{background-ff}.ansibggray{background-color:gray}div.cell{border:1px solid transparent;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;border-radius:2px;box-sizing:border-box;-moz-box-sizing:border-box;border-width:thin;border-style:solid;width:100%;padding:5px;margin:0;outline:0}div.cell.selected{border-color:#ababab}@media print{div.cell.selected{border-color:transparent}}.edit_mode div.cell.selected{border-color:green}.prompt{min-width:14ex;padding:.4em;margin:0;font-family:monospace;text-align:right;line-height:1.21429em}div.inner_cell{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1}@-moz-document url-prefix(){div.inner_cell{overflow-x:hidden}}div.input_area{border:1px solid #cfcfcf;border-radius:2px;background:#f7f7f7;line-height:1.21429em}div.prompt:empty{padding-top:0;padding-bottom:0}div.unrecognized_cell{padding:5px 5px 5px 0;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.unrecognized_cell .inner_cell{border-radius:2px;padding:5px;font-weight:700;color:red;border:1px solid #cfcfcf;background:#eaeaea}div.unrecognized_cell .inner_cell a,div.unrecognized_cell .inner_cell a:hover{color:inherit;text-decoration:none}@media (max-width:540px){.prompt{text-align:left}div.unrecognized_cell>div.prompt{display:none}}div.code_cell{}div.input{page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.input{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.input_prompt{color:navy;border-top:1px solid transparent}div.input_area>div.highlight{margin:.4em;border:none;padding:0;background-color:transparent}div.input_area>div.highlight>pre{margin:0;border:none;padding:0;background-color:transparent}.CodeMirror{line-height:1.21429em;font-size:14px;height:auto;background:0 0}.CodeMirror-scroll{overflow-y:hidden;overflow-x:auto}.CodeMirror-lines{padding:.4em}.CodeMirror-linenumber{padding:0 8px 0 4px}.CodeMirror-gutters{border-bottom-left-radius:2px;border-top-left-radius:2px}.CodeMirror pre{padding:0;border:0;border-radius:0}.highlight-base,.highlight-variable{}.highlight-variable-2{color:#1a1a1a}.highlight-variable-3{color:#333}.highlight-string{color:#<span class=\"caps\">BA2121</span>}.highlight-comment{color:#408080;font-style:italic}.highlight-number{80}.highlight-atom{color:#88F}.highlight-keyword{color:green;font-weight:700}.highlight-builtin{color:green}.highlight-error{color:red}.highlight-operator{color:#<span class=\"caps\">A2F</span>;font-weight:700}.highlight-meta{color:#<span class=\"caps\">A2F</span>}.highlight-def{f}.highlight-string-2{color:#f50}.highlight-qualifier{color:#555}.highlight-bracket{color:#997}.highlight-tag{color:#170}.highlight-attribute{c}.highlight-header{f}.highlight-quote{90}.highlight-link{c}.cm-s-ipython span.cm-keyword{color:green;font-weight:700}.cm-s-ipython span.cm-atom{color:#88F}.cm-s-ipython span.cm-number{80}.cm-s-ipython span.cm-def{f}.cm-s-ipython span.cm-variable{}.cm-s-ipython span.cm-operator{color:#<span class=\"caps\">A2F</span>;font-weight:700}.cm-s-ipython span.cm-variable-2{color:#1a1a1a}.cm-s-ipython span.cm-variable-3{color:#333}.cm-s-ipython span.cm-comment{color:#408080;font-style:italic}.cm-s-ipython span.cm-string{color:#<span class=\"caps\">BA2121</span>}.cm-s-ipython span.cm-string-2{color:#f50}.cm-s-ipython span.cm-meta{color:#<span class=\"caps\">A2F</span>}.cm-s-ipython span.cm-qualifier{color:#555}.cm-s-ipython span.cm-builtin{color:green}.cm-s-ipython span.cm-bracket{color:#997}.cm-s-ipython span.cm-tag{color:#170}.cm-s-ipython span.cm-attribute{c}.cm-s-ipython span.cm-header{f}.cm-s-ipython span.cm-quote{90}.cm-s-ipython span.cm-link{c}.cm-s-ipython span.cm-error{color:red}.cm-s-ipython span.cm-tab{background:url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=')right no-repeat}div.output_wrapper{display:-webkit-box;-webkit-box-align:stretch;display:-moz-box;-moz-box-align:stretch;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch;z-index:1}div.output_scroll{height:24em;width:100%;overflow:auto;border-radius:2px;-webkit-box-shadow:inset 0 2px 8px rgba(0,0,0,.8);box-shadow:inset 0 2px 8px rgba(0,0,0,.8);display:block}div.output_collapsed{margin:0;padding:0;display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}div.out_prompt_overlay{height:100%;padding:0 .4em;position:absolute;border-radius:2px}div.out_prompt_overlay:hover{-webkit-box-shadow:inset 0 0 1px #000;box-shadow:inset 0 0 1px #000;background:rgba(240,240,240,.5)}div.output_prompt{color:#8b0000}div.output_area{padding:0;page-break-inside:avoid;display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}div.output_area .MathJax_Display{text-align:left!important}div.output_area div.output_area img,div.output_area svg{max-width:100%;height:auto}div.output_area img.unconfined,div.output_area svg.unconfined{max-width:none}.output{display:-webkit-box;-webkit-box-orient:vertical;display:-moz-box;-moz-box-orient:vertical;display:box;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}@media (max-width:540px){div.output_area{-webkit-box-orient:vertical;-moz-box-orient:vertical;box-orient:vertical;box-align:stretch;display:flex;flex-direction:column;align-items:stretch}}div.output_area pre{margin:0;padding:0;border:0;vertical-align:baseline;background-color:transparent;border-radius:0}div.output_subarea{overflow-x:auto;padding:.4em;-webkit-box-flex:1;-moz-box-flex:1;box-flex:1;flex:1;max-width:calc(100% - 14ex)}div.output_text{text-align:left;line-height:1.21429em}div.output_stderr{background:#fdd}div.output_latex{text-align:left}div.output_javascript:empty{padding:0}.js-error{color:#8b0000}div.raw_input_container{font-family:monospace;padding-top:5px}span.raw_input_prompt{}input.raw_input{font-family:inherit;font-size:inherit;color:inherit;width:auto;vertical-align:baseline;padding:0 .25em;margin:0 .25em}input.raw_input:focus{box-shadow:none}p.p-space{margin-bottom:10px}div.output_unrecognized{padding:5px;font-weight:700;color:red}div.output_unrecognized a,div.output_unrecognized a:hover{color:inherit;text-decoration:none}.rendered_html{}.rendered_html :link,.rendered_html :visited,.rendered_html h1:first-child{margin-top:.538em}.rendered_html h2:first-child{margin-top:.636em}.rendered_html h3:first-child{margin-top:.777em}.rendered_html h4:first-child,.rendered_html h5:first-child,.rendered_html h6:first-child{margin-top:1em}.rendered_html *+ol,.rendered_html *+ul{margin-top:1em}.rendered_html *+table{margin-top:1em}.rendered_html *+p{margin-top:1em}.rendered_html *+img{margin-top:1em}div.text_cell{display:-webkit-box;-webkit-box-orient:horizontal;display:-moz-box;-moz-box-orient:horizontal;display:box;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch}@media (max-width:540px){div.text_cell>div.prompt{display:none}}div.text_cell_render{outline:0;resize:none;width:inherit;border-style:none;padding:.5em .5em .5em .4em;box-sizing:border-box;-moz-box-sizing:border-box;-webkit-box-sizing:border-box}a.anchor-link:link{text-decoration:none;padding:0 20px;visibility:hidden}h1:hover .anchor-link,h2:hover .anchor-link,h3:hover .anchor-link,h4:hover .anchor-link,h5:hover .anchor-link,h6:hover .anchor-link{visibility:visible}.text_cell.rendered .input_area{display:none}.text_cell.rendered .text_cell.unrendered .text_cell_render{display:none}.cm-header-1,.cm-header-2,.cm-header-3,.cm-header-4,.cm-header-5,.cm-header-6{font-weight:700;font-family:\"Helvetica Neue\",Helvetica,Arial,sans-serif}.cm-header-1{font-size:185.7%}.cm-header-2{font-size:157.1%}.cm-header-3{font-size:128.6%}.cm-header-4{font-size:110%}.cm-header-5,.cm-header-6{font-size:100%;font-style:italic} .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #<span class=\"caps\">FF0000</span> } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #<span class=\"caps\">BC7A00</span> } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #<span class=\"caps\">FF0000</span> } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #<span class=\"caps\">0044DD</span> } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #<span class=\"caps\">BA2121</span> } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #<span class=\"caps\">0000FF</span>; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #<span class=\"caps\">AA22FF</span> } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #<span class=\"caps\">D2413A</span>; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #<span class=\"caps\">0000FF</span> } /* Name.Function */ .highlight .nl { color: #<span class=\"caps\">A0A000</span> } /* Name.Label */ .highlight .nn { color: #<span class=\"caps\">0000FF</span>; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #<span class=\"caps\">AA22FF</span>; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sb { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Backtick */ .highlight .sc { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Char */ .highlight .sd { color: #<span class=\"caps\">BA2121</span>; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Double */ .highlight .se { color: #<span class=\"caps\">BB6622</span>; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Heredoc */ .highlight .si { color: #<span class=\"caps\">BB6688</span>; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #<span class=\"caps\">BB6688</span> } /* Literal.String.Regex */ .highlight .s1 { color: #<span class=\"caps\">BA2121</span> } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ Python for Non-Programmers Python is a widely used general-purpose, high-level programming language. Due to its focus on readability, wide-spread popularity and existence of a plethora of libraries (also called modules), it is one of the most preferred programming languages for scientists and engineers. In this series of python tutorials, I will provide a set of lectures on various basic topics of python. The prime target audience for this series are scientist in non-programming fields like microbiology, genetics, psychology etc. who have some to none programming experience. Here is a brief list of topics I will cover per week. I will also post exercises at the end of each session, along with the expected outputs. You should plan to complete these exercises within 5-6 days, before the new tutorial is posted. You will be judging your exercises on your own. The goal should be to match your program's output to the expected output. Week 1 : Working with Python on Windows, Concept of Variables & Math Operations, Displaying Outputs Week 2 : User Inputs, Modules, Comments and Basics of Strings Week 3 : More on Strings, Lists and Other Containers Week 4 : Looping/iterating, if/else Conditions Week 5 : Advanced String Operations Week 6 : Regular Expressions and Strings Week 7 : Reading and Writing Files Week 8 : Functions and Writing Scripts Week 9 : Interacting with Operating System Week 10 : Handling and Plotting Data in Python Week 11 : Basic Statistics in Python using Pandas Week 12 : Introduction to Biopython Week 1. Introduction to Python To start working with any programming language, first thing you need is a working installation of that language. Today, we will go through installation of python on Windows machines. To keep things simple, We will be running our simple programs in google chrome browser, without any need of an installation. For later exercises, from Week 7 onwards, I would highly recommend getting access to a Linux/Mac machine. However, I will also provide doing the same things on Windows machines and Google Chrome as well. In this week's session, I will be assuming you will be using online python in google chrome. To follow these tutorials, please run any code and observe output that you see in code blocks. Installing Python Go to the following website to get access to python: http://repl.it/languages/python3 You should get a window like this: You can work with this system in two ways: A. Write your code interactively (one command at a time) on the dark screen on the left. Pressing ENTER will show you the output of that particular command. Lets' try our first program: In [1]: print ( \"My Name is so-and-so\" ) My Name is so-and-so The above program will simply output whatever was put under \"quotes\". We will learn more about the print() method (what is a method/function in python?) towards the end of this session. Variables A variable is a symbol or name that stands for a value. For example, in the expression x = 23 x is a name/symbol whose value is numerical and equal to 23. if we write something like this: y = x+12-5, then y is a new variable whose value should be equal to 23+12-5. Let's try these in a program In [2]: x = 23 print ( \"x = \" , x ) x = 23 In [3]: y = x + 12 - 5 print ( \"y =\" , y ) y = 30 In [4]: print ( 23 + 12 - 5 ) 30 We ran 3 commands. First we created a variable ‘x' with a value of 23. We verified it value by using a print() method! Second, we created another variable called ‘y' whose values is equal to mathematical operation of ‘x+12-5'. If you remember basic algebra, this is exactly that. Finally, we confirmed that value of ‘y' is exactly equal to 23+12-5. Hopefully, you got a feel of variables. Variables are not limited to just numbers though. We can also store text. For example: In [5]: name = \"Sadanand Singh\" print ( \"My Name is:\" , name ) My Name is: Sadanand Singh Here, we saved my name Sadanand Singh in a variable called \"name\". Concept of variable is very fundamental to any programming language. You can think of them as tokens that store some value. Lets consider this example to understand variable in more detail. You are doing your taxes, all of your calculations depend on your net income. If you do all your calculation in terms of actual value of net income, every time you write the number, your chances of making a mistake increases. Furthermore, suppose, you want to update the income due to some mistake in the beginning, now you have update every place where you used your income. Lets consider a second case where you declare a variable called \"income\", and store income = 30000. Now, any time you do any calculation with income, you will be using the variable \"income\". In this framework, because you have to type your income just once, chances of making mistakes are least and changing it needs just one change! Math Operations Now that we know how to declare and use variables, we will look at first what all we can do with numbers in python. Using simple math operations are extremely easy in Python. Here are all the basic math operations that one can do in Python. Syntax Math Operation Name a+b a + b Addition a-b a - b Subtraction a*b a x b Multiplication a/b $a \\div b$ Division a**b $a&#94;b$ Power/Exponent abs(a) Absolute Value -a $-1 \\times a$ Negation a//b quotient of $a \\div b$ Quotient a%b Remainder of $a \\div b$ Remainder Here are some example operation. Please repeat these and observe the use of parenthesis in using the BODMAS principle. In [6]: 3 + 5 Out[6]: 8 In [7]: 2 * 3 + 3.34 + 4 - 45.67 Out[7]: -32.33 In [8]: 12.7 - 10 * 23.5 / 0.5 Out[8]: -457.3 In [9]: 12 - ( 11 + 34 ) / 2 Out[9]: -10.5 In [10]: a , b = 5 , 6 Above is a special case of assigning multiple variables together. In the above example we stored a=5 and b=6. Lets confirm these: In [11]: print ( \"a = \" , a ) print ( \"b = \" , b ) a = 5 b = 6 In [12]: c = a ** b print ( \" a raised to the power b is:\" , c ) a raised to the power b is: 15625 In [13]: b = - b In [14]: print ( \"b = \" , b ) b = -6 Can you guess what we did here? First we use negation operation to get \"-b\" i.e. -6. Then, we we redefined b to be equal to -6! Let consider the following example: a = a-b-3. What do you expect the value to be? Now lets check if you are correct: In [15]: a = a - b - 3 print ( \"new value of a is: \" , a ) new value of a is: 8 What do you expect if we do b = -b again? In [16]: b = - b print ( \"New Value of b is:\" , b ) New Value of b is: 6 In [17]: a // b Out[17]: 1 In [18]: a % b Out[18]: 2 In [19]: b **- 2 Out[19]: 0.027777777777777776 You can perform many other advanced math operations using the \"math module\". To use them, first you will need to import the math module in your code like this: In [20]: import math Then, you use operations like the following: In [21]: c = math . sqrt ( 25 ) print ( c ) c = math . log ( 10 ) print ( c ) c = math . log10 ( 10 ) print ( c ) c = math . cos ( math . pi ) print ( c ) 5.0 2.302585092994046 1.0 -1.0 You can all other available mathematical operation in this module on the python website . print() Function in Python Primary way to see and print information on your screen in python is using a method/fuction called \"print()\". We will learn more about functions in python later. For today, you can think of functions as a program that \"does something\". For example, the function print(), does the job of printing things on screen. Notice the use of () in functions. () separates a function from a variable. For example, \"foo\" is a variable, whereas \"foo()\" is a function, called sometimes called as method. Functions also have a concept of arguments. Arguments can be thought as inputs to functions. For example, we have function that adds 2 numbers, then this function will need 2 arguments, the two numbers that we want to add. We can denote this function as, addition(a,b). Similarly, functions also have a concept of return values. Return value can be thought as the output of that function. For example, in the above example of addition(a,b) function, sum of two numbers will be the \"return value\" of the function. We can write this as, c = addition(a,b). Here, a and b are arguments to function addition() and c is the return value of this function. A function can have any number of arguments, zero to any number; where it can have either zero or 1 return values. Now, coming back to the print() method, that we have been using throughout this tutorial. print() method can take any number of arguments separated by commas. All it does is to \"print\" those on your screen. Lets look at some examples: In [22]: print ( 3 , 4 ) print ( \"My Name is Sadanand Singh\" ) print ( \"My Name is \" , \"Sadanand Singh\" , \"and My age is: \" , 29 ) 3 4 My Name is Sadanand Singh My Name is Sadanand Singh and My age is: 29 Now, lets try something fun. In [23]: print ( \"My Name is\" , \"Sadanand\" , sep = \"***\" ) My Name is***Sadanand In [24]: print ( \"My Name is\" , \"Sadanand\" , \"Singh\" , \"My Age is\" , \"12\" , \"\" , sep = \"***\" ) My Name is***Sadanand***Singh***My Age is***12*** Can you explain what is happening here! Excercise We will all things we have learned today using the exercise below. We will follow the tax preparation example: Create a variable to store \"income\" Create another variable called \"taxRate\" which is equal to 1/100000th of \"income\". Net federal tax will be equal to 1.5 times income times taxRate Net state tax will be equal to square root on federal tax Net tax will be federal tax + state tax Total final tax will be Net tax + log to the base of 2 of Net Tax Print following values clearly using print(): income, taxRate, Federal Tax, State Tax, Net Tax and Final Tax. First run with an income of 60000 Repeat with an income of 134675 Your output should look like the following in two cases. Case 1: income = 60000 Total Income is: 60000 Tax Rate is: 0.059999999999999998 Total Federal Tax is: 1800.0 Total State Tax is: 42.426406871192853 Net Tax is: 1842.4264068711927 Total Tax is: 1853.2737981499038 Case 2: income = 134675 Total Income is: 134675 Tax Rate is: 0.13467499999999999 Total Federal Tax is: 9068.6778125000001 Total State Tax is: 95.229605756298284 Net Tax is: 9163.9074182562981 Total Tax is: 9177.0691654243201 Great! Next week we dive into Python further. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"programming","title":"Python Tutorial Week 1"},{"url":"http://sadanand-singh.github.io/GajarHalwa","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule. But, never is too late. Especially, when you can start with some exotic food! Here is one of Abha's finest creations from her Kitchen. Carrot halwa is a common north Indian dessert, healthy carrots cooked in milk and mango puree. Ingredients 8-10 organic carrots (for a more authentic taste). Condensed milk (medium size) or 1-2 cup Ricotta cheese 1-2 cup whole Milk. Dried nuts (Chopped Almonds, Cashews, Golden raisins and Dates) Sugar/Fresh Mango puree as per taste. 3-4 Tablespoon unsalted butter 3-4 whole cardamom or 1/2 teaspoon cardamom powder Preparation Wash and Peel the carrots and grate them. Heat a non-stick pan on medium flame and add 3-4 tablespoon of butter. When the butter melts, add the grated carrots and start frying. Stir the carrots till all the water from the carrots is lost. Add Ricotta cheese with 1-2 cup whole milk or 1 can of Condensed milk and let the carrots cook for 5-7 minutes on low flame. Continuously stir the carrots so as not to burn them. When 3/4th of the milk is dried, add chopped nuts and Sugar. Turn off the flame after 3-4 minutes, after all the milk is dried. Let the halwa cool down. Garnish it with nuts and dried milk. Interesting Facts Nutritional Value of carrots. More about Gajar Halwa","tags":"Recipes","title":"Carrot Halwa Recipe"},{"url":"http://sadanand-singh.github.io/FibonacciNumbers","text":"The world of computers is moving fast. While going through some materials on algorithms, I have come across an interesting discussion -enhancements in hardware (cpu) vis-a-vis algorithms. One side of the coin is the hardware - the speed of computers.The famous Moore's law states that: The complexity for minimum component costs has increased at a rate of roughly a factor of two per year . Certainly over the short term this rate can be expected to continue, if not to increase. Over the longer term, the rate of increase is a bit more uncertain, although there is no reason to believe it will not remain nearly constant for at least 10 years. — G. Moore, 1965 In simple words, Moore's law is the observation that, over the history of computing hardware, the number of transistors in a dense integrated circuit has doubled approximately every two years. More precisely, the number of transistors in a dense integrated circuit has increased by a factor of 1.6 every two years. More recently, keeping up with this has been challenging. In the context of this discussion, the inherent assumption is that number of transistors is directly proportional to the speed of computers. Now, looking at the other side of the coin - speed of algorithms. According to Excerpt from Report to the President and Congress: Designing a Digital Future, December 2010 (page 97) : Everyone knows Moore's Law – a prediction made in 1965 by Intel co-­founder Gordon Moore that the density of transistors in integrated circuits would continue to double every 1 to 2 years…. in many areas, performance gains due to improvements in algorithms have vastly exceeded even the dramatic performance gains due to increased processor speed. The gain in computing speed due to algorithms have been simply phenomenal, unprecedented, to say the least! Being actively involved with realization of Moore's law ( guess where I work! ), I have been naturally attracted in the study and design of algorithms. To get a more practical perspective on this, lets look at the problem of finding large Fibonacci numbers . These numbers have been used in wide areas ranging from arts to economics to biology to computer science to the game of poker! The simple definition of these numbers are: $$F_{n} = \\begin{cases} F_{n-2} + F_{n-1} & \\text{if } n > 1 \\\\ 1 & \\text{if } n = 1 \\\\ 0 & \\text{if } n = 0 \\end{cases}$$ So, first few Fibonacci numbers are: \\(0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144 \\ldots\\) These numbers grow almost as fast as powers of 2: for example, \\(F_{30}\\) is over a million, and \\(F_{100}\\) is 21 digits long! In general, \\(F_n \\approx 2&#94;{0.694n}\\) Clearly, we need a computing device to calculate say \\(F_{200}\\) . The most basic algorithm, that comes to mind is a recursive scheme that taps directly into the above definition of Fibonacci series. def fibRecursive ( n ): if n == 0 : return 0 if n == 1 : return 1 return fibRecursive ( n - 2 ) + fibRecursive ( n - 1 ) If you analyze this scheme, this is in fact an exponential algorithm, i.e. fibRecursive( n ) is proportional to \\(2&#94;{0.694n} \\approx (1.6)&#94;n\\) , so it takes 1.6 times longer to compute \\(F_{n+1}\\) than \\(F_n\\) . This is interesting. Recall, under Moore's law, computers get roughly 1.6 times faster every 2 years. So if we can reasonably compute \\(F_{100}\\) with this year's technology, then only after 2 years we will manage to get \\(F_{101}\\) ! Only one more Fibonacci number every 2 years! Luckily, algorithms have grown at a much faster pace. Let's consider improvements w.r.t to this current problem of finding \\(n&#94;{th}\\) Fibonacci number, \\(F_n\\) . First problem we should realize in the above recursive scheme is that we are recalculating lower \\(F_n\\) at each recursion level. Lets solve this issue by storing each calculation and avoiding any re-calculation! def fibN2 ( n ): a = 0 b = 1 if n == 0 : return 0 for i in range ( 1 , n + 1 ): c = a + b a = b b = c return b On first glance this looks like an \\(\\mathcal{O}(n)\\) scheme, as we consider each addition as one operation. However, we should realize that as \\(n\\) increases, addition can not be assumed as a single operation, rather every step of addition is an \\(\\mathcal{O}(n)\\) operation, recall first grade Math for adding numbers digit by digit!! Hence, this algorithm is an \\(\\mathcal{O}(n&#94;2)\\) scheme. Can we do better? You bet, we can! Lets consider the following scheme: $$\\begin{pmatrix} 1&1 \\\\ 1&0 \\end{pmatrix}&#94;n = \\begin{pmatrix} F_{n+1}&F_n \\\\ F_n&F_{n-1} \\end{pmatrix}$$ We can use a recursive scheme to calculate this matrix power using a divide and conquer scheme in \\(\\mathcal{O}(\\log{}n)\\) time. def mul ( A , B ): a , b , c = A d , e , f = B return a * d + b * e , a * e + b * f , b * e + c * f def pow ( A , n ): if n == 1 : return A if n & 1 == 0 : return pow ( mul ( A , A ), n // 2 ) else : return mul ( A , pow ( mul ( A , A ), ( n - 1 ) // 2 )) def fibLogN ( n ): if n < 2 : return n return pow (( 1 , 1 , 0 ), n - 1 )[ 0 ] Lets think a bit harder about this. Is it really an \\(\\mathcal{O}(\\log{}n)\\) scheme? It involves multiplication of numbers, the method mul(A, B). What happens when \\(n\\) is very large? Sure, this will blow up, as typical multiplication would be an \\(\\mathcal{O}(n&#94;2)\\) operation. So, in fact, our new scheme is \\(\\mathcal{O}(n&#94;2 \\log{}n)\\) ! Luckily, we can solve even large multiplications in \\(\\mathcal{O}(n&#94;{log_2{3}} \\approx n&#94;{1.585})\\) , using Karatsuba multiplication , which is again a divide and conquer scheme. Here is one simple implementation (Same as the above scheme, but with the following mul(A,B) method): _CUTOFF = 1536 def mul ( A , B ): a , b , c = A d , e , f = B return multiply ( a , d ) + multiply ( b , e ), multiply ( a , e ) + multiply ( b , f ), multiply ( b , e ) + multiply ( c , f ) def multiply ( x , y ): if x . bit_length () <= _CUTOFF or y . bit_length () <= _CUTOFF : return x * y else : n = max ( x . bit_length (), y . bit_length ()) half = ( n + 32 ) // 64 * 32 mask = ( 1 << half ) - 1 xlow = x & mask ylow = y & mask xhigh = x >> half yhigh = y >> half a = multiply ( xhigh , yhigh ) b = multiply ( xlow + xhigh , ylow + yhigh ) c = multiply ( xlow , ylow ) d = b - a - c return ((( a << half ) + d ) << half ) + c So, this final scheme is in \\(\\mathcal{O}(n&#94;{1.585}\\log{}n)\\) time. Here is one final way of solving this problem in the same \\(\\mathcal{O}(n&#94;{1.585}\\log{}n)\\) time, but using a somewhat simpler scheme! If we know \\(F_K\\) and \\(F_{K+1}\\) , then we can find, $$F_{2K} = F_K \\left [ 2F_{K+1}-F_K \\right ]$$ $$F_{2K+1} = {F_{K+1}}&#94;2+{F_K}&#94;2$$ We can implement this using the Karatsuba multiplication as follows: def fibFast ( n ): if n <= 2 : return 1 k = n // 2 a = fibFast ( k + 1 ) b = fibFast ( k ) if n % 2 == 1 : return multiply ( a , a ) + multiply ( b , b ) else : return multiply ( b ,( 2 * a - b )) That's it for today. We saw how far algorithms can go in speed for such simple problems. Let me know in the comments below, if you have any faster or alternate algorithms in mind. Have fun, May zero be with you! if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Algorithms","title":"Moore's Law and Algorithms - Case of Fibonacci Numbers"},{"url":"http://sadanand-singh.github.io/PrimeNumberAndPath","text":"Puzzle 1: Prime Numbers Prove that \\(p&#94;2-1\\) is divisible by 24, where \\(p\\) is a prime number with \\(p>3\\) . This is a simple one - do not go by the technicality of the problem statement. Solution \\(p&#94;2-1 = (p-1)\\times (p+1)\\) Given, \\(p\\) is a prime number \\(>3\\) , \\(p-1\\) and \\(p+1\\) are even. We can also write, $$p-1 = 2K, \\mbox{ and } p+1 = 2K+2 = 2(K+1)$$ Given, \\(K \\in \\mathbb{N}\\) , either \\(K\\) or \\(K+1\\) are also even. Hence, \\((p-1)\\times (p+1)\\) is divisible by \\(2\\times 4 = 8\\) . Furthermore, as \\(p\\) is prime, we can write it as either \\(p = 3s+1\\) or \\(p = 3s-1\\) , where \\(s \\in \\mathbb{N}\\) . In either case, one of \\(p-1\\) or \\(p+1\\) are divisible by 3 as well. Hence, \\(p&#94;2-1\\) is divisible by \\(8\\times 3 = 24\\) . Puzzle 2: Hopping on a Chess Board On a chess board, basically a \\(8\\times 8\\) grid, how many ways one can go from the left most bottom corner to the right most upper corner? In chess naming conventions, from \"a1\" to \"h8\". Clarification: In the original post this problem was ill defined. Please solve this problem with the constraints that only up and right moves are allowed. Can you find the generic answer for the case of \\(N\\times M\\) grid. Solution Correction: For an \\(N\\times M\\) grid, we need only \\(N-1\\) right and \\(M-1\\) up moves. Thank you Devin for pointing this out. Given only forward moves are allowed, for any arbitrary grid of \\(N\\times M\\) , a total of \\((N-1) + (M-1)\\) moves are needed. Any \\(N-1\\) of these moves can be of type right and \\(M-1\\) of type up . In short, this is a combinatorial problem of distributing \\(N+M-2\\) objects into groups of \\(N-1\\) and \\(M-1\\) . This is simply, $$\\dbinom{N+M-2}{N-1} = \\frac{(N+M-2)!}{(N-1)! (M-1)!}$$ In the particular case of the chess board, \\(N = M = 8\\) . Hence, total number of possible paths are: $$\\mbox{No. of Paths} = \\frac{14!}{7! 7!} =3432$$ Thank you Rohit and Amber for posting quick solutions! Let me know if you have any other interesting alternative solutions to these problems. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); var location_protocol = (false) ? 'https' : document.location.protocol; if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:'; mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Puzzle","title":"Two Simple Math Puzzles"},{"url":"http://sadanand-singh.github.io/plasmaInstall","text":"In my last post on Arch Installation, We installed the base system and we can now login into our new system as root using the password that we set. Now, we will proceed further to install the Plasma 5 desktop. Add New User Choose $ USER per your liking. I chose ssingh, so in future commands whenever you see ssingh please replace it with your $ USER . $ useradd -m -G wheel -s /bin/bash $USERNAME $ chfn --full-name \" $FULL_NAME \" $USERNAME $ passwd $USERNAME Plasma 5 Desktop To install anything new, we first need to enable wifi for this session. To do this I will use the wl* file that we saved from the initial installation setup at /etc/netctl. $ netctl start /etc/netctl/wl* To setup a graphical desktop, first we need to install some basic X related packages, and some essential packages (including fonts): $ pacman -S intel-dri xf86-video-intel xorg-server xorg-server-utils $ pacman -S intel xorg xorg-xinit xorg-twm xorg-xclock xterm mesa $ pacman -S tlp tlp-rdw acpi_call bash-completion git meld $ pacman -S firefox flashplugin $ pacman -S ttf-dejavu ttf-freefont ttf-liberation ttf-anonymous-pro $ pacman -S adobe-source-code-pro-fonts Now, we will install the packages related to Plasma 5: $ pacman -S kf5 kf5-aids $ pacman -S plasma kdebase kdeutils-kwalletmanager $ pacman -S kdegraphics-ksnapshot gwenview $ pacman -R plasma-mediacenter $ pacman -S networkmanager plasma-nm Now we have to setup a display manager. I chose recommended SDDM for plasma 5. $ pacman -S sddm sddm-kcm $ vim /etc/sddm.conf ... [ Theme ] # Current theme name Current = breeze # Cursor theme CursorTheme = breeze_cursors ... $ systemctl enable sddm Also make sure that networkmanager starts at boot: $ systemctl disable dhcpcd.service $ systemctl enable NetworkManager Audio Setup This is pretty simple. Install following packages and you should be done: $ pacman -S alsa-utils pulseaudio pulseaudio-alsa libcanberra-pulse $ pacman -S libcanberra-gstreamer jack2-dbus kmix $ pacman -S vlc mplayer Useful Tips This part is optional and you can choose as per your taste. Sync time: $ pacman -S ntp $ systemctl enable ntpd On Plasma 5, It is recommended to enable no-bitmaps to improve the font rendering: $ sudo ln -s /etc/fonts/conf.avail/70-no-bitmaps.conf /etc/fonts/conf.d I prefer to use zsh over bash. I also use the awesome prezto for zsh configuration. However, I ran into an issue with sddm, where use of their configurations will lead sddm to freeze. To fix these you need to do the following before you reboot/logout your system. $ vim /usr/share/sddm/scripts/Xsession ... */zsh ) [ -z \" $ZSH_NAME \" ] && exec $SHELL $0 \" $@ \" emulate -R sh [ -d /etc/zsh ] && zdir = /etc/zsh || zdir = /etc zhome = ${ ZDOTDIR :- $HOME } # zshenv is always sourced automatically. [ -f $zdir /zprofile ] && . $zdir /zprofile [ -f $zhome /.zprofile ] && . $zhome /.zprofile [ -f $zdir /zlogin ] && . $zdir /zlogin [ -f $zhome /.zlogin ] && . $zhome /.zlogin ;; ... Should be changed to: ... */zsh ) [ -z \" $ZSH_NAME \" ] && exec $SHELL $0 \" $@ \" [ -d /etc/zsh ] && zdir = /etc/zsh || zdir = /etc zhome = ${ ZDOTDIR :- $HOME } # zshenv is always sourced automatically. [ -f $zdir /zprofile ] && . $zdir /zprofile [ -f $zhome /.zprofile ] && . $zhome /.zprofile [ -f $zdir /zlogin ] && . $zdir /zlogin [ -f $zhome /.zlogin ] && . $zhome /.zlogin emulate -R sh ;; ... If you use vim as your primary editor, you may find this vimrc quite useful. That's It. You are done. Start playing your new beautiful desktop. Please leave your comments with suggestions or any word of appreciation if this has been of any help to you. Follow my blog for any further suggestions or improvements in this guide.","tags":"Computers","title":"Plasma 5 Installation on Arch Linux"},{"url":"http://sadanand-singh.github.io/archInstall","text":"You must be thinking - yet another installation guide! There is no dearth of \" Installation \" guides of Arch on web. So why another one? With advancements like BTRFS file system, UEFI motherboards and modern in-development desktop environment like Plasma 5; traditional Arch Wiki guide and Arch Beginners' Guide can only be of a limited help. After I got my new desktop , my goal was to setup it with a modern setup. I decided to go with Arch Linux with btrfs file system and Plasma 5 desktop. Coming from OSX , I just love how far linux has come in terms of looks - far better than OSX ! I will cover this in two parts. First in this post, I will install the base system. Then, in a follow up post, I will discuss details of setting up final working Plasma 5 desktop. Initial Setup Download the latest iso from Arch website and create the uefi usb installation media. I used my mac to do this on terminal: $ diskutil list $ diskutil unmountDisk /dev/disk1 $ dd if = image.iso of = /dev/rdisk1 bs = 1m 20480+0 records in 20480+0 records out 167772160 bytes transferred in 220.016918 secs ( 762542 bytes/sec ) $ diskutil eject /dev/disk1 Use this media to boot into your machine. You should boot into UEFI mode if you have a UEFI motherboard and UEFI mode enabled. To verify you have booted in UEFU mode, run: $ efivar -l This should give you a list of set UEFI variables. Please look at the Begineers' Guide in case you do not get any list of UEFI variables. Wifi To setup wifi simply run: $ wifi-menu This is a pretty straight forward tool and will setup wifi for you for this installation session. This will also create a file at /etc/netctl/ . We will use this file later to enable wifi at the first session after installation. For editing different configurations, I tend to use vim . So we will update our package cache and install vim. $ pacman -Syy $ pacman -S vim Hard Drives In my desktop, I have two hard drives, one 256 GB solid state drive ( SDD ) and another 1TB hard drive ( HDD ). I set up my drives as follows: - SDD for root(/), /boot, /opt and /home partitions - HDD for /var and /media partitions. For UEFI machines, we need to use a GPT partition table and /boot partition has to be a fat32 partition with a minimum size of 512 MB . We will format rest other partitions with BTRFS . See this link for benefits of using btrfs partitions. First list your hard drives with the following: $ lsblk $ cat /proc/partitions Assuming, my setup above, now create gpt partitions and format them. $ dd if = /dev/zero of = /dev/sda bs = 1M count = 5000 $ gdisk /dev/sda Found invalid MBR and corrupt GPT. What do you want to do ? ( Using the GPT MAY permit recovery of GPT data. ) 1 - Use current GPT 2 - Create blank GPT Then press 2 to create a blank GPT and start fresh ZAP: $ press x - to go to extended menu $ press z - to zap $ press Y - to confirm $ press Y - to delete MBR It might now kick us out of gdisk, so get back into it: $ gdisk /dev/sda $ Command ( ? for help ) : m $ Command ( ? for help ) : n $ Partition number ( 1-128, default 1 ) : $ First sector ( 34-500118158, default = 2048 ) or { +- } size { KMGTP } : $ Last sector ( 2048-500118, default = 500118 ) or { +- } size { KMGTP } : 512M $ Current type is 'Linux filesystem' $ Hex code or GUID ( L to show codes, Enter = 8300 ) : ef00 $ Changed type of partition to 'EFI System' $ Partition number ( 2-128, default 2 ) : $ First sector ( 34-500118, default = 16779264 ) or { +- } size { KMGTP } : $ Last sector ( 16779264-500118, default = 500118 ) or { +- } size { KMGTP } : $ Current type is 'Linux filesystem' $ Hex code or GUID ( L to show codes, Enter = 8300 ) : $ Changed type of partition to 'Linux filesystem' $ Command ( ? for help ) : p $ Press w to write to disk $ Press Y to confirm Repeat the above procedure for /dev/sdb , but create just one partition with all values as default. At the end we will have three partitions: /dev/sda1, /dev/sda2 and /dev/sdb1 Now we will format these partitions. $ mkfs.vfat -F32 /dev/sda1 $ mkfs.btrfs -L arch /dev/sda2 $ mkfs.btrfs -L data /dev/sdb1 Now, we will create btrfs subvolumes and mount them properly for installation and final setup. $ mount /dev/sda2 /mnt $ btrfs subvolume create /mnt/ROOT $ btrfs subvolume create /mnt/opt $ btrfs subvolume create /mnt/home $ umount /mnt $ mount /dev/sdb1 /mnt $ btrfs subvolume create /mnt/var $ btrfs subvolume create /mnt/media $ umount /mnt Now, once the subvolumes have been created, we will mount them in appropriate locations with optimal flags. $SSD_MOUNTS = \"rw,noatime,nodev,compress=lzo,ssd,discard,space_cache,aut odefrag,inode_cache\" $ HDD_MOUNTS = \"rw,nosuid,nodev,relatime,space_cache\" $ EFI_MOUNTS = \"rw,noatime,discard,nodev,nosuid,noexec\" $ mount -o $SSD_MOUNTS ,subvol = ROOT /dev/sda2 /mnt $ mkdir -p /mnt/opt $ mkdir -p /mnt/home $ mkdir -p /mnt/var $ mkdir -p /mnt/media $ mount -o $SSD_MOUNTS ,nosuid,subvol = opt /dev/sda2 /mnt/opt $ mount -o $SSD_MOUNTS ,nosuid,subvol = home /dev/sda2 /mnt/home $ mount -o $HDD_MOUNTS ,subvol = var /dev/sdb1 /mnt/var $ mount -o $HDD_MOUNTS ,subvol = media /dev/sdb1 /mnt/media $ mkdir -p /mnt/boot $ mount -o $EFI_MOUNTS /dev/sda1 /mnt/boot Base Installation Now, we will do the actually installation of base packages. $ pacstrap /mnt base base-devel btrfs-progs $ genfstab -U -p /mnt >> /mnt/etc/fstab Edit the /mnt/ect/fstab file to add following /tmp mounts. tmpfs /tmp tmpfs rw,nodev,nosuid 0 0 tmpfs /dev/shm tmpfs rw,nodev,nosuid,noexec 0 0 Copy our current wifi setup file into the new system. This will enable wifi at first boot. Next, chroot into our newly installed system: $ cp /etc/netctl/wl* /mnt/etc/netctl/ $ arch-chroot /mnt /bin/bash Some basic setup: $ pacman -Syy $ pacman -S sudo vim $ vim /etc/locale.gen ... $en_SG ISO-8859-1 en_US.UTF-8 UTF-8 $en_US ISO-8859-1 ... $ locale-gen $ echo LANG = en_US.UTF-8 > /etc/locale.conf $ export LANG = en_US.UTF-8 $ ls -l /usr/share/zoneinfo $ ln -sf /usr/share/zoneinfo/Zone/SubZone /etc/localtime $ hwclock --systohc --utc $ sed -i \"s/# %wheel ALL=(ALL) ALL/%wheel ALL=(ALL) ALL/\" /etc/sudoers $ HOSTNAME = euler $ echo $HOSTNAME > /etc/hostname $ pacman -S dosfstools efibootmgr $ sed -i 's/&#94;\\(HOOKS=.*fsck\\)\\(.*$\\)/\\1 btrfs\\2/g' /etc/mkinitcpio.conf $ mkinitcpio -p linux $ gummiboot install $ passwd We also need to install following packages for wifi to work at first boot: $ pacman -S iw wpa_supplicant We will also add hostname to our /etc/hosts file: $ vim /etc/hosts ... 127.0.0.1 localhost.localdomain localhost $HOSTNAME ::1 localhost.localdomain localhost $HOSTNAME ... We will do the final setup now, that should enable us to login into our newly installed system. Make sure following two files look as follows: $ vim /boot/loader/loader.conf ... timeout 3 default arch ... $ vim /boot/loader/entries/arch.conf ... title Arch Linux linux /vmlinuz-linux initrd /initramfs-linux.img options root = UUID = 1c27fa42-ca19-482f-b49a-3b8366eb7783 rw rootfstype = btrfs rootflags = subvol = ROOT ... $ exit $ umount -R /mnt $ reboot Awesome! We are ready to play with our new system. Please see my next post for more.","tags":"Computers","title":"Arch Installation Guide"},{"url":"http://sadanand-singh.github.io/PalakPaneer","text":"Last month or so has been all silent here. No puzzles, no math, no computers and most important, no Food! And as usual blame is on my work schedule. Yes, the silence is finally over, and what's better than giving your taste buds some rejuvenation after some long busy weeks at work. Continuing with my healthy but tasty choices, today I tried to cook Palak (Spinach) with Paneer . Palak Paneer is a common north Indian cuisine, Indian cottage cheese cooked in spinach puree. Its a bit involved than my last few dishes. Nevertheless, I promise you - the \"yummy-ness\" of this one is worth all the trouble! Ingredients 1 lb Paneer (Indian Cottage Cheese) 1 Bunch Fresh Spinach Leaves 1/2 Medium Onion Finely Cut 1/2 inch Ginger Root Grated 1 Medium Tomato Finely Chopped 1 Clove of Garlic Minced 1 Medium Bay Leaf 1/2 tbsp Turmeric Powder 1/4 tbsp Coriander Powder 1 tbsp Cumin Seeds 1 tbsp Garam Masala 2 pinch Asafoetida Heeng 2 tbsp Olive Oil Salt to taste 1 cup Milk or Cream Preparation Spinach Puree Wash the leaves thoroughly in running water. In a deep pot, bring about 6 cup of water to boiling temperature. Add a pinch of salt and 2 pinch asafoetida ( Heeng ) into the boiling water. Add Spinach leaves in boiling water and let it cook for 3 minutes. Strain boiled spinach leaves under cold water. In a blender, make a puree of leaves along with ginger, garlic and milk or cream. Curry for Paneer Cut Paneer into small cubes. Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add cumin seeds, bay leaf and minced garlic. Once cimun seeds start to pop, add finely cut onions. Fry the onions until oil starts to separate (about 1-2 minutes). Add finely chopped tomatoes and all the spices except Garam Masala . Once oil start to separate out, add the spinach puree. Stir well while cooking. Add salt and Garam Masala after about 2 minutes. After about another 2 minutes add Paneer cubes. Cook for another 3-4 minutes on low heat. Let it cool before serving. It can be served with Indian Breads or rice. Interesting Facts Nutritional Value of Paneer . More about Paneer Nutritional Value of Spinach.","tags":"Recipes","title":"Palak Paneer Recipe"},{"url":"http://sadanand-singh.github.io/TilapiaFish","text":"Typically, hunger and laziness come to me as inseparable couples. To make things worse, I have been trying to eat healthy. Nevertheless, here is another recipe that solves all these at once. Its my five minute Tilapia Fish Recipe. Two major ingredients - fish and veggies, of this recipe can be grabbed from the frozen section of any supermarket. Ingredients 2 frozen Boneless Tilapia Fish Pieces 1 Packet Frozen Steamed Veggies 2 tbsp Cooking Oil 1/2 tbsp Parsley 1 tbsp Gamram Masala 1 tbsp Cumin Powder 1/2 tbsp Turmeric Powder 1/2 tbsp Garlic Powder 1 tbsp Red Chili Powder 1/2 Lime Salt to Taste Preparation Thaw and wash fish pieces carefully. In a skillet, put a tbsp of oil and add frozen veggies to it. Add 1/2 of all the spices and salt to it. Fry for about 3-5 minutes and keep it aside on a plate. In the same skillet, add rest of the oil. After about a minute, add fish pieces on medium heat. Sprinkle salt and half of the remaining spices on the top of the fish. After about 3 minutes, turn the fish and sprinkle rest of the spices. Let the fish cook slowly. Sprinkle some lime juice on the fish. Once the fish is of mustard color, put fish pieces on veggies. Serve and garnish it with lime juice and some cilantro for an exotic lunch/dinner. Interesting Facts More about Tilapia Fish Nutritional Value of Tilapia Fish","tags":"Recipes","title":"Tilapia Fish Recipe"},{"url":"http://sadanand-singh.github.io/AlooParatha","text":"Today, I share one of my favorite dishes - Aloo Paratha . It is a dish of mashed potato stuffed bread from the Northern India. It is my favorite breakfast dish when I am in India. Here is my try in making some edible parathas . It might take few trials in making a perfectly round-shaped Paratha . Please share your views and inputs in comments below. Ingredients For Stuffing 2 medium Potatoes 2 tbsp Cooking Oil 1/2 tbsp Coriander Powder 1/2 tbsp Gamram Masala 1/2 tbsp Cumin Powder 1/2 tbsp Turmeric Powder 1/2 tbsp Garlic Powder 1 tbsp Red Chili Flakes 1/2 Medium Onion Salt to Taste For Bread 4 cups of Whole Wheat Flour 4-5 tbsp Butter or Ghee 1 tbsp Black Indian Onion Seeds ( Kalaunji ) 1 tbsp Ajwain (Carom Seeds) Salt to taste Preparation Stuffing Boil and peel Potatoes. Mash boiled Potatoes and mix salt and chilli flakes. In a skillet heat oil and add Garam Masala . Once it start to sputter, add cut onions. Add rest of spices and fry until golden brown. Add mashed Potatoes mix and frey for another 2-3 minbutes. Bread ( Paratha ) Knead flour with Ajwain , Kalaunji and salt. Make small balls Make craters in the balls and fill them with the stuffing. Roll these into flat circular breads, typical Indian Bread shape. Heat these slowly on a skillet on each side. Once almost cooked, apply some butter or Ghee . Let it cool before serving. It can be served with Indian spicy pickle and yogurt ( Dahi ). Interesting Facts More about Aloo Paratha More about Ajwain","tags":"Recipes","title":"Aloo Paratha Recipe"},{"url":"http://sadanand-singh.github.io/MixedVegPaneer","text":"I am quite found of Paneer . However, cooking it can be a hassle. You can get paneer generally at any Indian grocery store. For enthusiasts, Here is a recipe for making Paneer from milk. Here is a version of recipe that I use quite often. It involves two parts - Baking Paneer and Cooking the veggies. Ingredients For Baking Paneer 400 g Paneer 1 tbsp Coriander Powder 1 tbsp Gamram Masala 1/2 tbsp Cumin Powder 1/2 tbsp Turmeric Powder 1/2 tbsp Garlic Powder 1/4 cup Milk or Yogurt Salt to Taste For Veggies 500 g Cut Cauliflowers 1 cup Peas 1/2 Medium Onion 1 Medium Tomato 1/2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste Preparation Bake Paneer Thaw Paneer and cut in cubes. Preheat the oven at 400 F Mix Paneer with all the spices and milk/yogurt. Place cubes in a baking sheet and bake it at 400F for about 30 minutes until Paneer is well cooked. Prepare Veggies Cut onion, tomatoes and cauliflowers. Heat oil in the pan and add cumin seeds. Once Seeds start to pop, add cut onions. Fry the onions until oil starts to separate. (about 1-2 minutes) Add spices, peas and tomatoes. (about 1-2 minutes) After about a minute, add cauliflowers. After another minute or so, add salt, and keep stirring occasionally. Once the cauliflowers are almost cooked, add baked Paneer pieces (about 3-5 minutes). Add a little amount of water and cook for about a minute. ( optional ) Garnish with cilantro leaves. Let it cool before serving. It can be served either Indian breads or rice. Interesting Facts Nutritional Value of Paneer . Baked Paneer in this recipe is also referred as Paneer Tikka More about Paneer","tags":"Recipes","title":"Mixed Veg Paneer Recipe"},{"url":"http://sadanand-singh.github.io/myNewCompSpecs","text":"It has been long due. Just built a new desktop. Here are different parts I used to build this beuty. (Updated Cost) i7 4790 3.6 GHz (Haswell) - Can't Reveal - (Market Price $300) Asus Micro MATX B85-G R2.0 Intel LGA1150 - $50 ADATA XPG V1.0 DDR3 1866 2x4 GB RAM - $50 OCZ Vertex 460A Series 2.5\" 240 GB - $60 WD Blue 1TB 3.5\" 7200 RPM , 64MB Cache - $45 Ultra LSP V2 650 Watt PSU - $30 Ultra XBlaster Pro U12-42350 Case - $15 Asus BW - 12B1ST / BLK /G/ AS Blue Ray Burner - $20 Das Keyboard V4 Evoluent Ergo Right Mouse Total Value - $580 (Excluding Keyboard/Mouse) It is up and running Arch Linux with BTFRS filesystem, Plasma 5 Desktop and so on. Over the next few posts, I will be posting my installation ordeals. Do not forget to add your comments below. Please add your alternative solutions to anything I did.","tags":"Computers","title":"My New Desktop"},{"url":"http://sadanand-singh.github.io/DesiNoodles","text":"This is for all the lazy souls like me - in a mood to eat something tasty, but in no mood to cook for long. Vermicelli, or also known as Seviyan in Hindi, is commonly cooked as a sweet dish in Indian subcontinent. As I try to be away from all things sweet, I came across this recipe which uses this in a quite spicy flavor. So, here is my super quick and tasty recipe for these noodles. It takes less than 12 minutes ( 4 Minutes Preparation + 8 Minutes Cooking). Ingredients 300 g Vermicelli Seviyan 1 cup Peas 1/2 Medium Onion 1 Medium Tomato 1 Clove of Garlic 4 Curry Leaves 1/2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste 1/2 cup Peanuts optional Preparation Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add garlic and cut onions. Fry the onions until oil starts to separate (about 1-2 minutes) Add spices, curry leaves, peas and tomatoes. After about a minute, add noodles. After another minute or so, add salt, and keep stirring occasionally. Once the noodles have become light golden brown in color, add 1/2 cup of water (about 2-3 minutes). Keep stirring, so that noodles do not stick with each other, for about another 2 minutes Let it cool before serving. You can use some ketchup or sauce with it. I like adding peanuts to it while frying onions. Interesting Facts The sweet dish seviyan made out of this is also known as shemai in Bengali, sev in Gujarati, shavige in Kannada, sevalu or semiya in Telugu, and semiya in Tamil and Malayalam. The recipe above is also commonly known as upma in various parts of India. More about Vermicelli","tags":"Recipes","title":"Indian Vermicelli Recipe"},{"url":"http://sadanand-singh.github.io/shortestSubstring","text":"Problem Given an alphanumeric string, find the shortest substring that occurs exactly once as a (contiguous) substring in it. Overlapping occurrences are counted as distinct. If there are several candidates of the same length, you must output all of them in the order of occurrence. The \" space \" is NOT considered as a valid non-repeating substring. Example: Consider the following cases: Case 1: If the given string is asdfsasa , the answer should be ['d', 'f'] Case 2: If the given string is sadanands , the answer should be ['sa', 'ad', 'da', 'na', 'nd', 'ds'] Case 3: If the given string is wwwwwwww , the answer should be ['wwwwwwww'] My Solution Here is my solution in Python . It is quite brute force. I am not sure about the order of find() and rfind() built-in methods in Python. Assuming these are O(n) , my algorithm is in O(n 3 ) . Please put your answers in comments below, if your answer has a better scaling. The function definition that I use for finding non-empty non-repeating strings is recurcive. def findNsubString ( s , n ): subS = [] for index in range ( len ( s ) + 1 ) : x = s [ index : index + n ] if s . find ( x ) == s . rfind ( x ) : subS . append ( x ) if subS : return subS else : return findNsubString ( s , n + 1 ) I call this method as follows to get the desired results: #! /usr/bin/python import argparse # Parse Command Line Arguements parser = argparse . ArgumentParser () parser . add_argument ( \"-s\" , \"--string\" , default = \"asdfdfa\" , help = \"String Input\" ) args = parser . parse_args () s = args . string # Call Method to find smallest non-repeating sub-string ans = findNsubString ( s , 1 ) print ans A similar solution can also be written in JAVA or C++. The corresponding find() and rfind() methods in JAVA are called indexOf() and lastIndexOf() , respectively. In C++, these methods are called same as in Python. Please feel free to put your method definition in any programming language.","tags":"Programming","title":"Shortest Non-repeating Substring"},{"url":"http://sadanand-singh.github.io/RedSaagVeggies","text":"I have started eating a lot of greens these days. As a kid, I always loved a vegetable made by my mom, which was made of red leaves, called \" Laal Saag \" in Hindi. I could never find what exactly was the English/American name for those leaves. Recently, at one of the Korean grocery stores, I came across red Amaranth leaves. Those looked strikingly similar to the red leaves that I used to have back home in India. So, here is my super fast and healthy recipe for these red leaves. It took me just 10 minutes ( 5 Minutes Preparation + 5 Minutes Cooking). Ingredients 1 Bunch Red Amaranth Leaves 1/2 tbsp Turmeric Powder 1 tbsp Coriander Powder 1 tbsp Garam Masala 1 tbsp Mustard Black Seeds 1 tbsp Cumin Seeds 2 tbsp Olive Oil Salt to taste Preparation Wash the leaves thoroughly in running water and chop them (about 1 inch cuts). Heat a deep pan on Medium for about 1 minute and then pour olive oil into it. After about 30 seconds, Add mustard seeds and cumin seeds after another 30 seconds. Once Seeds start to pop, add cut leaves into the pan. Add salt, close the lid and keep stirring occasionally. Once the leaves have lost water, add all the spices and keep frying until things are dry (about 2-3 minutes). You can serve this with any Indian bread, or with rice and lentils ( Daal ). Interesting Facts Amaranth is also known as \" Chauli \" or \" Chavli \" in Hindi. Nutritional Value of Amaranth History of Amaranth","tags":"Recipes","title":"Lal Saag Recipe"},{"url":"http://sadanand-singh.github.io/ConsumeTransportProblem","text":"Problem Statement The owner of an apple plantation has a monkey. He wants to transport his 10000 apples to the market, which is located after the forest. The distance between his apple plantation and the market is about 1000 kilometer. So he decided to take his monkey to carry the apples. The monkey can carry at the maximum of 2000 apples at a time, and it eats one apple and throws another one for every kilometer it travels. What is the largest number of apples that can be delivered to the market? Please give your solutions in the comments below. Solution If the owner lets the monkey carry apples all the way to the end of the forest, in the first trip itself, all of 2000 aplles will be lost and the monkey will never return back, as the monkey will be out of any food and play. Lets approach this in a different way. Lets break the monkey's journey by per unit distance. To carry 10000 apples for 1 km, monkey has to make 2 x 5 - 1 = 9 trips! On each trip, the owner will loose 2 apples. Hence, for each km distance travelled untill number of apples is greater than 8000, monkey requires 9 x 2 = 18 apples. So, distance travelled by monkey until number of apples is less than 8000 is int(2000 / 18) + 1 = 112 km, and by this time, 10,000 - 112 x 18 = 7984 apples are left. Similarily, until 6000 apples are left, the monkey will require 2 x 4 - 1 = 7 trips for every km. Hence, the total distance travelled until no. of apples left is less than 6000 is 112 + int(2000 / 14) + 1 = 255 km, and by this time no. of apples left is 7984 - 143 x 14 = 5982. Now proceeding in a similar manner, until 4000 apples are left, the monkey will require 5 trips for each km. Total distance travelled until no. of apples left is less than 4000 is 255 + int(2000/10) = 455 km. By this time, number of apples left is 5982 - 200 x 10 = 3982. Until 2000 apples are left, the monkey will require 3 trips per km travelled. Total distance travelled till this time is 455 + int(2000/6) + 1 = 789 km. By this time, no. of apples left is 3982 - 334 x 6 = 1978. Below 2000 apples, the mokey will require only one trip to reach to market. Distance left is 1000 - 789 = 211 km. Number of apples required by the monkey to travel this distance is 211 x 2 = 422. Hence, total number of apples that can reach the market is just 1978 - 422 = 1556! That's just 15.56% of all apples. That's quite bad monkey. Moral of the story is, avoid dealing with fools :) Thank you all who tried this. Disclaimer: This puzzle has been inspired by a problem at the Blog on Technical Interviews.","tags":"Puzzle","title":"Puzzle 2"},{"url":"http://sadanand-singh.github.io/TrainSpeedProblem","text":"Here is the first puzzle. Problem Statement A man needs to go through a train tunnel to reach the other side. He starts running through the tunnel in an effort to reach his destination as soon as possible. When he is 1/4th of the way through the tunnel, he hears the train whistle behind him. Assuming the tunnel is not big enough for him and the train, he has to get out of the tunnel in order to survive. We know that the following conditions are true. If he runs back, he will make it out of the tunnel by a whisker. If he continues running forward, he will still make it out through the other end by a whisker. What is the speed of the train compared to that of the man? Please give your solutions in the comments below. Solution If the man decided to go back, he would have met the train at the entrance of tunnel (i.e The man would have travelled 0.25 of the tunnel). Instead, if man went forward- by the time train would have reached at the entrance of tunnel, the man would have been at the 0.25+0.25 = 0.5 of tunnel. Hence, according to the second condition, the time taken by man to travel half of tunnel is same as the time taken by the train to travel all of the tunnel. Therefore, the train is traveling at twice the speed of the man. Disclaimer: This puzzle has been copied from a Blog on Technical Interviews.","tags":"Puzzle","title":"Puzzle 1"},{"url":"http://sadanand-singh.github.io/FirstPost","text":"This is Sadanand Singh. I am a process engineer, a physicist, a programmer, an Indian and a human being; with interests in world politics, economics, and society. This space is for my personal notes on different subjects. I plan to share my thoughts on following topics from time to time. Technology Statistics Machine Leanrning News Economics Education Politics, Society & Education Indian Food Puzzles If you have interests in any of these topics, you are welcome to have a peek into my world. Please drop me your views through comments or any of the social media links. HAPPY WEB SURFING !!!","tags":"News","title":"Welcome to My World"}]}